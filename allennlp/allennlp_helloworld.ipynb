{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiRCu128GeCZ",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#AllenNLP-WalkThrough---Part-of-Speech-Tagging\" data-toc-modified-id=\"AllenNLP-WalkThrough---Part-of-Speech-Tagging-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>AllenNLP WalkThrough - Part of Speech Tagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#DatasetReader\" data-toc-modified-id=\"DatasetReader-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a href=\"https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html\" target=\"_blank\">DatasetReader</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#text_to_instance\" data-toc-modified-id=\"text_to_instance-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>text_to_instance</a></span></li><li><span><a href=\"#_read\" data-toc-modified-id=\"_read-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>_read</a></span></li><li><span><a href=\"#init\" data-toc-modified-id=\"init-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>init</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><a href=\"https://allenai.github.io/allennlp-docs/api/allennlp.data.vocabulary.html\" target=\"_blank\">Vocabulary</a></a></span></li><li><span><a href=\"#Iterators\" data-toc-modified-id=\"Iterators-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><a href=\"https://allenai.github.io/allennlp-docs/api/allennlp.data.iterators.html\" target=\"_blank\">Iterators</a></a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span><a href=\"https://allenai.github.io/allennlp-docs/api/allennlp.models.model.html#allennlp.models.model.Model\" target=\"_blank\">Model</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Embedder\" data-toc-modified-id=\"Embedder-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Embedder</a></span></li><li><span><a href=\"#Encoder\" data-toc-modified-id=\"Encoder-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Encoder</a></span></li><li><span><a href=\"#init\" data-toc-modified-id=\"init-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>init</a></span></li><li><span><a href=\"#forward\" data-toc-modified-id=\"forward-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>forward</a></span></li><li><span><a href=\"#get_metrics\" data-toc-modified-id=\"get_metrics-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>get_metrics</a></span></li></ul></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Trainer</a></span></li><li><span><a href=\"#Predictor\" data-toc-modified-id=\"Predictor-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Predictor</a></span></li><li><span><a href=\"#Serialization-&amp;-DeSerialization\" data-toc-modified-id=\"Serialization-&amp;-DeSerialization-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Serialization &amp; DeSerialization</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yE8LLz-FGihy",
    "outputId": "203f8ead-ab89-406b-db7c-72550585a546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
      "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from watermark) (5.5.0)\n",
      "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n",
      "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.2.0)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.1)\n",
      "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.6)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
      "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.6)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
      "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
      "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.14.0)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
      "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
      "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
      "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.8)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.243)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
      "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.2)\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
      "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (41.2.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (2.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.3.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.7.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (1.0.18)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.4.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.8.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n",
      "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.2.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.1.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.243 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.243)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.8.19)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.83)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
      "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "vR_NgUn8GeCb",
    "outputId": "d4aef38e-524e-4cd1-a256-7572423e9adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2019-10-16 17:58:33 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 5.5.0\n",
      "\n",
      "numpy 1.16.5\n",
      "torch 1.2.0\n",
      "allennlp 0.9.0\n",
      "nltk 3.2.5\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from overrides import overrides\n",
    "from typing import Iterator, List, Dict, Tuple\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,torch,allennlp,nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gy6z_LtGeCf"
   },
   "source": [
    "# AllenNLP WalkThrough - Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ft6vc52GeCg"
   },
   "source": [
    "The problem of **Part of Speech Tagging (POS)** looks like the following: given a sentence (e.g. \"The dog ate the apple\") we want to predict the part-of-speech tags for each word [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]. This this documentation, we'll walkthrough the process of using the AllenNLP framework to solve for this particular problem.\n",
    "\n",
    "AllenNLP at its core is a framework for constructing pipelines to train NLP models. We can leverage different components of the framework or implement our custom components at different steps to tackle various NLP problems.\n",
    "\n",
    "A typical AllenNLP pipeline is composed of the following components:\n",
    "\n",
    "- DatasetReader: Extracts necessary information from the data and turns them into a list of `Instance` objects.\n",
    "- Model: The model to be trained. This is where we define the various architecture of the neural network.\n",
    "- Iterator: Batches the data.\n",
    "- Trainer: Handles training/optimization and metric recording.\n",
    "- Predictor: Generates predictions from raw strings.\n",
    "\n",
    "Each of these components is loosely coupled, meaning it is easy to swap different components in without having to change other parts of your code. To take full advantage of all the features available, we'll need to spend some time and understand what each component is responsible for and what protocols it must respect for these parts to work well together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "t5WN8fI0GeCh",
    "outputId": "4042b815-7acb-4eef-b7d2-057801aa8411"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f47cfb7a4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some constant parameters we can tweak along the way\n",
    "seed = 1\n",
    "batch_size = 32\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "lr = 0.1\n",
    "num_epochs = 300\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eSRonJmGeCk"
   },
   "source": [
    "## [DatasetReader](https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CeyPdjmSGeCk"
   },
   "source": [
    "In AllenNLP each training example is represented as an `Instance` consisting of `Fields` of various types. A `DatasetReader` defines the logic to generate those instances (typically) from data stored on disk.\n",
    "\n",
    "Typically to create a `DatasetReader` we'd implement two methods:\n",
    "\n",
    "- `text_to_instance`. The naming for this method is slightly misleading, as it handles not only our text, but also labels, metadata, and anything else that we model will need later on. The essence of this method is to take the data for a single example and pack it into an `Instance` object. Instance objects are very similar to dictionaries, and all you need to know about them in practice is that they are instantiated with a dictionary mapping field names to `Field`.\n",
    "\n",
    "- `_read` takes the path to an input file and returns an Iterator of `Instances`. (It will probably delegate most of its work to `text_to_instance`.)\n",
    "\n",
    "We'll introduce more as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acki_1tmGeCl"
   },
   "outputs": [],
   "source": [
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBrDusUrGeCn"
   },
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one sentence per line, like\n",
    "\n",
    "    The\\u0001DET dog\\u0001NN ate\\u0001V the\\u0001DET apple\\u0001NN\n",
    "    \n",
    "    i.e. we have the corresponding part of speech tagging for each token in the sentence\n",
    "    where each token and tag is delimited by a \\u0001 symbol and each pair is then delimited\n",
    "    by a white-space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 lazy: bool = False,\n",
    "                 token_tag_delimiter: str = '\\u0001') -> None:\n",
    "        super().__init__(lazy=lazy)\n",
    "        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        self.token_tag_delimiter = token_tag_delimiter\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {'sentence': sentence_field}\n",
    "\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields['labels'] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        \"\"\"takes a filename and produces a stream of Instance.\"\"\"\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(self.token_tag_delimiter) for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81Lu0dhYGeCq"
   },
   "source": [
    "### text_to_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzre2HmJGeCq"
   },
   "source": [
    "A couple of things to notice. The first is that the tokens variable is a `List[Token]` (and not a `List[str]`). If we use some of the built in tokenizer, e.g. `WordTokenizer`, that's already the output we'll get. If we have pre-tokenized data, we just need to wrap each string token in a call to `Token`.\n",
    "\n",
    "Another thing to notice is that the tags/labels are optional. This is so that after we train a model we can use it to make predictions on untagged data (which clearly won't have any tags).\n",
    "\n",
    "We've mentioned in the previous section that `Instance` is a dictionary of `Field`. Now is a good time to touch upon `Field`. `Field` objects in AllenNLP handles the conversion of our data into tensors, these tensors are then fed into the model.\n",
    "\n",
    "Here we're using the `TextField` for our tokens. The `TextField` does what all good NLP libraries do: it converts a sequence of tokens into integers. Be careful here though, since this is all the `TextField` does. It doesn’t clean the text, tokenize the text, etc.. we'll need to do that yourself. `TextField` takes an additional argument on `init`: the token indexer. Though the `TextField` handles converting tokens to integers, we need to tell it how to do this. Why? Because we might want to use a character-level model instead of a word-level model. AllenNLP gives us the flexibility of specifying these attributes.\n",
    "\n",
    "As for the tags/labels. We put them in a `SequenceLabelField`, which is for labels corresponding to each element of a sequence. If we had a label that applied to the entire sentence, for example \"sentiment\", we would instead use a `LabelField`.\n",
    "\n",
    "Finally, we just return an Instance containing the dict field_name -> Field.\n",
    "\n",
    "Usually a `DatasetReader` will need to have a `dict` of `TokenIndexer`s that specify how we want to convert text tokens into indices. For instance, we will usually have a `SingleIdTokenIndexer` which maps each word to a unique ID, and you might also (or instead) have a TokenCharactersIndexer, which maps each word to a sequence of indices corresponding to its characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQvFVAG6GeCr"
   },
   "source": [
    "### _read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMsfgZkWGeCs"
   },
   "source": [
    "The main purpose of this method is to produce a stream of `Instance`.\n",
    "\n",
    "We split each line on spaces to get pairs word###TAG, split each pair to get tuples (word, tag), use zip to break those into a list of words and a list of tags, wrap each word in Token (as described in the previous section), and then call `text_to_instance`.\n",
    "\n",
    "The reason for splitting the logic into two functions is that `text_to_instance` is useful on its own, for instance, if you build an interactive demo for your model and want to produce Instances from user-supplied sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezh84X1fGeCt"
   },
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJhF5Yt4GeCt"
   },
   "source": [
    "The `__init__` method takes in the token indexer that we'll use for the `TextField` and a `lazy` parameter.\n",
    "\n",
    "- `lazy`. If we're working with datasets that don't fit into memory. AllenNLP can lazily load the data (only read the data into memory when you actually need it). This does impose some additional complexity and runtime overhead.\n",
    "\n",
    "We can also add other parameters to make our reader more customizable, e.g. provide various options for the tokenization part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jil4LVabsKCA"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wW6atfkxsMLV"
   },
   "source": [
    "The [Penn Treebank](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf) contains a corpus of annotated POS tags. A sample is available in the NLTK python library which contains a lot of corpora that can be used to train and test some NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "6F0EO9V2GeCu",
    "outputId": "2034d409-e423-4183-b6f3-abeed3756b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
      "number of sentences:  3914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "print('number of sentences: ', len(sentences))\n",
    "\n",
    "# a sentence consists of multiple tuples, where the tuples\n",
    "# are a pair of token and their corresponding POS tag\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wRvbPguGeCw"
   },
   "source": [
    "We would write out the file, so that each line of our dataset looks something like:\n",
    "\n",
    "`The\\u0001DET dog\\u0001NN ate\\u0001V the\\u0001DET apple\\u0001NN`\n",
    "\n",
    "Where there is a `u0001` delimiter between each token and tag and each pair is then delimited by a white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMJqadHvGeCx"
   },
   "outputs": [],
   "source": [
    "def write_nltk_treebank_tagged_sents(sentences: List[Tuple[str, str]], output_filename: str,\n",
    "                                     token_tag_delimiter: str = '\\u0001') -> None:\n",
    "    directory, _ = os.path.split(output_filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for sentence in sentences:\n",
    "            line = ' '.join(token + token_tag_delimiter + tag for token, tag in sentence)\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q3bW0elGeCz"
   },
   "outputs": [],
   "source": [
    "# split the data into training and validation\n",
    "validation_size = 0.2\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "train_index = 1 - int(validation_size * len(sentences)) \n",
    "training_sentences = sentences[:train_index]\n",
    "validation_sentences = sentences[train_index:]\n",
    "\n",
    "data_dir = 'data'\n",
    "train_dataset_path = os.path.join(data_dir, 'treebank_tagged_sents_train.txt')\n",
    "validation_dataset_path = os.path.join(data_dir, 'treebank_tagged_sents_validation.txt')\n",
    "write_nltk_treebank_tagged_sents(training_sentences, train_dataset_path)\n",
    "write_nltk_treebank_tagged_sents(validation_sentences, validation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "zqbkPivDGeC1",
    "outputId": "b6061407-cb9e-4f77-eb4c-53c4c7eeca37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3133it [00:00, 10107.68it/s]\n",
      "781it [00:00, 3568.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Pierre,\n",
       " Vinken,\n",
       " ,,\n",
       " 61,\n",
       " years,\n",
       " old,\n",
       " ,,\n",
       " will,\n",
       " join,\n",
       " the,\n",
       " board,\n",
       " as,\n",
       " a,\n",
       " nonexecutive,\n",
       " director,\n",
       " Nov.,\n",
       " 29,\n",
       " .]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = PosDatasetReader()\n",
    "train_dataset = reader.read(train_dataset_path)\n",
    "validation_dataset = reader.read(validation_dataset_path)\n",
    "\n",
    "# for example, we can access the first element of the Stream of Instance\n",
    "# and access the TextField, which we keyed as 'sentence', and access the\n",
    "# tokens attribute to look at the tokenized word\n",
    "train_dataset[0].fields['sentence'].tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gdBnc1NGeC3"
   },
   "source": [
    "## [Vocabulary](https://allenai.github.io/allennlp-docs/api/allennlp.data.vocabulary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hu_CMJkqGeC4"
   },
   "source": [
    "The other thing that goes hand in hand with our `DatasetReader` and `Iterator` that we'll discuss in then next section is `Vocabulary`. To build the vocabulary, you need to pass through all the text. We can only convert fields into tensors after you know what the vocabulary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EFuPYGxNGeC5",
    "outputId": "ee66743a-0ccd-4610-de7e-d6e12da6abb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3914/3914 [00:00<00:00, 34066.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tokens, Size: 12410 || labels, Size: 12 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "h3gVp1mCGeC7",
    "outputId": "fa1975fd-9c43-4dd8-f456-88be7e8bdde9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'VERB',\n",
       " '.',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'X',\n",
       " 'ADJ',\n",
       " 'NUM',\n",
       " 'PRT',\n",
       " 'ADV',\n",
       " 'PRON',\n",
       " 'CONJ']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the distinct labels we have in our dataset\n",
    "[vocab.get_token_from_index(i, 'labels') for i in range(vocab.get_vocab_size('labels'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1nNIDE1GeC9"
   },
   "source": [
    "## [Iterators](https://allenai.github.io/allennlp-docs/api/allennlp.data.iterators.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZobkSbvKGeC-"
   },
   "source": [
    "Neural networks are usually trained on mini batches of tensors, not lists of data. Therefore, datasets need to be batched and converted to tensors. This seems trivial at first glance, but there is a lot of subtlety here. To list just a few things we have to consider:\n",
    "\n",
    "- Sequences of different lengths need to be padded\n",
    "- To minimize padding, sequences of similar lengths can be put in the same batch\n",
    "- Tensors need to be sent to the GPU if using the GPU\n",
    "- Data needs to be shuffled at the end of each epoch during training, but we don't want to shuffle in the midst of an epoch in order to cover all examples evenly\n",
    "\n",
    "Thankfully, AllenNLP has several convenient iterators that will take care of all of these problems behind the scenes, Therefore, you will rarely have to implement your own Iterators from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TBS5Vl8GeC_"
   },
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator\n",
    "\n",
    "iterator = BucketIterator(batch_size=batch_size, sorting_keys=[('sentence', 'num_tokens')])\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMkyVUENGeDB"
   },
   "source": [
    "The `BucketIterator` batches sequences of similar lengths together to minimize padding. `sorting_keys` keyword argument tells the iterator which field to reference when determining the text length of each instance. Here `sentence` is the key to our `TextField` and `num_tokens` is a padding key keyword that tells it to sort according to the number of tokens for that field.\n",
    "\n",
    "Iterators are responsible for converting our text to numerical ids. We pass the vocabulary we built earlier so that the Iterator knows how to map our text to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "jzNYy1D4GeDB",
    "outputId": "c6e0e71e-3134-4206-fc16-51d6045a31b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[ 4,  0,  0,  ...,  0,  0,  0],\n",
       "         [ 6,  4,  0,  ...,  0,  0,  0],\n",
       "         [ 7,  0,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 2,  4,  1,  ...,  0,  2,  2],\n",
       "         [10,  1,  1,  ...,  0,  0,  0],\n",
       "         [ 5,  1,  3,  ...,  0,  0,  0]]),\n",
       " 'sentence': {'tokens': tensor([[  93, 5378, 1020,  ...,    0,    0,    0],\n",
       "          [2631,   59,  171,  ...,    0,    0,    0],\n",
       "          [3157, 3820,   28,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  20,  325,  133,  ..., 8208,    4,   21],\n",
       "          [  99,   54, 5529,  ...,    0,    0,    0],\n",
       "          [  10, 4534,   27,  ...,    0,    0,    0]])}}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a peak at how the output from the iterator would look like\n",
    "# when passing in the train_dataset from earlier\n",
    "batch = next(iter(iterator(train_dataset)))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1dhEpXFGeDE"
   },
   "source": [
    "Note the `tokens` key is the key name that we've specified for our `token_indexers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSHt8gMuGeDE"
   },
   "source": [
    "## [Model](https://allenai.github.io/allennlp-docs/api/allennlp.models.model.html#allennlp.models.model.Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1VYVkkbGeDF"
   },
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3agWkFlGeDH"
   },
   "source": [
    "Now that we've prepared the data, the next part is the `Model`. AllenNLP models is mostly a subclass of `torch.nn.Module`. How the forward pass looks like is mainly up to us. The only key difference is that AllenNLP models are required to compute the loss function within the forward method during training and return a dictionary for every forward pass that includes that loss value. This output is what the downstream `Trainer` expects. (we'll touch upon in the next section).\n",
    "\n",
    "AllenNLP models are generally composed from the following components:\n",
    "\n",
    "- A token embedder\n",
    "- An encoder\n",
    "- (For seq-to-seq models) A decoder\n",
    "\n",
    "In this example, we will create a model that consists of an embedding layer, a sequence encoder, and a feedforward network that predicts the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F2_9HG9GeDI"
   },
   "source": [
    "### Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWqZFr21GeDI"
   },
   "source": [
    "The embedder maps a sequence of token ids (or character ids) into a sequence of tensors.\n",
    "\n",
    "For embedding the tokens we'll use the `BasicTextFieldEmbedder` which takes a mapping from index names to embeddings. If we go back to where we defined our `DatasetReader`, the key to our token_indexers was called `tokens`, so our mapping just needs an embedding corresponding to that index. \n",
    "\n",
    "We'll also use the `Vocabulary` to find how many embeddings we need and our `embedding_dim` parameter to specify the output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE9RIJ8FGeDK"
   },
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=embedding_dim)\n",
    "word_embeddings = BasicTextFieldEmbedder({'tokens': token_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ED5a9kw2GeDN"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOuBoKTmGeDN"
   },
   "source": [
    "We next need to specify the sequence encoder. AllenNLP provides a `PytorchSeq2SeqWrapper` that has some extra functionality to the built-in PyTorch module. In AllenNLP, we do everything batch first, so we specify that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99OTBS9GGeDP"
   },
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8zXnOS5GeDR"
   },
   "source": [
    "We can define and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMxkW5SaGeDS"
   },
   "outputs": [],
   "source": [
    "class LstmTagger(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2class = nn.Linear(encoder.get_output_dim(),\n",
    "                                      vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        class_logits = self.hidden2class(encoder_out)\n",
    "        output = {'class_logits': class_logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            self.accuracy(class_logits, labels, mask)\n",
    "            output['loss'] = sequence_cross_entropy_with_logits(class_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "vV4_fTLfGeDU",
    "outputId": "9e770923-0035-477d-8dc8-0a61452faf78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[[-0.0594,  0.0619, -0.0828,  ...,  0.0825, -0.0795,  0.0140],\n",
       "          [-0.0688,  0.0606, -0.0804,  ...,  0.0798, -0.0787,  0.0130],\n",
       "          [-0.0745,  0.0592, -0.0799,  ...,  0.0812, -0.0779,  0.0128],\n",
       "          ...,\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100]],\n",
       " \n",
       "         [[-0.0611,  0.0610, -0.0846,  ...,  0.0808, -0.0799,  0.0152],\n",
       "          [-0.0692,  0.0625, -0.0824,  ...,  0.0820, -0.0788,  0.0139],\n",
       "          [-0.0747,  0.0622, -0.0799,  ...,  0.0816, -0.0773,  0.0125],\n",
       "          ...,\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100]],\n",
       " \n",
       "         [[-0.0607,  0.0608, -0.0812,  ...,  0.0833, -0.0796,  0.0149],\n",
       "          [-0.0681,  0.0616, -0.0796,  ...,  0.0807, -0.0798,  0.0159],\n",
       "          [-0.0732,  0.0622, -0.0798,  ...,  0.0797, -0.0789,  0.0139],\n",
       "          ...,\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0595,  0.0622, -0.0830,  ...,  0.0843, -0.0819,  0.0144],\n",
       "          [-0.0694,  0.0615, -0.0796,  ...,  0.0833, -0.0809,  0.0139],\n",
       "          [-0.0741,  0.0610, -0.0788,  ...,  0.0829, -0.0802,  0.0125],\n",
       "          ...,\n",
       "          [-0.0799,  0.0602, -0.0804,  ...,  0.0767, -0.0780,  0.0116],\n",
       "          [-0.0816,  0.0611, -0.0794,  ...,  0.0768, -0.0788,  0.0124],\n",
       "          [-0.0823,  0.0615, -0.0805,  ...,  0.0755, -0.0794,  0.0117]],\n",
       " \n",
       "         [[-0.0604,  0.0625, -0.0824,  ...,  0.0838, -0.0787,  0.0130],\n",
       "          [-0.0680,  0.0595, -0.0810,  ...,  0.0822, -0.0782,  0.0133],\n",
       "          [-0.0730,  0.0603, -0.0810,  ...,  0.0813, -0.0779,  0.0118],\n",
       "          ...,\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100]],\n",
       " \n",
       "         [[-0.0584,  0.0615, -0.0832,  ...,  0.0830, -0.0795,  0.0137],\n",
       "          [-0.0679,  0.0607, -0.0823,  ...,  0.0810, -0.0778,  0.0134],\n",
       "          [-0.0742,  0.0610, -0.0818,  ...,  0.0810, -0.0773,  0.0118],\n",
       "          ...,\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100],\n",
       "          [-0.0422,  0.0618, -0.0874,  ...,  0.0809, -0.0773,  0.0100]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'loss': tensor(2.5016, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LstmTagger(word_embeddings, lstm, vocab)\n",
    "\n",
    "# we can now unpack the batch and feed it to the model\n",
    "model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaEdcBkjGeDW"
   },
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhlphmHGGeDX"
   },
   "source": [
    "Notice that the `word_embeddings` and `encoder` parameter type is the base class of the embedder and encoder that we're using. This is so that we can mix and match different embedder and encoder to see which one works best for our problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKi2kk9gGeDX"
   },
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQXwulVBGeDY"
   },
   "source": [
    "Each Instance in our dataset will get (batched with other instances and) fed into forward. The forward method expects dicts of tensors as input, and it expects their names to be the names of the fields in our Instance. In this case we have a sentence field and (possibly) a labels field, so we'll construct our forward accordingly.\n",
    "\n",
    "Then the core logic inside the forward method is pretty much identical to a regular PyTorch module, where we feed through each layer and make an update on the model based on the loss computed if the labels were provided.\n",
    "\n",
    "The other interesting part is the `get_text_field_mask` method. AllenNLP is designed to operate on batched inputs, but different input sequences have different lengths. Behind the scenes AllenNLP is padding the shorter inputs so that the batch has uniform shape, which means our computations should use a mask to exclude the padding. Here we use the utility function `get_text_field_mask`, which returns a tensor of 0s and 1s corresponding to the padded and unpadded locations.\n",
    "\n",
    "The output of the forward pass should consist of the dictionary with the `loss` key if we wish to use this Model allow-side `Trainer` (we'll discuss in the next section).\n",
    "\n",
    "Note that for this model, we are using the various modules/building blocks that comes with AllenNLP, this doesn't mean we can't implement our own if we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3F1Lxg28GeDZ"
   },
   "source": [
    "### get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgZGYJSoGeDZ"
   },
   "source": [
    "This method gives us the flexibility to log additional metrics that we care about. In this case, we've computed an accuracy metric that gets updated for each forward pass during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSYBp0d3GeDa"
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ums_Vs-sGeDb"
   },
   "source": [
    "One of the pain points in flexible framework such as PyTorch has been doing the training after defining the model and dataset. Often times, we will need to write a lot of boilerplate code just do get a training loop. AllenNLP includes a Trainer class that handles most of the gory details of training models. After passing in all the necessary parameters, we can call `.train` to train it.\n",
    "\n",
    "Of course, this type of stuff can be a double-edged sword. Some people enjoy having the ability of customize the logic of the training loop. Some frameworks such as `fastai`, `keras` gives user the flexibility of adding different callbacks to do certain customization to the training loop.\n",
    "\n",
    "The next code chunk contains boilerplate code to move our model to a GPU if we have access to one and define a optimizer to train the model. After that, we have all the bare minimum parameter we need to use the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfekLXn8GeDb"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rtu1gYTlGeDd",
    "outputId": "4b255570-b525-4bf7-f505-234031ce575c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2724, loss: 2.2897 ||: 100%|██████████| 98/98 [00:01<00:00, 64.43it/s]\n",
      "accuracy: 0.2942, loss: 2.1997 ||: 100%|██████████| 25/25 [00:00<00:00, 123.04it/s]\n",
      "accuracy: 0.2849, loss: 2.1952 ||: 100%|██████████| 98/98 [00:01<00:00, 69.66it/s]\n",
      "accuracy: 0.2942, loss: 2.1874 ||: 100%|██████████| 25/25 [00:00<00:00, 153.95it/s]\n",
      "accuracy: 0.2849, loss: 2.1880 ||: 100%|██████████| 98/98 [00:01<00:00, 73.00it/s]\n",
      "accuracy: 0.2942, loss: 2.1845 ||: 100%|██████████| 25/25 [00:00<00:00, 146.71it/s]\n",
      "accuracy: 0.2849, loss: 2.1859 ||: 100%|██████████| 98/98 [00:01<00:00, 73.46it/s]\n",
      "accuracy: 0.2942, loss: 2.1830 ||: 100%|██████████| 25/25 [00:00<00:00, 142.30it/s]\n",
      "accuracy: 0.2849, loss: 2.1843 ||: 100%|██████████| 98/98 [00:01<00:00, 73.52it/s]\n",
      "accuracy: 0.2942, loss: 2.1801 ||: 100%|██████████| 25/25 [00:00<00:00, 149.85it/s]\n",
      "accuracy: 0.2849, loss: 2.1824 ||: 100%|██████████| 98/98 [00:01<00:00, 73.58it/s]\n",
      "accuracy: 0.2942, loss: 2.1805 ||: 100%|██████████| 25/25 [00:00<00:00, 154.95it/s]\n",
      "accuracy: 0.2849, loss: 2.1803 ||: 100%|██████████| 98/98 [00:01<00:00, 73.17it/s]\n",
      "accuracy: 0.2942, loss: 2.1760 ||: 100%|██████████| 25/25 [00:00<00:00, 156.01it/s]\n",
      "accuracy: 0.2849, loss: 2.1777 ||: 100%|██████████| 98/98 [00:01<00:00, 72.89it/s]\n",
      "accuracy: 0.2942, loss: 2.1736 ||: 100%|██████████| 25/25 [00:00<00:00, 157.24it/s]\n",
      "accuracy: 0.2849, loss: 2.1739 ||: 100%|██████████| 98/98 [00:01<00:00, 71.96it/s]\n",
      "accuracy: 0.2942, loss: 2.1675 ||: 100%|██████████| 25/25 [00:00<00:00, 158.11it/s]\n",
      "accuracy: 0.2849, loss: 2.1684 ||: 100%|██████████| 98/98 [00:01<00:00, 74.12it/s]\n",
      "accuracy: 0.2942, loss: 2.1613 ||: 100%|██████████| 25/25 [00:00<00:00, 156.80it/s]\n",
      "accuracy: 0.2849, loss: 2.1595 ||: 100%|██████████| 98/98 [00:01<00:00, 72.87it/s]\n",
      "accuracy: 0.2942, loss: 2.1501 ||: 100%|██████████| 25/25 [00:00<00:00, 154.47it/s]\n",
      "accuracy: 0.2849, loss: 2.1441 ||: 100%|██████████| 98/98 [00:01<00:00, 76.25it/s]\n",
      "accuracy: 0.2942, loss: 2.1300 ||: 100%|██████████| 25/25 [00:00<00:00, 161.11it/s]\n",
      "accuracy: 0.3106, loss: 2.1149 ||: 100%|██████████| 98/98 [00:01<00:00, 73.60it/s]\n",
      "accuracy: 0.3680, loss: 2.0936 ||: 100%|██████████| 25/25 [00:00<00:00, 151.49it/s]\n",
      "accuracy: 0.3725, loss: 2.0613 ||: 100%|██████████| 98/98 [00:01<00:00, 73.58it/s]\n",
      "accuracy: 0.3792, loss: 2.0311 ||: 100%|██████████| 25/25 [00:00<00:00, 151.90it/s]\n",
      "accuracy: 0.3740, loss: 1.9880 ||: 100%|██████████| 98/98 [00:01<00:00, 64.43it/s]\n",
      "accuracy: 0.3747, loss: 1.9627 ||: 100%|██████████| 25/25 [00:00<00:00, 159.55it/s]\n",
      "accuracy: 0.3701, loss: 1.9254 ||: 100%|██████████| 98/98 [00:01<00:00, 73.51it/s]\n",
      "accuracy: 0.3746, loss: 1.9121 ||: 100%|██████████| 25/25 [00:00<00:00, 155.48it/s]\n",
      "accuracy: 0.3716, loss: 1.8784 ||: 100%|██████████| 98/98 [00:01<00:00, 73.39it/s]\n",
      "accuracy: 0.3776, loss: 1.8681 ||: 100%|██████████| 25/25 [00:00<00:00, 158.65it/s]\n",
      "accuracy: 0.3820, loss: 1.8323 ||: 100%|██████████| 98/98 [00:01<00:00, 74.28it/s]\n",
      "accuracy: 0.4162, loss: 1.8166 ||: 100%|██████████| 25/25 [00:00<00:00, 152.40it/s]\n",
      "accuracy: 0.4263, loss: 1.7803 ||: 100%|██████████| 98/98 [00:01<00:00, 73.11it/s]\n",
      "accuracy: 0.4348, loss: 1.7562 ||: 100%|██████████| 25/25 [00:00<00:00, 161.21it/s]\n",
      "accuracy: 0.4445, loss: 1.7245 ||: 100%|██████████| 98/98 [00:01<00:00, 73.84it/s]\n",
      "accuracy: 0.4670, loss: 1.6953 ||: 100%|██████████| 25/25 [00:00<00:00, 152.69it/s]\n",
      "accuracy: 0.4611, loss: 1.6708 ||: 100%|██████████| 98/98 [00:01<00:00, 72.82it/s]\n",
      "accuracy: 0.4701, loss: 1.6388 ||: 100%|██████████| 25/25 [00:00<00:00, 148.87it/s]\n",
      "accuracy: 0.4618, loss: 1.6212 ||: 100%|██████████| 98/98 [00:01<00:00, 75.66it/s]\n",
      "accuracy: 0.4697, loss: 1.5860 ||: 100%|██████████| 25/25 [00:00<00:00, 155.73it/s]\n",
      "accuracy: 0.4617, loss: 1.5719 ||: 100%|██████████| 98/98 [00:01<00:00, 72.94it/s]\n",
      "accuracy: 0.4715, loss: 1.5311 ||: 100%|██████████| 25/25 [00:00<00:00, 158.71it/s]\n",
      "accuracy: 0.4703, loss: 1.5177 ||: 100%|██████████| 98/98 [00:01<00:00, 73.81it/s]\n",
      "accuracy: 0.5019, loss: 1.4702 ||: 100%|██████████| 25/25 [00:00<00:00, 150.75it/s]\n",
      "accuracy: 0.5124, loss: 1.4551 ||: 100%|██████████| 98/98 [00:01<00:00, 72.80it/s]\n",
      "accuracy: 0.5683, loss: 1.3976 ||: 100%|██████████| 25/25 [00:00<00:00, 158.55it/s]\n",
      "accuracy: 0.5625, loss: 1.3852 ||: 100%|██████████| 98/98 [00:01<00:00, 75.37it/s]\n",
      "accuracy: 0.6039, loss: 1.3189 ||: 100%|██████████| 25/25 [00:00<00:00, 160.92it/s]\n",
      "accuracy: 0.5945, loss: 1.3154 ||: 100%|██████████| 98/98 [00:01<00:00, 73.06it/s]\n",
      "accuracy: 0.6267, loss: 1.2501 ||: 100%|██████████| 25/25 [00:00<00:00, 152.86it/s]\n",
      "accuracy: 0.6130, loss: 1.2518 ||: 100%|██████████| 98/98 [00:01<00:00, 73.51it/s]\n",
      "accuracy: 0.6386, loss: 1.1874 ||: 100%|██████████| 25/25 [00:00<00:00, 151.35it/s]\n",
      "accuracy: 0.6270, loss: 1.1950 ||: 100%|██████████| 98/98 [00:01<00:00, 72.91it/s]\n",
      "accuracy: 0.6542, loss: 1.1297 ||: 100%|██████████| 25/25 [00:00<00:00, 156.81it/s]\n",
      "accuracy: 0.6446, loss: 1.1432 ||: 100%|██████████| 98/98 [00:01<00:00, 74.10it/s]\n",
      "accuracy: 0.6817, loss: 1.0802 ||: 100%|██████████| 25/25 [00:00<00:00, 149.09it/s]\n",
      "accuracy: 0.6675, loss: 1.0934 ||: 100%|██████████| 98/98 [00:01<00:00, 72.91it/s]\n",
      "accuracy: 0.7012, loss: 1.0257 ||: 100%|██████████| 25/25 [00:00<00:00, 151.92it/s]\n",
      "accuracy: 0.6849, loss: 1.0458 ||: 100%|██████████| 98/98 [00:01<00:00, 74.23it/s]\n",
      "accuracy: 0.7141, loss: 0.9741 ||: 100%|██████████| 25/25 [00:00<00:00, 142.69it/s]\n",
      "accuracy: 0.7044, loss: 0.9977 ||: 100%|██████████| 98/98 [00:01<00:00, 73.47it/s]\n",
      "accuracy: 0.7343, loss: 0.9303 ||: 100%|██████████| 25/25 [00:00<00:00, 148.81it/s]\n",
      "accuracy: 0.7231, loss: 0.9534 ||: 100%|██████████| 98/98 [00:01<00:00, 74.50it/s]\n",
      "accuracy: 0.7509, loss: 0.8923 ||: 100%|██████████| 25/25 [00:00<00:00, 155.85it/s]\n",
      "accuracy: 0.7357, loss: 0.9102 ||: 100%|██████████| 98/98 [00:01<00:00, 71.90it/s]\n",
      "accuracy: 0.7391, loss: 0.8515 ||: 100%|██████████| 25/25 [00:00<00:00, 135.60it/s]\n",
      "accuracy: 0.7379, loss: 0.8780 ||: 100%|██████████| 98/98 [00:01<00:00, 72.95it/s]\n",
      "accuracy: 0.7645, loss: 0.8062 ||: 100%|██████████| 25/25 [00:00<00:00, 157.39it/s]\n",
      "accuracy: 0.7463, loss: 0.8416 ||: 100%|██████████| 98/98 [00:01<00:00, 73.42it/s]\n",
      "accuracy: 0.7720, loss: 0.7729 ||: 100%|██████████| 25/25 [00:00<00:00, 158.56it/s]\n",
      "accuracy: 0.7539, loss: 0.8043 ||: 100%|██████████| 98/98 [00:01<00:00, 73.15it/s]\n",
      "accuracy: 0.7697, loss: 0.7627 ||: 100%|██████████| 25/25 [00:00<00:00, 159.15it/s]\n",
      "accuracy: 0.7609, loss: 0.7706 ||: 100%|██████████| 98/98 [00:01<00:00, 72.98it/s]\n",
      "accuracy: 0.7775, loss: 0.7133 ||: 100%|██████████| 25/25 [00:00<00:00, 161.31it/s]\n",
      "accuracy: 0.7705, loss: 0.7378 ||: 100%|██████████| 98/98 [00:01<00:00, 74.61it/s]\n",
      "accuracy: 0.7830, loss: 0.6981 ||: 100%|██████████| 25/25 [00:00<00:00, 151.07it/s]\n",
      "accuracy: 0.7801, loss: 0.7113 ||: 100%|██████████| 98/98 [00:01<00:00, 72.33it/s]\n",
      "accuracy: 0.7715, loss: 0.7248 ||: 100%|██████████| 25/25 [00:00<00:00, 156.35it/s]\n",
      "accuracy: 0.7948, loss: 0.6770 ||: 100%|██████████| 98/98 [00:01<00:00, 72.61it/s]\n",
      "accuracy: 0.8123, loss: 0.6271 ||: 100%|██████████| 25/25 [00:00<00:00, 145.96it/s]\n",
      "accuracy: 0.8040, loss: 0.6524 ||: 100%|██████████| 98/98 [00:01<00:00, 73.29it/s]\n",
      "accuracy: 0.8182, loss: 0.6050 ||: 100%|██████████| 25/25 [00:00<00:00, 150.56it/s]\n",
      "accuracy: 0.8150, loss: 0.6206 ||: 100%|██████████| 98/98 [00:01<00:00, 68.18it/s]\n",
      "accuracy: 0.8068, loss: 0.6121 ||: 100%|██████████| 25/25 [00:00<00:00, 160.83it/s]\n",
      "accuracy: 0.8196, loss: 0.5979 ||: 100%|██████████| 98/98 [00:01<00:00, 74.04it/s]\n",
      "accuracy: 0.8383, loss: 0.5565 ||: 100%|██████████| 25/25 [00:00<00:00, 157.76it/s]\n",
      "accuracy: 0.8316, loss: 0.5665 ||: 100%|██████████| 98/98 [00:01<00:00, 73.83it/s]\n",
      "accuracy: 0.8386, loss: 0.5308 ||: 100%|██████████| 25/25 [00:00<00:00, 159.88it/s]\n",
      "accuracy: 0.8387, loss: 0.5412 ||: 100%|██████████| 98/98 [00:01<00:00, 79.55it/s]\n",
      "accuracy: 0.8489, loss: 0.5084 ||: 100%|██████████| 25/25 [00:00<00:00, 156.51it/s]\n",
      "accuracy: 0.8451, loss: 0.5193 ||: 100%|██████████| 98/98 [00:01<00:00, 72.57it/s]\n",
      "accuracy: 0.8564, loss: 0.5160 ||: 100%|██████████| 25/25 [00:00<00:00, 157.97it/s]\n",
      "accuracy: 0.8531, loss: 0.4898 ||: 100%|██████████| 98/98 [00:01<00:00, 72.72it/s]\n",
      "accuracy: 0.8677, loss: 0.4748 ||: 100%|██████████| 25/25 [00:00<00:00, 153.66it/s]\n",
      "accuracy: 0.8632, loss: 0.4638 ||: 100%|██████████| 98/98 [00:01<00:00, 73.44it/s]\n",
      "accuracy: 0.8500, loss: 0.4699 ||: 100%|██████████| 25/25 [00:00<00:00, 155.61it/s]\n",
      "accuracy: 0.8646, loss: 0.4510 ||: 100%|██████████| 98/98 [00:01<00:00, 73.99it/s]\n",
      "accuracy: 0.8721, loss: 0.4310 ||: 100%|██████████| 25/25 [00:00<00:00, 151.38it/s]\n",
      "accuracy: 0.8739, loss: 0.4245 ||: 100%|██████████| 98/98 [00:01<00:00, 73.61it/s]\n",
      "accuracy: 0.8841, loss: 0.4379 ||: 100%|██████████| 25/25 [00:00<00:00, 146.88it/s]\n",
      "accuracy: 0.8793, loss: 0.4091 ||: 100%|██████████| 98/98 [00:01<00:00, 72.45it/s]\n",
      "accuracy: 0.8922, loss: 0.4066 ||: 100%|██████████| 25/25 [00:00<00:00, 146.63it/s]\n",
      "accuracy: 0.8853, loss: 0.3874 ||: 100%|██████████| 98/98 [00:01<00:00, 73.26it/s]\n",
      "accuracy: 0.8657, loss: 0.4178 ||: 100%|██████████| 25/25 [00:00<00:00, 153.99it/s]\n",
      "accuracy: 0.8889, loss: 0.3703 ||: 100%|██████████| 98/98 [00:01<00:00, 71.62it/s]\n",
      "accuracy: 0.8964, loss: 0.3735 ||: 100%|██████████| 25/25 [00:00<00:00, 156.15it/s]\n",
      "accuracy: 0.8951, loss: 0.3557 ||: 100%|██████████| 98/98 [00:01<00:00, 76.66it/s]\n",
      "accuracy: 0.9005, loss: 0.3599 ||: 100%|██████████| 25/25 [00:00<00:00, 152.64it/s]\n",
      "accuracy: 0.9005, loss: 0.3380 ||: 100%|██████████| 98/98 [00:01<00:00, 73.56it/s]\n",
      "accuracy: 0.9036, loss: 0.3510 ||: 100%|██████████| 25/25 [00:00<00:00, 152.26it/s]\n",
      "accuracy: 0.9054, loss: 0.3211 ||: 100%|██████████| 98/98 [00:01<00:00, 71.69it/s]\n",
      "accuracy: 0.9011, loss: 0.3334 ||: 100%|██████████| 25/25 [00:00<00:00, 154.01it/s]\n",
      "accuracy: 0.9106, loss: 0.3055 ||: 100%|██████████| 98/98 [00:01<00:00, 63.15it/s]\n",
      "accuracy: 0.9077, loss: 0.3243 ||: 100%|██████████| 25/25 [00:00<00:00, 137.31it/s]\n",
      "accuracy: 0.9121, loss: 0.2982 ||: 100%|██████████| 98/98 [00:01<00:00, 72.12it/s]\n",
      "accuracy: 0.9115, loss: 0.3252 ||: 100%|██████████| 25/25 [00:00<00:00, 144.18it/s]\n",
      "accuracy: 0.9167, loss: 0.2826 ||: 100%|██████████| 98/98 [00:01<00:00, 72.21it/s]\n",
      "accuracy: 0.9117, loss: 0.3031 ||: 100%|██████████| 25/25 [00:00<00:00, 150.75it/s]\n",
      "accuracy: 0.9221, loss: 0.2697 ||: 100%|██████████| 98/98 [00:01<00:00, 71.34it/s]\n",
      "accuracy: 0.9157, loss: 0.3108 ||: 100%|██████████| 25/25 [00:00<00:00, 146.63it/s]\n",
      "accuracy: 0.9242, loss: 0.2616 ||: 100%|██████████| 98/98 [00:01<00:00, 70.95it/s]\n",
      "accuracy: 0.9196, loss: 0.2870 ||: 100%|██████████| 25/25 [00:00<00:00, 152.39it/s]\n",
      "accuracy: 0.9297, loss: 0.2485 ||: 100%|██████████| 98/98 [00:01<00:00, 73.20it/s]\n",
      "accuracy: 0.9183, loss: 0.2842 ||: 100%|██████████| 25/25 [00:00<00:00, 153.92it/s]\n",
      "accuracy: 0.9330, loss: 0.2372 ||: 100%|██████████| 98/98 [00:01<00:00, 73.82it/s]\n",
      "accuracy: 0.9210, loss: 0.2748 ||: 100%|██████████| 25/25 [00:00<00:00, 156.03it/s]\n",
      "accuracy: 0.9363, loss: 0.2291 ||: 100%|██████████| 98/98 [00:01<00:00, 73.21it/s]\n",
      "accuracy: 0.9234, loss: 0.2696 ||: 100%|██████████| 25/25 [00:00<00:00, 154.24it/s]\n",
      "accuracy: 0.9369, loss: 0.2224 ||: 100%|██████████| 98/98 [00:01<00:00, 73.91it/s]\n",
      "accuracy: 0.9261, loss: 0.2619 ||: 100%|██████████| 25/25 [00:00<00:00, 148.59it/s]\n",
      "accuracy: 0.9412, loss: 0.2106 ||: 100%|██████████| 98/98 [00:01<00:00, 70.86it/s]\n",
      "accuracy: 0.9279, loss: 0.2544 ||: 100%|██████████| 25/25 [00:00<00:00, 154.99it/s]\n",
      "accuracy: 0.9440, loss: 0.2045 ||: 100%|██████████| 98/98 [00:01<00:00, 71.21it/s]\n",
      "accuracy: 0.9298, loss: 0.2491 ||: 100%|██████████| 25/25 [00:00<00:00, 150.23it/s]\n",
      "accuracy: 0.9462, loss: 0.1968 ||: 100%|██████████| 98/98 [00:01<00:00, 74.40it/s]\n",
      "accuracy: 0.9326, loss: 0.2442 ||: 100%|██████████| 25/25 [00:00<00:00, 150.35it/s]\n",
      "accuracy: 0.9490, loss: 0.1897 ||: 100%|██████████| 98/98 [00:01<00:00, 73.08it/s]\n",
      "accuracy: 0.9330, loss: 0.2420 ||: 100%|██████████| 25/25 [00:00<00:00, 148.48it/s]\n",
      "accuracy: 0.9511, loss: 0.1836 ||: 100%|██████████| 98/98 [00:01<00:00, 74.18it/s]\n",
      "accuracy: 0.9347, loss: 0.2368 ||: 100%|██████████| 25/25 [00:00<00:00, 150.97it/s]\n",
      "accuracy: 0.9531, loss: 0.1777 ||: 100%|██████████| 98/98 [00:01<00:00, 74.09it/s]\n",
      "accuracy: 0.9366, loss: 0.2309 ||: 100%|██████████| 25/25 [00:00<00:00, 138.29it/s]\n",
      "accuracy: 0.9547, loss: 0.1722 ||: 100%|██████████| 98/98 [00:01<00:00, 72.75it/s]\n",
      "accuracy: 0.9352, loss: 0.2306 ||: 100%|██████████| 25/25 [00:00<00:00, 155.03it/s]\n",
      "accuracy: 0.9566, loss: 0.1664 ||: 100%|██████████| 98/98 [00:01<00:00, 72.57it/s]\n",
      "accuracy: 0.9363, loss: 0.2261 ||: 100%|██████████| 25/25 [00:00<00:00, 151.13it/s]\n",
      "accuracy: 0.9576, loss: 0.1639 ||: 100%|██████████| 98/98 [00:01<00:00, 72.29it/s]\n",
      "accuracy: 0.9352, loss: 0.2267 ||: 100%|██████████| 25/25 [00:00<00:00, 156.43it/s]\n",
      "accuracy: 0.9599, loss: 0.1573 ||: 100%|██████████| 98/98 [00:01<00:00, 72.37it/s]\n",
      "accuracy: 0.9383, loss: 0.2218 ||: 100%|██████████| 25/25 [00:00<00:00, 161.63it/s]\n",
      "accuracy: 0.9611, loss: 0.1528 ||: 100%|██████████| 98/98 [00:01<00:00, 74.02it/s]\n",
      "accuracy: 0.9393, loss: 0.2141 ||: 100%|██████████| 25/25 [00:00<00:00, 153.22it/s]\n",
      "accuracy: 0.9619, loss: 0.1480 ||: 100%|██████████| 98/98 [00:01<00:00, 73.02it/s]\n",
      "accuracy: 0.9384, loss: 0.2164 ||: 100%|██████████| 25/25 [00:00<00:00, 153.97it/s]\n",
      "accuracy: 0.9634, loss: 0.1440 ||: 100%|██████████| 98/98 [00:01<00:00, 73.96it/s]\n",
      "accuracy: 0.9400, loss: 0.2092 ||: 100%|██████████| 25/25 [00:00<00:00, 152.74it/s]\n",
      "accuracy: 0.9647, loss: 0.1406 ||: 100%|██████████| 98/98 [00:01<00:00, 74.02it/s]\n",
      "accuracy: 0.9398, loss: 0.2086 ||: 100%|██████████| 25/25 [00:00<00:00, 159.79it/s]\n",
      "accuracy: 0.9650, loss: 0.1370 ||: 100%|██████████| 98/98 [00:01<00:00, 73.61it/s]\n",
      "accuracy: 0.9382, loss: 0.2135 ||: 100%|██████████| 25/25 [00:00<00:00, 148.65it/s]\n",
      "accuracy: 0.9656, loss: 0.1339 ||: 100%|██████████| 98/98 [00:01<00:00, 73.24it/s]\n",
      "accuracy: 0.9407, loss: 0.2043 ||: 100%|██████████| 25/25 [00:00<00:00, 151.87it/s]\n",
      "accuracy: 0.9677, loss: 0.1302 ||: 100%|██████████| 98/98 [00:01<00:00, 77.41it/s]\n",
      "accuracy: 0.9397, loss: 0.2072 ||: 100%|██████████| 25/25 [00:00<00:00, 153.48it/s]\n",
      "accuracy: 0.9680, loss: 0.1271 ||: 100%|██████████| 98/98 [00:01<00:00, 72.85it/s]\n",
      "accuracy: 0.9413, loss: 0.1997 ||: 100%|██████████| 25/25 [00:00<00:00, 151.83it/s]\n",
      "accuracy: 0.9693, loss: 0.1237 ||: 100%|██████████| 98/98 [00:01<00:00, 72.58it/s]\n",
      "accuracy: 0.9410, loss: 0.2010 ||: 100%|██████████| 25/25 [00:00<00:00, 147.92it/s]\n",
      "accuracy: 0.9692, loss: 0.1219 ||: 100%|██████████| 98/98 [00:01<00:00, 74.12it/s]\n",
      "accuracy: 0.9393, loss: 0.2107 ||: 100%|██████████| 25/25 [00:00<00:00, 149.35it/s]\n",
      "accuracy: 0.9705, loss: 0.1187 ||: 100%|██████████| 98/98 [00:01<00:00, 73.78it/s]\n",
      "accuracy: 0.9421, loss: 0.1958 ||: 100%|██████████| 25/25 [00:00<00:00, 149.12it/s]\n",
      "accuracy: 0.9699, loss: 0.1173 ||: 100%|██████████| 98/98 [00:01<00:00, 72.78it/s]\n",
      "accuracy: 0.9427, loss: 0.1945 ||: 100%|██████████| 25/25 [00:00<00:00, 139.60it/s]\n",
      "accuracy: 0.9719, loss: 0.1138 ||: 100%|██████████| 98/98 [00:01<00:00, 72.04it/s]\n",
      "accuracy: 0.9434, loss: 0.1933 ||: 100%|██████████| 25/25 [00:00<00:00, 146.28it/s]\n",
      "accuracy: 0.9716, loss: 0.1119 ||: 100%|██████████| 98/98 [00:01<00:00, 72.12it/s]\n",
      "accuracy: 0.9433, loss: 0.1906 ||: 100%|██████████| 25/25 [00:00<00:00, 151.78it/s]\n",
      "accuracy: 0.9729, loss: 0.1097 ||: 100%|██████████| 98/98 [00:01<00:00, 71.54it/s]\n",
      "accuracy: 0.9432, loss: 0.1924 ||: 100%|██████████| 25/25 [00:00<00:00, 154.39it/s]\n",
      "accuracy: 0.9729, loss: 0.1081 ||: 100%|██████████| 98/98 [00:01<00:00, 72.54it/s]\n",
      "accuracy: 0.9431, loss: 0.1909 ||: 100%|██████████| 25/25 [00:00<00:00, 153.72it/s]\n",
      "accuracy: 0.9735, loss: 0.1055 ||: 100%|██████████| 98/98 [00:01<00:00, 73.58it/s]\n",
      "accuracy: 0.9443, loss: 0.1892 ||: 100%|██████████| 25/25 [00:00<00:00, 146.03it/s]\n",
      "accuracy: 0.9743, loss: 0.1037 ||: 100%|██████████| 98/98 [00:01<00:00, 72.94it/s]\n",
      "accuracy: 0.9445, loss: 0.1898 ||: 100%|██████████| 25/25 [00:00<00:00, 155.59it/s]\n",
      "accuracy: 0.9742, loss: 0.1020 ||: 100%|██████████| 98/98 [00:01<00:00, 72.11it/s]\n",
      "accuracy: 0.9449, loss: 0.1868 ||: 100%|██████████| 25/25 [00:00<00:00, 153.91it/s]\n",
      "accuracy: 0.9743, loss: 0.1007 ||: 100%|██████████| 98/98 [00:01<00:00, 72.74it/s]\n",
      "accuracy: 0.9446, loss: 0.1827 ||: 100%|██████████| 25/25 [00:00<00:00, 154.71it/s]\n",
      "accuracy: 0.9753, loss: 0.0987 ||: 100%|██████████| 98/98 [00:01<00:00, 70.97it/s]\n",
      "accuracy: 0.9450, loss: 0.1813 ||: 100%|██████████| 25/25 [00:00<00:00, 157.36it/s]\n",
      "accuracy: 0.9757, loss: 0.0965 ||: 100%|██████████| 98/98 [00:01<00:00, 72.07it/s]\n",
      "accuracy: 0.9456, loss: 0.1838 ||: 100%|██████████| 25/25 [00:00<00:00, 146.55it/s]\n",
      "accuracy: 0.9756, loss: 0.0952 ||: 100%|██████████| 98/98 [00:01<00:00, 71.92it/s]\n",
      "accuracy: 0.9450, loss: 0.1844 ||: 100%|██████████| 25/25 [00:00<00:00, 151.25it/s]\n",
      "accuracy: 0.9764, loss: 0.0939 ||: 100%|██████████| 98/98 [00:01<00:00, 71.82it/s]\n",
      "accuracy: 0.9456, loss: 0.1790 ||: 100%|██████████| 25/25 [00:00<00:00, 148.78it/s]\n",
      "accuracy: 0.9761, loss: 0.0929 ||: 100%|██████████| 98/98 [00:01<00:00, 71.98it/s]\n",
      "accuracy: 0.9442, loss: 0.1828 ||: 100%|██████████| 25/25 [00:00<00:00, 150.96it/s]\n",
      "accuracy: 0.9770, loss: 0.0907 ||: 100%|██████████| 98/98 [00:01<00:00, 72.20it/s]\n",
      "accuracy: 0.9449, loss: 0.1840 ||: 100%|██████████| 25/25 [00:00<00:00, 155.30it/s]\n",
      "accuracy: 0.9771, loss: 0.0893 ||: 100%|██████████| 98/98 [00:01<00:00, 63.09it/s]\n",
      "accuracy: 0.9455, loss: 0.1775 ||: 100%|██████████| 25/25 [00:00<00:00, 149.14it/s]\n",
      "accuracy: 0.9772, loss: 0.0882 ||: 100%|██████████| 98/98 [00:01<00:00, 72.61it/s]\n",
      "accuracy: 0.9454, loss: 0.1817 ||: 100%|██████████| 25/25 [00:00<00:00, 151.15it/s]\n",
      "accuracy: 0.9778, loss: 0.0871 ||: 100%|██████████| 98/98 [00:01<00:00, 72.96it/s]\n",
      "accuracy: 0.9446, loss: 0.1804 ||: 100%|██████████| 25/25 [00:00<00:00, 148.57it/s]\n",
      "accuracy: 0.9780, loss: 0.0857 ||: 100%|██████████| 98/98 [00:01<00:00, 72.06it/s]\n",
      "accuracy: 0.9450, loss: 0.1778 ||: 100%|██████████| 25/25 [00:00<00:00, 151.89it/s]\n",
      "accuracy: 0.9779, loss: 0.0847 ||: 100%|██████████| 98/98 [00:01<00:00, 71.42it/s]\n",
      "accuracy: 0.9457, loss: 0.1800 ||: 100%|██████████| 25/25 [00:00<00:00, 150.04it/s]\n",
      "accuracy: 0.9786, loss: 0.0833 ||: 100%|██████████| 98/98 [00:01<00:00, 73.29it/s]\n",
      "accuracy: 0.9457, loss: 0.1771 ||: 100%|██████████| 25/25 [00:00<00:00, 148.26it/s]\n",
      "accuracy: 0.9786, loss: 0.0824 ||: 100%|██████████| 98/98 [00:01<00:00, 72.68it/s]\n",
      "accuracy: 0.9458, loss: 0.1794 ||: 100%|██████████| 25/25 [00:00<00:00, 152.90it/s]\n",
      "accuracy: 0.9788, loss: 0.0820 ||: 100%|██████████| 98/98 [00:01<00:00, 73.74it/s]\n",
      "accuracy: 0.9465, loss: 0.1766 ||: 100%|██████████| 25/25 [00:00<00:00, 154.12it/s]\n",
      "accuracy: 0.9787, loss: 0.0808 ||: 100%|██████████| 98/98 [00:01<00:00, 78.76it/s]\n",
      "accuracy: 0.9459, loss: 0.1765 ||: 100%|██████████| 25/25 [00:00<00:00, 156.65it/s]\n",
      "accuracy: 0.9788, loss: 0.0794 ||: 100%|██████████| 98/98 [00:01<00:00, 73.64it/s]\n",
      "accuracy: 0.9467, loss: 0.1747 ||: 100%|██████████| 25/25 [00:00<00:00, 156.21it/s]\n",
      "accuracy: 0.9792, loss: 0.0790 ||: 100%|██████████| 98/98 [00:01<00:00, 72.70it/s]\n",
      "accuracy: 0.9465, loss: 0.1805 ||: 100%|██████████| 25/25 [00:00<00:00, 142.44it/s]\n",
      "accuracy: 0.9793, loss: 0.0777 ||: 100%|██████████| 98/98 [00:01<00:00, 72.67it/s]\n",
      "accuracy: 0.9469, loss: 0.1790 ||: 100%|██████████| 25/25 [00:00<00:00, 155.57it/s]\n",
      "accuracy: 0.9797, loss: 0.0771 ||: 100%|██████████| 98/98 [00:01<00:00, 72.51it/s]\n",
      "accuracy: 0.9470, loss: 0.1740 ||: 100%|██████████| 25/25 [00:00<00:00, 150.08it/s]\n",
      "accuracy: 0.9799, loss: 0.0758 ||: 100%|██████████| 98/98 [00:01<00:00, 72.56it/s]\n",
      "accuracy: 0.9468, loss: 0.1758 ||: 100%|██████████| 25/25 [00:00<00:00, 154.44it/s]\n",
      "accuracy: 0.9802, loss: 0.0752 ||: 100%|██████████| 98/98 [00:01<00:00, 71.78it/s]\n",
      "accuracy: 0.9471, loss: 0.1747 ||: 100%|██████████| 25/25 [00:00<00:00, 157.66it/s]\n",
      "accuracy: 0.9801, loss: 0.0744 ||: 100%|██████████| 98/98 [00:01<00:00, 73.32it/s]\n",
      "accuracy: 0.9469, loss: 0.1751 ||: 100%|██████████| 25/25 [00:00<00:00, 160.02it/s]\n",
      "accuracy: 0.9806, loss: 0.0733 ||: 100%|██████████| 98/98 [00:01<00:00, 75.63it/s]\n",
      "accuracy: 0.9466, loss: 0.1728 ||: 100%|██████████| 25/25 [00:00<00:00, 158.49it/s]\n",
      "accuracy: 0.9806, loss: 0.0723 ||: 100%|██████████| 98/98 [00:01<00:00, 73.59it/s]\n",
      "accuracy: 0.9457, loss: 0.1726 ||: 100%|██████████| 25/25 [00:00<00:00, 152.15it/s]\n",
      "accuracy: 0.9807, loss: 0.0720 ||: 100%|██████████| 98/98 [00:01<00:00, 70.88it/s]\n",
      "accuracy: 0.9477, loss: 0.1738 ||: 100%|██████████| 25/25 [00:00<00:00, 157.28it/s]\n",
      "accuracy: 0.9810, loss: 0.0709 ||: 100%|██████████| 98/98 [00:01<00:00, 73.36it/s]\n",
      "accuracy: 0.9459, loss: 0.1724 ||: 100%|██████████| 25/25 [00:00<00:00, 149.71it/s]\n",
      "accuracy: 0.9811, loss: 0.0701 ||: 100%|██████████| 98/98 [00:01<00:00, 71.82it/s]\n",
      "accuracy: 0.9451, loss: 0.1715 ||: 100%|██████████| 25/25 [00:00<00:00, 148.34it/s]\n",
      "accuracy: 0.9811, loss: 0.0696 ||: 100%|██████████| 98/98 [00:01<00:00, 72.58it/s]\n",
      "accuracy: 0.9467, loss: 0.1736 ||: 100%|██████████| 25/25 [00:00<00:00, 151.94it/s]\n",
      "accuracy: 0.9813, loss: 0.0689 ||: 100%|██████████| 98/98 [00:01<00:00, 72.18it/s]\n",
      "accuracy: 0.9472, loss: 0.1716 ||: 100%|██████████| 25/25 [00:00<00:00, 151.02it/s]\n",
      "accuracy: 0.9814, loss: 0.0689 ||: 100%|██████████| 98/98 [00:01<00:00, 70.94it/s]\n",
      "accuracy: 0.9479, loss: 0.1732 ||: 100%|██████████| 25/25 [00:00<00:00, 152.15it/s]\n",
      "accuracy: 0.9815, loss: 0.0677 ||: 100%|██████████| 98/98 [00:01<00:00, 72.39it/s]\n",
      "accuracy: 0.9476, loss: 0.1706 ||: 100%|██████████| 25/25 [00:00<00:00, 149.15it/s]\n",
      "accuracy: 0.9819, loss: 0.0667 ||: 100%|██████████| 98/98 [00:01<00:00, 72.82it/s]\n",
      "accuracy: 0.9471, loss: 0.1728 ||: 100%|██████████| 25/25 [00:00<00:00, 159.20it/s]\n",
      "accuracy: 0.9815, loss: 0.0663 ||: 100%|██████████| 98/98 [00:01<00:00, 73.35it/s]\n",
      "accuracy: 0.9475, loss: 0.1699 ||: 100%|██████████| 25/25 [00:00<00:00, 156.72it/s]\n",
      "accuracy: 0.9820, loss: 0.0658 ||: 100%|██████████| 98/98 [00:01<00:00, 72.43it/s]\n",
      "accuracy: 0.9466, loss: 0.1778 ||: 100%|██████████| 25/25 [00:00<00:00, 158.04it/s]\n",
      "accuracy: 0.9823, loss: 0.0652 ||: 100%|██████████| 98/98 [00:01<00:00, 72.55it/s]\n",
      "accuracy: 0.9491, loss: 0.1703 ||: 100%|██████████| 25/25 [00:00<00:00, 159.27it/s]\n",
      "accuracy: 0.9824, loss: 0.0642 ||: 100%|██████████| 98/98 [00:01<00:00, 73.42it/s]\n",
      "accuracy: 0.9478, loss: 0.1709 ||: 100%|██████████| 25/25 [00:00<00:00, 151.79it/s]\n",
      "accuracy: 0.9822, loss: 0.0640 ||: 100%|██████████| 98/98 [00:01<00:00, 71.39it/s]\n",
      "accuracy: 0.9488, loss: 0.1733 ||: 100%|██████████| 25/25 [00:00<00:00, 147.76it/s]\n",
      "accuracy: 0.9828, loss: 0.0630 ||: 100%|██████████| 98/98 [00:01<00:00, 75.60it/s]\n",
      "accuracy: 0.9478, loss: 0.1664 ||: 100%|██████████| 25/25 [00:00<00:00, 153.89it/s]\n",
      "accuracy: 0.9829, loss: 0.0627 ||: 100%|██████████| 98/98 [00:01<00:00, 74.06it/s]\n",
      "accuracy: 0.9486, loss: 0.1746 ||: 100%|██████████| 25/25 [00:00<00:00, 153.31it/s]\n",
      "accuracy: 0.9830, loss: 0.0620 ||: 100%|██████████| 98/98 [00:01<00:00, 80.59it/s]\n",
      "accuracy: 0.9488, loss: 0.1732 ||: 100%|██████████| 25/25 [00:00<00:00, 158.77it/s]\n",
      "accuracy: 0.9829, loss: 0.0616 ||: 100%|██████████| 98/98 [00:01<00:00, 71.72it/s]\n",
      "accuracy: 0.9489, loss: 0.1761 ||: 100%|██████████| 25/25 [00:00<00:00, 155.37it/s]\n",
      "accuracy: 0.9829, loss: 0.0613 ||: 100%|██████████| 98/98 [00:01<00:00, 73.23it/s]\n",
      "accuracy: 0.9497, loss: 0.1724 ||: 100%|██████████| 25/25 [00:00<00:00, 157.31it/s]\n",
      "accuracy: 0.9833, loss: 0.0604 ||: 100%|██████████| 98/98 [00:01<00:00, 73.72it/s]\n",
      "accuracy: 0.9484, loss: 0.1680 ||: 100%|██████████| 25/25 [00:00<00:00, 151.24it/s]\n",
      "accuracy: 0.9834, loss: 0.0602 ||: 100%|██████████| 98/98 [00:01<00:00, 72.09it/s]\n",
      "accuracy: 0.9497, loss: 0.1716 ||: 100%|██████████| 25/25 [00:00<00:00, 153.40it/s]\n",
      "accuracy: 0.9836, loss: 0.0594 ||: 100%|██████████| 98/98 [00:01<00:00, 72.64it/s]\n",
      "accuracy: 0.9491, loss: 0.1717 ||: 100%|██████████| 25/25 [00:00<00:00, 156.41it/s]\n",
      "accuracy: 0.9834, loss: 0.0589 ||: 100%|██████████| 98/98 [00:01<00:00, 73.26it/s]\n",
      "accuracy: 0.9482, loss: 0.1699 ||: 100%|██████████| 25/25 [00:00<00:00, 156.92it/s]\n",
      "accuracy: 0.9837, loss: 0.0584 ||: 100%|██████████| 98/98 [00:01<00:00, 74.24it/s]\n",
      "accuracy: 0.9493, loss: 0.1698 ||: 100%|██████████| 25/25 [00:00<00:00, 155.08it/s]\n",
      "accuracy: 0.9839, loss: 0.0577 ||: 100%|██████████| 98/98 [00:01<00:00, 72.72it/s]\n",
      "accuracy: 0.9494, loss: 0.1713 ||: 100%|██████████| 25/25 [00:00<00:00, 150.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 134,\n",
       " 'best_validation_accuracy': 0.9477555922534154,\n",
       " 'best_validation_loss': 0.16635991930961608,\n",
       " 'epoch': 143,\n",
       " 'peak_cpu_memory_MB': 2129.736,\n",
       " 'peak_gpu_0_memory_MB': 389,\n",
       " 'training_accuracy': 0.9837160596334255,\n",
       " 'training_cpu_memory_MB': 2129.736,\n",
       " 'training_duration': '0:03:46.738170',\n",
       " 'training_epochs': 143,\n",
       " 'training_gpu_0_memory_MB': 389,\n",
       " 'training_loss': 0.05842211561239496,\n",
       " 'training_start_epoch': 0,\n",
       " 'validation_accuracy': 0.9492568683380874,\n",
       " 'validation_loss': 0.16979450821876527}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=num_epochs,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUno08VcGeDf"
   },
   "source": [
    "The `patience` parameter tells the model to stop training early if it ever spends 10 epochs without the validation loss improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhQlU8uSGeDg"
   },
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUELfOMqGeDh"
   },
   "source": [
    "To look at the predictions that our model is making AllenNLP contains a Predictor abstraction that takes inputs, converts them to Instances, feeds them through our model, and returns JSON-serializable results.\n",
    "\n",
    "Often we would need to implement our own Predictor, but AllenNLP already has a `SentenceTaggerPredictor` that works perfectly here, so we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jHOsYtCoGeDi",
    "outputId": "ee6f48cd-ebe1-4769-c9b5-8f1033cbf412"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.796961784362793,\n",
       "  -5.007230281829834,\n",
       "  -15.54598617553711,\n",
       "  9.824734687805176,\n",
       "  16.484712600708008,\n",
       "  -12.386752128601074,\n",
       "  9.272146224975586,\n",
       "  -4.658377647399902,\n",
       "  -4.870985984802246,\n",
       "  4.820064544677734,\n",
       "  -6.4194793701171875,\n",
       "  -2.0905094146728516],\n",
       " [7.959966659545898,\n",
       "  3.6185107231140137,\n",
       "  -7.2956743240356445,\n",
       "  -0.41348427534103394,\n",
       "  -0.6415822505950928,\n",
       "  -1.552140474319458,\n",
       "  5.120409965515137,\n",
       "  0.7968708872795105,\n",
       "  -3.256251335144043,\n",
       "  1.9163012504577637,\n",
       "  -2.056368112564087,\n",
       "  -4.145318984985352],\n",
       " [7.676870346069336,\n",
       "  6.545048713684082,\n",
       "  -7.966844081878662,\n",
       "  -0.5770882368087769,\n",
       "  -4.420485019683838,\n",
       "  -0.2390516698360443,\n",
       "  3.7002615928649902,\n",
       "  2.178879499435425,\n",
       "  -1.561387538909912,\n",
       "  2.284492254257202,\n",
       "  -2.8406355381011963,\n",
       "  -5.06860876083374],\n",
       " [17.334558486938477,\n",
       "  -7.458230972290039,\n",
       "  -28.466014862060547,\n",
       "  15.429902076721191,\n",
       "  27.2178897857666,\n",
       "  -22.110519409179688,\n",
       "  18.125486373901367,\n",
       "  -6.4928765296936035,\n",
       "  -6.579277515411377,\n",
       "  9.070069313049316,\n",
       "  -12.107759475708008,\n",
       "  -3.9573781490325928],\n",
       " [11.888884544372559,\n",
       "  4.398420333862305,\n",
       "  -9.624578475952148,\n",
       "  -0.34324324131011963,\n",
       "  -2.5150253772735596,\n",
       "  -0.5976796746253967,\n",
       "  6.6707048416137695,\n",
       "  2.1993930339813232,\n",
       "  -3.8423495292663574,\n",
       "  1.8017785549163818,\n",
       "  -4.5237135887146,\n",
       "  -5.729985237121582]]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "\n",
    "# we can provide completely new text to the predict method and access\n",
    "# the 'class_logits' key to get the predicted class/part of speech tag for each token\n",
    "class_logits = predictor.predict('The dog ate the apple')['class_logits']\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DZivQ8JZGeDl",
    "outputId": "39dad5fa-dea5-4231-e113-0aaaa0243ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'NOUN', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "class_ids = np.argmax(class_logits, axis=-1)\n",
    "print([vocab.get_token_from_index(i, 'labels') for i in class_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1CUsyuLGeDo"
   },
   "source": [
    "## Serialization & DeSerialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndp31kCdGeDo"
   },
   "source": [
    "We would need to save both the model and the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRj3hGfAGeDp"
   },
   "outputs": [],
   "source": [
    "# Here's how to save the model.\n",
    "model_checkpoint_dir = 'models'\n",
    "if not os.path.exists(model_checkpoint_dir):\n",
    "    os.makedirs(model_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_filename = os.path.join(model_checkpoint_dir, 'model.pt')\n",
    "with open(model_filename, 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "\n",
    "vocab_filename = os.path.join(model_checkpoint_dir, 'vocabulary.allennlp')\n",
    "vocab.save_to_files(vocab_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cJdI1oeGeDq"
   },
   "outputs": [],
   "source": [
    "# And here's how to reload the model.\n",
    "vocab2 = Vocabulary.from_files(vocab_filename)\n",
    "model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
    "\n",
    "with open(model_filename, 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))\n",
    "\n",
    "if cuda_device > -1:\n",
    "    model2.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o78tSUMqGeDs"
   },
   "outputs": [],
   "source": [
    "# generate the predictions again\n",
    "predictor2 = SentenceTaggerPredictor(model2, dataset_reader=reader)\n",
    "class_logits2 = predictor2.predict(\"The dog ate the apple\")['class_logits']\n",
    "np.testing.assert_array_almost_equal(class_logits2, class_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDWzPD-3GeDv"
   },
   "source": [
    "There are a lot of different improvements that we can work on, but hopefully this gives a taste of what a NLP pipeline looks like in AllenNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMTEPTP3GeDw"
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbhrJyiyGeDx"
   },
   "source": [
    "- [Github: AllenNLP Tutorial - Getting Started with AllenNLP](https://allennlp.org/tutorials)\n",
    "- [Blog: An In-Depth Tutorial to AllenNLP (From Basics to ELMo and BERT)](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)\n",
    "- [Blog: Part-of-Speech tagging tutorial with the Keras Deep Learning library](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537)\n",
    "- [PyTorch Documentation: Saving And Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ED5a9kw2GeDN",
    "NaEdcBkjGeDW",
    "nKi2kk9gGeDX",
    "3F1Lxg28GeDZ"
   ],
   "name": "allennlp_tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
