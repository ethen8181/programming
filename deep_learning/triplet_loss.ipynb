{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5316d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/home/mingyuliu/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c1ffa7e52e41449744ccb5b160d4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_dict = load_dataset(\"fashion_mnist\")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704d81fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAAFAUlEQVR4nOWWy3JkRxGG/8yqOnX6dLe61d2jK5rRaC4OT4xjBNjAeOENCy+IYAcLvwSvw4uwYuOdWRA2wW1sgpmwZdnStNRSX8+1KpOFTDAgiTUR5LKqvszI+jMzEvh/MbrpkP/b09FbPQAg80+UiAB7i3sFJ2sH25pDBET6xt3NCEG5ff9wFFeqi1MEgEAEUYDoFoTEbjx5vzWuvJ1nlzOBgqAKgG6LQvD3nzyyCQZtPfzz72YABIBhjXozogqze7CLQNlC7yX+ZLrKl9wb9peTaXkzEgG/vdOPfdaj8uHbj06//Pbk2D16uPPt5y8vbkRIqXPv/lAkZcMLrkvpbvcfpAO+HD33R7d8srn7/YO1ShPX3pu/PLlostT1k+bryf7di+U15Eq15IOf3W0JVAXt/eEKcXJZTEP/4Q/txavbojx8mhKYhGJyZxi5PvFjMftP9745G19DrnTW5Yy4NNYYrw0AtzUoAw0H8xfL2wpGm1oQo4CIoKpIWuuEbjor420IEm/YxLoqV2aQBBCxp2htZ8PfgBAU4CzTEIGQT/2II1lDgahsuHund634iZiATq+rVYAlFWp5YueTEJP86NJsbl1DFAzsvHuvzTAMcp021FAU5bQ3//0XM85uyIUUgwdDby0gSSe1AqOqbBJfn0o2a/4TIaIAnHx+qcaxSsZNU0cm40mK896PvvrsxbH9LuV/SSIK9LZdJcRKiXVlEw1bC9R1//D01TczCwDERCAR1as2woe/6s1LdaoBpJaFJEKUum2fR1iArvyrCAAka+tr2QePJxNlo9pEw8zMFCSoMW64/cXCgljljWmw++TZ473vnc6KhBUSQrDWWpIQlG0odt/949hC43d6Z50k29q5f3dnc0RVgqJOCc5F48tz8YNeFJEwetCGBUAEbnXXRhudweHbm0kpIYr3q0aM7fiGW/nr2N8cxCrX0NttwcL2nj5ru3bq2hl41OYYgzZV7veaY+00fj11yKlDF3VQ69Y3PCw673z4075rcUMoi6qaIYIovuSdzIvRyFroukuLeUXes00NLLZ/+d6mlVhXQkIIIFg/WPzmS33vmU41HF+cm34nrRs4JinmNezw8U/2VhVUgsI4E0MIbFy69foPFblpDKtlnnVDqbCWIU1t2b7/g0E1tql3nkC2RdNprumEfvHOrz/9uN8xra2d/Q5BiCyTQF3X28xVCqmDSZkdo8kL0abk4b0f90J3q9W+szFMQgOwZVGB6a3b87OzjSzUFaXWtJLV6qxJuWnkVeejxVFrs+u0LiqJMTIbAQn1tu20oHoKk0IKKSzqAGZr1LIbOOOt5GWoyiARhtmni2J4YC/mVbE0WUKromZjybE11jDjLGnHZV3nMamXQWJsKOn0Yt7esuO//+Vwf7WYxBLGJdYYthShEut8IU0Ioo06E7RaaRrrTSuFXX31iW+BQl0TiQgDIoEZsYkCqFpETmIlxoohoHl9ZHHx26MXz9/y9fmqLomsZQMiAhGzMYYJIM7Fdrtk1rpy/Olnlpqz5Twfb6x3slpBbJmJCQ0MIjOB6KpnjWumczP+098uiVXJ+fbB85/vm9QRDDOUZVGZfBpRBSZAY6jFTf56Mnl5vApWAa3rxaKutmyvl1Jj2s0K4TynahVRRyJAoY2a+deT6VjeWAaIgOFmzyzcqDiLzWl+fVrpVcf/+/6Qeq44a3KReJ34n7d/ANjijbAVvFZRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F53F0182D10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = dataset_dict['train'][0]['image']\n",
    "print(type(temp))\n",
    "temp.resize((50, 50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78cdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=(0.286,), std=(0.353,))\n",
    "transformations = Compose([ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdc1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn(examples):\n",
    "    examples[\"pixel_values\"] = [transformations(img) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c14b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_transformed_dict = dataset_dict.with_transform(transform_fn)\n",
    "dataset_transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a55fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([4, 6]),\n",
       " 'pixel_values': tensor([[[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]],\n",
       " \n",
       " \n",
       "         [[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "data_loader = DataLoader(dataset_transformed_dict['train'], batch_size=batch_size, shuffle=True)\n",
    "examples = next(iter(data_loader))\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9615be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "class ClassificationModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU()\n",
    "                                )\n",
    "        self.fc1 = nn.Linear(256, n_classes)\n",
    "        \n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, pixel_values_batch):        \n",
    "        output = self.convnet(pixel_values_batch)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return self.fc1(output)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        loss = self.compute_metric(batch, split='train')\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.compute_metric(batch, split='val')\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('val_accuracy', self.val_accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def compute_metric(self, batch, split):\n",
    "        pixel_values_batch = batch['pixel_values']\n",
    "        labels_batch = batch['label']\n",
    "        output_batch = self(pixel_values_batch)\n",
    "        loss = F.cross_entropy(output_batch, labels_batch)\n",
    "        pred_batch = output_batch.argmax(dim=1)\n",
    "        if split == 'train':\n",
    "            self.train_accuracy.update(pred_batch, labels_batch)\n",
    "        elif split == 'val':\n",
    "            self.val_accuracy.update(pred_batch, labels_batch)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_embedding(self, pixel_values_batch):\n",
    "        with torch.no_grad():\n",
    "            output = self.convnet(pixel_values_batch)\n",
    "            output = output.view(output.size()[0], -1)\n",
    "            output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833de768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0654, -0.0521, -0.0539, -0.0142, -0.0256, -0.0109, -0.1167,  0.0472,\n",
       "         -0.0188, -0.0584],\n",
       "        [-0.0557, -0.0254, -0.0499, -0.0318, -0.0036,  0.0111, -0.0803,  0.0413,\n",
       "         -0.0222, -0.0335]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_module = ClassificationModule(\n",
    "    n_classes=10,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "outputs = classification_module(examples['pixel_values'])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8110a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data_loader = DataLoader(dataset_transformed_dict['train'], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = DataLoader(dataset_transformed_dict['test'], batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d28d4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "csv_logger = CSVLogger(save_dir=os.getcwd())\n",
    "trainer = pl.Trainer(logger=csv_logger, callbacks=callbacks, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63948511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | convnet        | Sequential | 52.1 K\n",
      "1 | fc             | Sequential | 328 K \n",
      "2 | fc1            | Linear     | 2.6 K \n",
      "3 | train_accuracy | Accuracy   | 0     \n",
      "4 | val_accuracy   | Accuracy   | 0     \n",
      "----------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.531     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c9f9c2fdf945ef993f902f7be9b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(classification_module, train_dataloaders=train_data_loader, val_dataloaders=val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54d3129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_module.get_embedding(examples['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b66285",
   "metadata": {},
   "source": [
    "## Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8d384e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([9, 0, 0, 3, 0]),\n",
       " 'pixel_values': tensor([[[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]],\n",
       " \n",
       " \n",
       "         [[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]],\n",
       " \n",
       " \n",
       "         [[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]],\n",
       " \n",
       " \n",
       "         [[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]],\n",
       " \n",
       " \n",
       "         [[[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           ...,\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
       "           [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "data_loader = DataLoader(dataset_transformed_dict['train'], batch_size=batch_size, shuffle=False)\n",
    "examples = next(iter(data_loader))\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5495b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hardest_negative(loss_values):\n",
    "#     hard_negative = np.argmax(loss_values)\n",
    "#     return hard_negative if loss_values[hard_negative] > 0 else None\n",
    "\n",
    "\n",
    "# def random_hard_negative(loss_values):\n",
    "#     hard_negatives = np.where(loss_values > 0)[0]\n",
    "#     return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n",
    "\n",
    "\n",
    "# def semihard_negative(loss_values, margin):\n",
    "#     semihard_negatives = np.where(np.logical_and(loss_values < margin, loss_values > 0))[0]\n",
    "#     return np.random.choice(semihard_negatives) if len(semihard_negatives) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c819240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_triplets(output_batch, labels_batch, margin: float = 1.0):\n",
    "    triplets = []\n",
    "    with torch.no_grad():\n",
    "        distance_matrix = torch.cdist(output_batch, output_batch)\n",
    "\n",
    "        for label in set(labels_batch.cpu().numpy()):\n",
    "            label_mask = labels_batch == label\n",
    "            positive_indices = label_mask.nonzero(as_tuple=True)[0]\n",
    "            if len(positive_indices) < 2:\n",
    "                continue\n",
    "\n",
    "            negative_indices = torch.logical_not(label_mask).nonzero(as_tuple=True)[0]\n",
    "            # sample positive pairs, combination of pairs that have the same label\n",
    "            # in a particular input batch\n",
    "            # [[1, 2],\n",
    "            #  [1, 4],\n",
    "            #  [2, 4]]\n",
    "            anchor_positives = torch.combinations(positive_indices)\n",
    "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
    "            # for every one the the anchor record, lookup the distance for all the negative indices\n",
    "            an_distances = distance_matrix[\n",
    "                anchor_positives[:, 0].unsqueeze(dim=1),\n",
    "                negative_indices.unsqueeze(dim=0)\n",
    "            ]\n",
    "            # loss values looks something like the following\n",
    "            # tensor([[1.4446, 1.2542],\n",
    "            #         [0.8806, 0.6901],\n",
    "            #         [0.9336, 1.5853]])\n",
    "            # where the number of rows is the number of anchored samples,\n",
    "            # and number of columns is the number of anchored negative samples\n",
    "            # each cell represents the triplet loss between each pair\n",
    "            loss_values = ap_distances.unsqueeze(dim=1) - an_distances + margin\n",
    "            for i, loss_value in enumerate(loss_values):\n",
    "                hard_negatives = (loss_value > 0).nonzero(as_tuple=True)[0]\n",
    "                if len(hard_negatives) > 0:\n",
    "                    anchor_positive = anchor_positives[i]\n",
    "                    hard_negative = hard_negatives[random.randrange(len(hard_negatives))]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "\n",
    "    return torch.LongTensor(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a741a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, learning_rate: float, margin: float = 1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU()\n",
    "                                )\n",
    "        \n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin)\n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, pixel_values_batch):        \n",
    "        output = self.convnet(pixel_values_batch)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        loss = self.compute_metric(batch, split='train')\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.compute_metric(batch, split='val')\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('val_accuracy', self.val_accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def compute_metric(self, batch, split):\n",
    "        pixel_values_batch = batch['pixel_values']\n",
    "        labels_batch = batch['label']\n",
    "        output_batch = self(pixel_values_batch)\n",
    "        triplets = get_triplets(output_batch, labels_batch)\n",
    "        loss = self.triplet_loss(\n",
    "            output_batch[triplets[:, 0]],\n",
    "            output_batch[triplets[:, 1]],\n",
    "            output_batch[triplets[:, 2]]\n",
    "        )\n",
    "\n",
    "        pred_batch = output_batch.argmax(dim=1)\n",
    "        if split == 'train':\n",
    "            self.train_accuracy.update(pred_batch, labels_batch)\n",
    "        elif split == 'val':\n",
    "            self.val_accuracy.update(pred_batch, labels_batch)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, pixel_values_batch):\n",
    "        with torch.no_grad():\n",
    "            return self(pixel_values_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2d318de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0975,  0.1213, -0.0118,  ...,  0.1768, -0.0196,  0.0240],\n",
       "        [ 0.0373,  0.1125, -0.0132,  ...,  0.1379, -0.0075, -0.0066],\n",
       "        [-0.0004,  0.0445, -0.0068,  ...,  0.0468,  0.0284, -0.0086],\n",
       "        [-0.0042,  0.0692, -0.0049,  ...,  0.0335,  0.0506, -0.0063],\n",
       "        [ 0.0488,  0.0666, -0.0185,  ...,  0.1263,  0.0495, -0.0051]],\n",
       "       grad_fn=<PreluBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_module = TripletModule(\n",
    "    learning_rate=0.01\n",
    ")\n",
    "output_batch = triplet_module(examples['pixel_values'])\n",
    "output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5d5f551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1],\n",
       "        [1, 4, 0],\n",
       "        [2, 4, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch = examples['label']\n",
    "loss_values = get_triplets(output_batch, labels_batch)\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d333bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data_loader = DataLoader(dataset_transformed_dict['train'],\n",
    "                               batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_data_loader = DataLoader(dataset_transformed_dict['test'],\n",
    "                             batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af7d505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "csv_logger = CSVLogger(save_dir=os.getcwd())\n",
    "trainer = pl.Trainer(logger=csv_logger, callbacks=callbacks, max_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "968c4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type              | Params\n",
      "-----------------------------------------------------\n",
      "0 | convnet        | Sequential        | 52.1 K\n",
      "1 | fc             | Sequential        | 328 K \n",
      "2 | triplet_loss   | TripletMarginLoss | 0     \n",
      "3 | train_accuracy | Accuracy          | 0     \n",
      "4 | val_accuracy   | Accuracy          | 0     \n",
      "-----------------------------------------------------\n",
      "380 K     Trainable params\n",
      "0         Non-trainable params\n",
      "380 K     Total params\n",
      "1.521     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9ccceda75244e5a0db088505603fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(triplet_module, train_dataloaders=train_data_loader, val_dataloaders=val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81e5b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b21fe",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c84ec55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02374348, -0.0330015 , -0.01327282, ..., -0.02302296,\n",
       "        -0.02591417, -0.07701071],\n",
       "       [-0.02785763, -0.03012087, -0.02293666, ..., -0.01449264,\n",
       "        -0.02499893, -0.01534667],\n",
       "       [-0.01808958, -0.02305187, -0.02700065, ..., -0.02027208,\n",
       "        -0.01377209, -0.0205295 ],\n",
       "       [-0.02022461, -0.02194866, -0.02869061, ..., -0.01896765,\n",
       "        -0.01464459, -0.02315643],\n",
       "       [-0.02493794, -0.02058965, -0.03398969, ..., -0.01907356,\n",
       "        -0.01696688, -0.03401559]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_embedding = triplet_module.get_embedding(examples['pixel_values']).cpu().numpy()\n",
    "triplet_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "081f92e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([63, 17, 17, 17, 17])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_module(examples['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36d4415c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 256)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0640b4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18590035,  0.69543445, -0.12769291, ..., -0.38221785,\n",
       "        -0.07774707, -0.14000985],\n",
       "       [-1.1597943 , -0.84390825, -0.85677   , ..., -0.39835724,\n",
       "        -0.8526008 , -0.5245402 ],\n",
       "       [-0.56920743, -0.28905666, -0.3798801 , ..., -0.24877268,\n",
       "        -0.2996576 , -0.20176551],\n",
       "       [-0.8669773 , -0.18268648, -0.19739115, ..., -0.16345558,\n",
       "        -0.25248703, -0.26821074],\n",
       "       [-0.5066965 , -0.236029  , -0.5352296 , ..., -0.18034178,\n",
       "        -0.18281743, -0.14495897]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_embedding = classification_module.get_embedding(examples['pixel_values']).cpu().numpy()\n",
    "classification_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "751b52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "def create_index(module, data_loader):\n",
    "    # https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances#how-can-i-index-vectors-for-cosine-similarity\n",
    "    faiss_index = faiss.IndexHNSWFlat(256, 32, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    for examples in data_loader:\n",
    "        embedding = module.get_embedding(examples['pixel_values']).cpu().numpy()\n",
    "        faiss.normalize_L2(embedding)\n",
    "        faiss_index.add(embedding)\n",
    "    \n",
    "    return faiss_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7f897bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984\n"
     ]
    }
   ],
   "source": [
    "faiss_index = create_index(classification_module, val_data_loader)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c56db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = dataset_transformed_dict['test'][0]['pixel_values'].unsqueeze(dim=0)\n",
    "input_embedding = classification_module.get_embedding(input_example).cpu().numpy()\n",
    "faiss.normalize_L2(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "741c66df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5151, 9363, 4320, 7402]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = faiss_index.search(input_embedding, 5)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0707ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADQklEQVR4nO2UT28cRRDFX3VXd8/szp9dZ2Mbx46dRFkFEywROPPROfAREAeUAxAlCgqxY8e7O7vT013FYSESKDYnTuQdW/17r1pdVcAnfdJ/K/r3KwaqABlkH2IPmNsNjSWira01qA4b3IwQEcD1/GmtWRSApgF3vjkAwDcnKIq9Jw1+XigAQNgfHP16KyJA+/jxtGm///NgNj9edLcgBDXF/v22PEJcJoOo9eG8WeitCIrDk73hon7yLC3Znq/bttI35c0IqaI9exA26ie7+ZrdbANrJdgbESIB9p82SxSFWaXOhNKf99bB/AMhBQFQGCjcl1/odefGwa6TQpIkYRf8R1JIASJVe/frg+tedjznhbDVISW1Xkc+fkAICgUUSgaiODm9Z9+vA5dD995WxFYBY0K7czH87feJoABUw+6TuY9JKPi4GlKWbTcQz46KDykKEBGJqipw/NWD+no9Di7wYgiMwWgGQczB568XZutOAAEqooCZnMyPwmadQtV4GcR70pSyauqH3fkYvK0HAKlsA6v5WYu3yTtbjN0gxjFIskiK4vdoDDbbUgAF4IpRVU0PJtSLLbwpAtHYjDUKSKF9pkRNw9sXb0eibKazvTs1xSihKJxxJnPt7BAFWYmyyHrTzlgUKOuRYWt5slONA5PAutJBrfZDtmYp5CgmLpxoaZuKlXg8mTY+eHaTqTMp9uoCM4laDBlFuuTKYZOsH+VYhiIw/MmsZV+VhuBSNlyWRI76ARxYYu7TomgpyzD00VpWCLd7z6a6yaUnyTEp++CgSjkZGAeS4JpibDO5aNixiX3Hp4++Ta+uuq4zksnCMrNqcFmdJGO9nwQxDqSppwLZdov33CXfch+HjKFPecg5E1TdpPCj2vG00tj7xpKRBI4ru2LDz83DR3dHDjnHzarruk4cUdHsT7ksYY+r315dzfZHSgrI6pKWo4LXL7/7oS6rnXbUNK2qCBkC+4JWby5ev62Ly6vV2c5nFPuc4rqz795dMs7PAdQn92b3H0zKMgQLKNL66vcXP/34XAHgl6OHdrXs+01W9/blxV8LdlwVVR2YrSUAKqlfL64u1wCA9rSmNGRJgNm8uPr4tvik/4f+AFw4i3E/AKgBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F537464D310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADe0lEQVR4nO2U7U9cVRDGn5kz597du7DLy7a8FGpLJdYopNqaEDCmEqPW1vgX+Mf4nxk1vsQmTaOpCqYVWqh0AXeXvSx77zkzftDGtEK/+Ul+H+fkmefM5DkHOOWU/xZ60ZmBai/rD/a0wGxmJi/u2Li4Ovh9V5/2cKYRJ0mY2ALw5s35dvfbtb+LMQI4WcIUURtbWck6qxWLqqoxagyh1BNmIU8hYvXT2apyYtEGB71eZ79zsLP9W+94FyKzKONXr/d3rFpvjqHM8373jzxvrR2cKImKsdVF7Yeqz4s9KBHXh1OJ925vHCshqGFm6eOL7UGlKhr7qiwJU1JptQbHjk9khsbSrXd761k9IQVUYzQryuTuV3sQgj2ncKbAytKVi3uHDsGxGsiDOGhgOPq3CxEr0rEPPhzqb3DmgnMKOMdQ08BDEynkOQ9KUCpu3Los23148mKRiQmlWigJjZkqBM/kzECmvrl8s/t430YSZlJjIjKNFkpQ41wVAhATw8wIMLVCcfad2fbG/nCWOkQ1UooEx4VGc/XJFAJAobCnF1R37tpybbNVDjtTIyKCGsxiWQQSVxv2AhieWVrzveX53YdpNeTiSIQZphoHhWrkEGh8TABUm95n1C8tzYZq2cTilCLLMmECIhwBgJETApir8y0BaGYhcSPcKzA8PnlmNA1He27CCwOAgtnAEE4cc1CWqXlB7fLSws7mduVSQ7wfkrxbwCdOSy/iPRNUjZw5RGPVkP/0jfjJlbcvrB20wthUEgeW9wOGaglHkKQ+EQ5lhAMERQRR7Nz7Xs4vLL8kUzJXVgZxcCSS1CqVqmdyqVDskUsFLrVISDSSs8NdyMjEdGPQHI15OzjnfFJJU2FmJ8JGlKQZ5UbGCmJ26pLzLdGiX/rUUdmMVAZyKUdVYmFVqtVGPLB7kLOYKicW/dlPrtDs7LVRmTuf1ZtJVAX5cEQSzTszGnS73U7j1bRXqUIjcDRo0ufrsrV9m/j6cnOumRXsCIjeexgBwOadr+/+uvjZ65wJAO0eHo7zox//iuTkhdqZubSYnnGG+vTe5rnJnScdVNa/u7/VPvvRW/NPHlh2ZnD/QEdw59E/KWbFpQWnNHt17cs3Xvv5ly2rP3wAAHjl/btfoDZf3AMZnvtgk1EChpqdx83Rdie35LAAAKQze20g06NjHv0p/zP+BE3zpFrKGp5HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F52B8EFD210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADM0lEQVR4nO2TyY7kRBRF73sxOGyn05lZlVmVNVAUgwSIYY2AHR+AxH/yEfwEUi9oBKK7hq7uLqedTjuGxwJoBlX2Gok+q1jEiaer+wJ4wxv+AxDATEC+LAHw6+8yM/1xAlCdzjWg9z4sANnVdNjciACRBMiW6DZhnwIAKI8uDsNV14I4inHlxapXtHcKAGD9+XJS/XzVQtV3OP3gw3n/+D68TtHu7NLALT+udwfLF+Ho4l13C/9AFoIAAATF2+e2bW7Lr5TMF7y77ZoBCvvjA8guFmEYBleXsXDWBdr0zW6vQhCgXLtmYJIBw+ZOUSLbP9/unyJQq1NuO2hriQwxp13SZW33KgIqLta+70xpMqUUkY6bYPLlBA+0L0LMQHZwXA3BcYAyRiVPtpBnfVmqhxdGBHp+XOmQiAmitCZhk5sx2IOV+YdCRETMIoLy/EQGir3YFElpo7VxZW3l/NP631kEAqh8tjqpE2VjcAWDlWhB2AZnsHznB2iA8Kq935mcXK4ZrEj7aho0cwT58caXGtUqg2aRv00xWZ4X9XxuEuc8GpM7r0K7HSOPd0JJFXnlNEsE/RnGVPV8scgkbZTTEtjllpTvXo6JQ0cmak2F0ylhMjOIwtpY6zKX0RASqG+SLKq5a8HKaAo0U3nsXUpRJy6P1yb5qGyeGRJO3ntW5D2czi2r6CVj6lEYvRm8RK/hPjqbxl0fYHIWn6wikNJKa+um/Dza1Og8DjspxO+2NQP6/PSLBYZtu/OJY/AEJtbKZjZzLs+81OlOTdiPkqvgw4It9Lcnn+GlH+6btmnGRForEJmssNpk1bSc1OHXUJZuDBx3s+4yVKyvMCs5r2Z+3DYJu0BMpGw+KazNyhxxm30y3LkqBollVh6nk7X+7vD99y6X9fTAcXLqxf2QFLMtJkWWWRWunzxafROfbUESWRut1dGFHq/Hm0eTPC/q+Wx1wJOSNYNkc91s2nbTvLie316WoWEO4K714fEVvSp+cXT21tnhNDOaU+yePfnpl6c3PQC9/vLreKtUinHb2e77H/9SUJTlxFnFDKTQbzdtFwAA6vhcdkQiEgKFm/bBT/mG/wm/ATYWZex7JZRfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F52B8EFD210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADZ0lEQVR4nO1U224URxA9VV3dc9ndWa+99hpMANsCKRgcCFJQIkXKS/KZ+YD8Rf4iLwnkwgsmWXZ3dnZm+lJ5sEkcjPmBcB5aVVKdPqdK1Q18xEdcC6LL2cFJBf5AJREAokskc/rdDHKlFvqfPF2KpzfK191VygX0/LAmKQgEqMr46PjNz2/epdDlUIFHDzPt83FpQvOaZ1n94nlzrQpAUM4efMNsq+2hTYvncbr6abPAu5RzFwoQVDG6e3vwyp5MwChltVh3mQOunRgAYO/JLNStc8n7RLmL6gRXVPB2XoQE3HlWvOmytelZEKOxavkDKqpwd453nQ6q3BDAxjDnI3dVhc51CApMnx1RssV0ZGNiRyukfFK8zxgBAClwcPr0RtepyygFpUQx+KKwAL1nyAQQeHzy5NAsWmLWGJUTaexiZklJLlv5ZwIJhydf3PI9ooISiAic2b7Pre31Pe2rqts9eXpUdZ4NEykxM7Fjv+nLqahcaAAAXSwTcPPp55/EmsREtcwsyswMv7Y79zdz+bcBAEgKoNp78njGbWQRUaGkUUlVY+j6g89ezEXPr2eo6vmiu/tf35/Mlxl5S6SAhwdpijH5dLv8EYJzMxfPgobbO/v37g3a1rMwYhcGcMaQZKnzonFodgohkOpba7xz8+j4cKyrOrFCKPS+T+KIOXVNGASYcjoRqAKQsiiGW+Nqe6usKu5giYlBMoq0WaIOqrGVg74VTGaiLNYWg6oa7sym25U0PoVgSkMRxOK0X6Nf+hSSrXIL0awUuOMHRxyts3kh1KJNzhSqwgFkmDW1q75jE8K6rouB+PVcRvuPT++GFsQE33UK8jCqAZGtIWHb1rFwErp1bKyVfj2XR4dfDRc+qBGkEGBy9ilo9EHzStiVlS46sqZrliEfjLKmWcm3e/fSIqbELnUxeWV2bIw4aweVY0LkW2HDxvsdzhhbmrN8aWyaEJSz0GhqW8+5lWy0OykKA9/89bt9aM4im0FeuE095qqUH0DDLZeNSsdqjHPsSiGmP18u62a56pqzWX5YdpvElJqmyZbeyveg/VvFYH8see5clpfUhxjql7/+8sfZqxbA7f161M6DGoQ+jlavEwGQgbGlZSPMYkQMafL1arneRACwn+662KkSUlIJv82v+S0+4n+BvwGS5pVAhMTaBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F52B8EFD210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADYElEQVR4nO2TzW8cRRDF36uu6Zmd3Xi9WW+Ck9jGGDshKBJIOcCBDyE480cjceKCkCKC8gWJ4yQkjrG9O7sz3VUcbF+Cc+DAibxbd9dPr6r1Cnind/pvxX9XvjL5a6Zvf5YAh5ufHgMBfLB57/k5COGEA5tf81nz6k57ev3J5U7Capi+fosLXeLWN/po9rTdA+keRp+vzWMxfXQ0PQdxgES5vSnlFX1/ZR9FbNrR5Y3quN/7abfBuS6Eo9jZQIr9UTVDUTRpuBxSf1ilg+5chHRH/HDDZx3qQZ+ea2EzS0G7pvPzXeBYvbE2nJlqQE7m4KLzIASFb0EQb302EYlShXkzdyUWHGrSoh4o5M3ioOpWb360pdOjFL010SJoUUYhzCeb9T8QShAJ67e3l7ujaZI0zxKjxqpXBSClyfbgzcaU1kE3vvtCZ9M2MHVtICiU4Oo5YbRZnyJnSXN4lqXVj29t/HnQsA5dMhKkm5NgspWVmgqA5MnfWk7A4Kuba737KXtRs3MSRsvBQ/aczcIgKgC4nyAZqK5t317rzQ6jFGVM3Uk04dlhcFCK0YoC8LOuAF77/lbd7SdKDCUtEwaShGUjpFCMr57MEktaJ73RpffWt/u5zSZwa5nFM2nCIAo63FjEqAAlXlzSfFxc3LqxM9zfz04hcksLwXJwc0pByzlns5AWiuXro0rjMo/ChfG4WXTJ3clEimcR5JzFvbXUGjxM+eq5YvjpFZ37uJiyVH+ZaiUgwT2IwOmp85DB1CVXwNL0WBEu9hu3QybJJEsYg0TN2RlouZtaqaFUM2hFGc2WhjpZv9p/rV2XHSkUpZIUFcLdWQgxZqaU0T1oBOpWo365vVMM5rODOdo2lvAQRCW3GeIeB73hVrh/RA3mQkvuqV3oq+HeuOqPRouckmhyCYwl4NT+kk1fPH44fTK6XVvyAh1yKMpKf3jo16+MB8voQpEXR3MQ1aAsDDK69PTRz7/89ti+vXmha400c2pvoPnZj/eW+/3JUrk86YV+HeikNfsvXzQ4eLK7u+c4TG5tMrTJuP9sT7G4exfA+uXh6vp42ItRrJ1Z8/j+r3+8Ps23/d60x636Iln98sHuWeyLOtb9KmoQuiXLx4cHx2dLNNmpcjK6GcLiwcH5q/9O/w/9DcvMsW7QP5J/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F536A3AA3D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in indices[0]:\n",
    "    index = int(index)\n",
    "    dataset_dict['test'][index]['image'].resize((50, 50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccb83425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984\n"
     ]
    }
   ],
   "source": [
    "faiss_index = create_index(triplet_module, val_data_loader)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2e506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = dataset_transformed_dict['test'][0]['pixel_values'].unsqueeze(dim=0)\n",
    "input_embedding = triplet_module.get_embedding(input_example).cpu().numpy()\n",
    "faiss.normalize_L2(input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcc78829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5788, 8871, 6052, 9034]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = faiss_index.search(input_embedding, 5)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f226bdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADQklEQVR4nO2UT28cRRDFX3VXd8/szp9dZ2Mbx46dRFkFEywROPPROfAREAeUAxAlCgqxY8e7O7vT013FYSESKDYnTuQdW/17r1pdVcAnfdJ/K/r3KwaqABlkH2IPmNsNjSWira01qA4b3IwQEcD1/GmtWRSApgF3vjkAwDcnKIq9Jw1+XigAQNgfHP16KyJA+/jxtGm///NgNj9edLcgBDXF/v22PEJcJoOo9eG8WeitCIrDk73hon7yLC3Znq/bttI35c0IqaI9exA26ie7+ZrdbANrJdgbESIB9p82SxSFWaXOhNKf99bB/AMhBQFQGCjcl1/odefGwa6TQpIkYRf8R1JIASJVe/frg+tedjznhbDVISW1Xkc+fkAICgUUSgaiODm9Z9+vA5dD995WxFYBY0K7czH87feJoABUw+6TuY9JKPi4GlKWbTcQz46KDykKEBGJqipw/NWD+no9Di7wYgiMwWgGQczB568XZutOAAEqooCZnMyPwmadQtV4GcR70pSyauqH3fkYvK0HAKlsA6v5WYu3yTtbjN0gxjFIskiK4vdoDDbbUgAF4IpRVU0PJtSLLbwpAtHYjDUKSKF9pkRNw9sXb0eibKazvTs1xSihKJxxJnPt7BAFWYmyyHrTzlgUKOuRYWt5slONA5PAutJBrfZDtmYp5CgmLpxoaZuKlXg8mTY+eHaTqTMp9uoCM4laDBlFuuTKYZOsH+VYhiIw/MmsZV+VhuBSNlyWRI76ARxYYu7TomgpyzD00VpWCLd7z6a6yaUnyTEp++CgSjkZGAeS4JpibDO5aNixiX3Hp4++Ta+uuq4zksnCMrNqcFmdJGO9nwQxDqSppwLZdov33CXfch+HjKFPecg5E1TdpPCj2vG00tj7xpKRBI4ru2LDz83DR3dHDjnHzarruk4cUdHsT7ksYY+r315dzfZHSgrI6pKWo4LXL7/7oS6rnXbUNK2qCBkC+4JWby5ev62Ly6vV2c5nFPuc4rqz795dMs7PAdQn92b3H0zKMgQLKNL66vcXP/34XAHgl6OHdrXs+01W9/blxV8LdlwVVR2YrSUAKqlfL64u1wCA9rSmNGRJgNm8uPr4tvik/4f+AFw4i3E/AKgBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F537491C5D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADfklEQVR4nO2US2/cNhDH50FS1GNXluTdtWNv4thGC8SX9FikhwL9HP2W/Qw99NBrHyiQwEib1zprr3allURyerCbookd9J78DwRBzo8zA/4xAJ/16Qr/Z4QAAMRJ2wDQXYH4bkNEhIgAUJ1NAUDdgQiiXG/k38PdJ/DsbgQEUYhEBBBvqsP5o98+kgVBgE+Om8UiFJbThBCS8nB59TFEQBenj1evLs39BPMRu5WZhb8alA8RBAFACADZ2aHOi3sPjkyrNS1+fr0w2rK7uxfQ5UFlvLn/aE4bRBTcbHbiRN2CyM3CxSxLojeXy7fILTMtLpZqnMS6k1uzoADvnMx5oDYYG0Q8RtHQaMc2bt1tCKJgPPui7Nb9NsrGKABqvGWPLthx7ej9aESQALQ3j5XlAIRIKEBIcRKhy2bRh4ZBBEBdzvfRRxqJQQgDoICJY9gme/aW9gXAHh3vZUEEAUlpFpTAHLwZ1rrQ7yMCACatjo8sOAhEREwgIAEZQLqQFhGom1oE8MaBan9+mIUWCQKyYgLnMfiAker7qNhJr7MIgFx/CNs4359lNKDSigAAIQxBADzko1eOKR6PFP7H3tnsYMfKUkeaIw3gRSCEAMQedsvnfQaUT9T16ypiIZWk5U6qxQuqJKKArDQ5LKBXJg4KkVWsGZUAIEVJakDZajflph3IGJvq3rMxVrfbEh1Ct77aMBmrfatAV9NKPCpjNPk1IDgWVhCEDYXcXPy6cIvLtWter3yEQ1/XirIHJ3vbVSOpVc2qkwRJJ3EEgQiHYPHlIh4uXi464DiNFPvgVDY73UXEdnAhNRkOntN8YtEHGlb9AOA8IPSalYlMFE/0pFCnD78yqwaHjWycplh7itKM+96R82yAjXTCSqMyFExWcVmpbx5+7f6sV9Plm7dLOx6bzivZyrbuJM6r3XR8L27WtTYI7FvgvMkqdR7K3Eynfr24XKMZqa4XmyGCsqM8jRi2Ps8dISKG3oeMi1L9UP5y9uVBZaFDaluUvh/Y2rQoSwvNRf3ip/7x0cS3RIgKB+QoRoDJ/l4xKkZpNU0MEQggwrBpunpZ19vLZ+7wyXf9iyB+6Pqern48/2eOFpPp/Phw1yijFPh28fsf5+fPb5z07ffbp4Prm7rr+6unm3dj3MbZKLNEjATitpdX9drfXFWnvg4huMEH3ze3DYvP+mT0N1iAhNgLI+oCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F5374917E90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAADXklEQVR4nO1TS2+cRRCs7nl9z32vE++uHRRsBYKCBBIIzogbv5W/gJA48AcizgQrCRBn3/u9ZqY52AEHcG6ccJ1mWl1d1aMp4A53+G9B76gLoFR7dSWIYDBLmmdbffswAZlZ/nzrrzgBGD8plq9vpQhIcPTRVydPf/wegAQAYL9+1eJ2FYDnX3x5MqbflxEIEcnktH/xS3XrLixCxdff6MZQ12yYLnc8PUtffftdG9+hkj2c8W/rxcl9Ohiz2sferBGpcasxEYw/n1y0NlN7mxXZcaiq9WbtAfC/tBMTBOXp++Z5U/TSEI0CKyVNtApvVAiCvw6kYkByej5+sewNhtk2om0p1FVtBhkAprcegIiZEQLSB5+eQY+HZeasdZqso3aLcQ6A32YAzCxRpScfPBrXfG/at9o5q1SS6rCn0SQlaJGr6QBIIPAAMD8/O8IS3ozv6aANiRArgps8+vinjb5eQgASEYCsc73Hj++ry0NiikG5bxRiCKxAluL8ycVGE0hE5IoFIJk/PD2eZtW2YTuYlHSoU+27oG1EVj1Lzn+AxrUzrY1R2rjB7MHxJI2NLskM5kPnJEsMCzX7KtbIFimujek075VFlvXKPNW83isqnW/K4wENfWqIxK+WGxLTH1lomEmR9QqXphqks4TFi9TRJo4pdtGmXoe267arbdAibC009T+cjaZ9rbjeVJ7DQXSmfRO5Y9stVRF8V+8Ph3VNxhiSqKFnp58MmfainFhDFKqokyyHMs4lZrUiZo6h85w7o5Ti0EHPFouiORw8B6OMoWBEuzx1REjKsN+0xrDSLtNGESQSnNPDpC4GU+q6ECJIKIEuBv3ColNOUxcVBbZpQtjW0XdkzvZ6Q247mfSd+BBCiGDmNDEmN15IW8toK08SJOyq2DWy780pzabj6XvzXpFZ9m2EYijDdlwq6TyVSbPb7PZeJLSBfa0OT1/qqqouX7yc9Ebjfq7YOGWUompXWbS7Q6tUq3uqrTuGCIUudiFqoKouf2Y3OD4aFeWgn+eJxe71oW52q81u1WDx2cJ2jTFAhCZpqz//vurlmUvSIs9THbbrqqnrqmkOHY/OJ7ptFMcoAmp+3dKNSAIAp4kj37YhdG8ylGm0LSTeCNXfwEwxys0KyT+a7vB/xR8n+4uqoOkQ2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F537491C5D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAAEyklEQVR4nO1VS49cRxX+Tr3us2/ffrfd8TjODIlN7BgBYoNYsSGwNis2+TH8ACR+QJbZsPMKiUgJSRDOIigSCXJsZcbjjGfs6enu6bmvuvVgYUdkErcjlkh8i5Kqzvn0narvqA7wf/x3oBeEiDECAVxY45xzz87FhnzmPY8HOSHgNH1pvneyPCm+R4V5H1945RLYUFUyVebw8Gix9v5ktVnFe0Sza1etuBru3/nLm79dHJyuV9zc/mAzBYAaZowFMkjOPvux6rbdumBV9oK7AFC5PVDR/cSY9vBfVDlr3KrZSCEAiCaDhrsnevarHw1Y65z3y93lZhVyoDjvAKiC6I3Z8e6KrGnnz6cQPACAZ5leLGQyiA8e8YtDXZS1qT1tVvEQk56uTqNkzO5+/ujN33BrrPUEAOw7yf7pImcTeMnlMHrw0Z//Wdc6yDJl7AtejKUXRwwikK248rPxxaJ24DLtqE0U8hQMxnmpWaQOLtx6q5k/0bziSf/Si3wZjM2qcjJMkyzvwjwGF76JXso3UTzEdLSuEUdJPovvg9mwNZEu8klfmOdRyINFSSzDeDDq5K/s/WH+w19c31vFOGvlZHu/fH5h8aTXjdNsMkYQr99/WN/ocB0RaRq/vizPNT/BP7XytZ/sTJIwzLO6CReflv2dC4fH3GuC3nvny++qeIjo0uVRFkkBXayK9NdyXR3VonXM2c7F3kNxPpvIAb3rM3vaCiUiuSxFJ4pRawq5sZ4H6fTo2yoevHflSlj4SgooxYJ4eXv8qlDGECPmmOxPzjUMkffo3LzRs8tl0ZTL42Wydeno93/8SnSl0U4F3Lp0/A0VYt4B/as/TZZepVnCHA9T7tSFIRnjiYi3RlCYnKM4ULR989XiCJ1uN1EiCNDo8a1eomtDjGBqCSLxrCKCcw60s/OD6VkRMn9aJ3GUiYYmW2+4pmHOeui6iX3ztZVE8OCif+3G0D22kdS6rsuUJGdVMnXHjTEWXmtLrDgRAMF7C4D6O9tTeVKto16KutG6qv22+9PDnZ2tcu4Zc9og4cUjAXiAlJAyne5ssbptuVRRQKY2Ipbd+t7Hx8FVtibOHSOT+Wr+tLB42OsPO4E808g6kgse9mLy3lC9gmLdl23YOOZ12XTLphRA2OuPcxn3ldeaC0ZMKu5amzDj24X/5c14+OX+/uUptY4k2sYJqNH1631aNKJtHM+DqjIspXZe9EXb+pPBrfjg89vvHfxuZKq1FXXZQMzi6UA4Es5biDBUxhmt45BLa30Uhfavjx7s3rtf1cIYB2IMENfScdg4yUPNSUWChLewKguodipgdv+dD+YAlFKVJXiAIH6eTUzBhLNNoxm32og8jgJXi06efvH+vb0v5gDAoqBeWwu5aCCIR5CB8rZtasD7RmSRYsTJrJafvvvJ2VOrmQiUMuRlKCHeHr8+G43TSMBZJ0bh/LSGUtF4/bc7/zieFwB5AM4Fl4dnrXitnH0sdg9OZ9NhN5JMSBm2aVkbUzr34MmHf78LAAwegL17J3MLI0VRgEBCcMYYKMzzTppmvZFbzr/67HHdtIx9PVPZy9tbauEYLT85/OZ3IZJQBmk392fr410AYOT8s5hKR7L01Jan7fnx+p+dx7dBmwL/y/g3iLRBGsIYNQgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F537491C5D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAAE60lEQVR4nOXUS48dRxUH8P85VdXvvn0fM+NxPC97cGzZsRM7iERIvLPKIoqQWOUrIPHY8l1YIFiCxDZISIlAKASDYtkBLGQSZ5LxjO153Nu3u6u6HlnYMojxjcQWalmnfjr/KukU8L+z6L87GwDIhXUW7B3gw9Ot1dge1V9AAojhnoaQkZpcSqd//SIC5wCAnjRZv3p2c81+NLu3gFAAFWty2jTzx2J18+UrZ1eS4y5bcBdmS+LyD4fv/e3O7Q4A8Pr3K0/dwf5+s4AQg8brW5OD6sK1uzeOgfzsC+YgmGCtX0AAiLWVz2b24lZ2+yd/Rna+3Ds+KkmlqVhAggNvbJrDrsfk/IuNv/Dtq3XjPflknCwgHhCb50ytj3cTugr/6pt+txfsjVoqFgejre0PbRV3e7yWx1vCUGDh2mw0Uv0ikpw5fctmVCOpBiJvDAVPoffRcPjIP4NQAMaTwrS+q1Xhj/ZWK/IuOBK+G6y3NT+DAOPteN5a9p2Tieq1dQHQNg7d6WvDZwQjAravu3tziiJBnFGX+xCkaznV8y/lN3ZOEEIALn4tedhLZ3ttVTGQve+Nj5Oo79aqCieCMXng+VfSI7DpdKspSpWzttVxWQgdTU4GI/bgYaWaNk6IJANCkWRvghRCcN1vfPlkMMLo+mR6pNOEOGJnRQzBIfUMlvxIbsj/JMF6XPnB+eNjnSdCKVtPq9z3LkTEHAvzQC2PT3Txorz+rfmtVghoYwLneea0BQkKQc9cOUxPPvLgmy+bex9x1Td2NmO7FDyE7QnK1Y/2ypXs3wgBCEB++Y3L+w+0ENp7zrJJpTrvQAC5dqaXRwk/JcRACJDb33kjvD/LjzRHealOLQ/cQR+xCsy+tcnSStNJEBF8ePL7ZJe+9w1//yHH3pXjkdGprJ1UjEBMcrShzg13PpUAE0JAAIjEhdfeeu7GPZ0rVkunxof7fTsTy5H1nhlRGRcrvHdHInj/OFq2/dILG+tit81Sq6OkUtPDOqecnKFYkffUHd53kZES8BwrKTnavP7aV7PuwREqVYdccDerQTKhQHGeGO379uGuK5BIAKNr57fWc12eGpq6c7HwYVAk80cupMuVgswHJXBYt45MIAFIFBsvvfL81kY6h5rP5r1UsmsVhb7lohrFSqal+WeNIp6biFgIBInLP35xqMhOdR/Aqeq9DN1BV9tstJxJEY0L3Pz57fxHX7fTJJZMFOSVr7x6Zq7bRlvrozgEB9fNZwbl0qhSYiDu/2Xnvd/cPTVXEgEsWAj53Yv5wx1jpq31yHISSmLeWFUtLaXgaGD+8Ot3j6xPBJxlR8Qs5KXNYRMFz2421zJNsiohHxdZVSpOUvP279+/uQ8kQ9/Z4L0zbdPK3nQhFYqo75w2TrogkqQcpNz1cv7pz34BACgn0JAshNfWy59eak6rpFx2bVMbr5IkYqY4Epy2n7379gcApAsiLUsdASKV5ybytx9je1IWZREPq+CFYBZMwWvT3b/1q3cAEDv0M+O9DRSxNVpi55dppPLx6pm11SrNIqkEwTazj/908++7AIVggemtf1zYOWbdiuk7dyX0LgAkS6efWx1maSSkYvTt/JMbd44fD5En9Pu/0/sN255mf/yECMIFgIQQkokIBEII3uneP/nVAKCInacQyGtLBA7+xDT/a1TDwtr/6foctitbOQW4prYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50 at 0x7F537491C5D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in indices[0]:\n",
    "    index = int(index)\n",
    "    dataset_dict['test'][index]['image'].resize((50, 50)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061805ed",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f53f4f",
   "metadata": {},
   "source": [
    "- https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/mnist-hello-world.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
