{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86fe56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2022-09-18\n",
      "\n",
      "torch       : 1.12.1\n",
      "datasets    : 2.3.2\n",
      "transformers: 4.20.1\n",
      "evaluate    : 0.2.2\n",
      "numpy       : 1.21.6\n",
      "pandas      : 1.2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import perf_counter\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -p torch,datasets,transformers,evaluate,numpy,pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1cd19",
   "metadata": {},
   "source": [
    "# Bi-Encoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36fb73",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e60dc2",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/Tevatron/msmarco-passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96a3b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset msmarco-passage (/home/mingyuliu/.cache/huggingface/datasets/Tevatron___msmarco-passage/default/0.0.1/1874f5d9ae5257b9dbc7d8f89c76f8d4c321be6b660bb5df208e5e64decfa978)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "        num_rows: 360703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "        num_rows: 40079\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "dataset = load_dataset(\"Tevatron/msmarco-passage\", split='train')\n",
    "dataset_dict = dataset.train_test_split(test_size=test_size)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4053cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': '889473',\n",
       " 'query': 'what providence is bay of fundy in',\n",
       " 'positive_passages': [{'docid': '5980953',\n",
       "   'title': '-',\n",
       "   'text': 'Over the next 150 years, a number of other French settlements and seigneuries were founded in the area occupied by present-day New Brunswick, including along the Saint John River, the upper Bay of Fundy region, in the Tantramar Marshes at Beaubassin, and finally at St. Pierre (site of present-day Bathurst).'}],\n",
       " 'negative_passages': [{'docid': '7178187',\n",
       "   'title': 'Divine providence',\n",
       "   'text': \"In theology, divine providence, or just providence, is God's intervention in the world. The term Divine Providence is also used as a title of God. A distinction is usually made between general providence, which refers to God's continuous upholding the existence and natural order of the Universe, and special providence, which refers to God's extraordinary intervention in the life of people. Miracles generally fall in the latter category.\"},\n",
       "  {'docid': '3841880',\n",
       "   'title': '-',\n",
       "   'text': 'Mayor Jorge O. Elorza joined City and State elected officials, staff from the Cityâ\\x80\\x99s Planning and Development Department, representatives from the Rhode Island Public Transit Authority (RIPTA), the Providence Foundation, the Downtown Providence Parks Conservancy (DPPC), and the Providence Parks Department, as well as business leaders and community ...'},\n",
       "  {'docid': '8801551',\n",
       "   'title': 'Providence College',\n",
       "   'text': \"For the college in Manitoba, see Providence University College and Theological Seminary. Providence College (also known as Providence or PC) is a private, coeducational, Roman Catholic university located about two miles west of downtown Providence, Rhode Island, United States, the state's capital city.\"},\n",
       "  {'docid': '409672',\n",
       "   'title': '-',\n",
       "   'text': 'ANTONINE SISTERS CANADA. To educate is to love and to hope. Such is the theme that inspired our educative mission of the Providence Foundation. May the Divine Providence, source of love and hope, keep an eye on our mission in order to guide it to its full expansion in the service of the Community of Ottawa.'},\n",
       "  {'docid': '7786076',\n",
       "   'title': 'What does the pyramid with the eye symbol thingy mean :S?',\n",
       "   'text': 'They eye is supposed to be the Eye of Providence http://en.wikipedia.org/wiki/Eye_of_Prov... The pyramid is supposed to be unfinished to represent that the objective is always a work in progress.'},\n",
       "  {'docid': '7744998',\n",
       "   'title': 'Developer Prepares ICO to Beat Ethereum in Stolen Intellectual Property Case',\n",
       "   'text': 'The ICO and a case vs. US government. In its announcement, Mimms stated that the properties of Peercover and Providence are still viable. It says Providence holds $20 million in cases and the Peercover patent is alive and well.'},\n",
       "  {'docid': '2825941',\n",
       "   'title': '-',\n",
       "   'text': 'It originated in Woonsocket, relocating once in Woonsocket, it made itâ\\x80\\x99s move to Providence in 1966, when gay bars and same sex dancing were illegal in Providence! Bob Thibeault was the owner for a brief time while in Woonsocket. Mirabar took up residence at 93 Clemence Street. There was a bar on the first floor and a dance room on the second floor. As previously mentioned it was illegal for two men or two women to dance together back then.'},\n",
       "  {'docid': '2990130',\n",
       "   'title': 'Bundled Payment Services',\n",
       "   'text': 'programs. 1  No other Third Party Administrator has the level of experience Providence has with Centers for Medicare & Medicaid (CMS) bundled payment for care improvement initiative (BPCI). 2  Providence utilizes the most advanced web- based system, LuminX, to achieve industry leading accuracy.'},\n",
       "  {'docid': '6019563',\n",
       "   'title': '-',\n",
       "   'text': '6 Unique Ways to See Whales in their Natural Habitat. Watch finbacks, humpbacks and minkes play in the powerful tides. 1  Fundy Fish Chowder The Bay of Fundy has the highest tides in the world and is the inspiration for this very delicious chowder recipe. 1  Top 10 Bay of Fundy High Tide Adventures 10 Things to do when the Tide is High on the Bay of Fundy.'},\n",
       "  {'docid': '7454870',\n",
       "   'title': '-',\n",
       "   'text': '166 County Route 65 Bernhards Bay, NY 13028: 173 County Route 65 Bernhards Bay, NY 13028: 185 County Route 65 Bernhards Bay, NY 13028: 188 County Route 65 Bernhards Bay, NY 13028: 19 County Route 65 Bernhards Bay, NY 13028: 195 County Route 65 Bernhards Bay, NY 13028: 225 County Route 65 Bernhards Bay, NY 13028: 237 County Route 65 Bernhards Bay, NY 13028'},\n",
       "  {'docid': '7068381',\n",
       "   'title': 'Providence flowers - Order Flowers From A Local Providence flower',\n",
       "   'text': \"Local florist shops in Providence, RI can usually provide same day delivery when you place your order early. For your convenience, you can place your order online through the Providence florist's Web site or use the toll free telephone number of the florist.For any Providence florist shops in RI that do not have toll free telephone numbers, we also provide the Providence flower shop's local number including area code.et one of our local Providence flower shops help you choose the perfect flower or gift selection. Our local florists know that the gift of flowers conveys your thoughts and feelings in a very special way.\"},\n",
       "  {'docid': '2798969',\n",
       "   'title': 'Average Temperatures in Nassau, New Providence, Bahamas',\n",
       "   'text': '1 The annual average temperature in Nassau, New Providence, Bahamas is fairly hot at 24.5 degrees Celsius (76.1 degrees Fahrenheit).verage Temperatures in Nassau, New Providence, Bahamas. 1  The annual average temperature in Nassau, New Providence, Bahamas is fairly hot at 24.5 degrees Celsius (76.1 degrees Fahrenheit).'},\n",
       "  {'docid': '8207083',\n",
       "   'title': '-',\n",
       "   'text': 'Oyster Bay is a bay within Jamaica and is nearby to Mountain Spring Point and Bush Cay. Oyster Bay from Mapcarta, the interactive map.'},\n",
       "  {'docid': '7768518',\n",
       "   'title': 'Market by the Bay',\n",
       "   'text': 'People found this by searching for: Market By The Bay, Market By The Bay Daphne, Market By The Bay Daphne Al, Market By The Bay Menu, Market By The Bay Menu Daphne Al, Market By The Bay Menu, Daphne, Market By The Bay Menu Daphne, Market By The Bay Daphne Al Menu, and Market By The Bay Daphne Menu.'},\n",
       "  {'docid': '8560015',\n",
       "   'title': '-',\n",
       "   'text': 'News for Immediate Release - Upper Providence Township, Montgomery Co., PA - February 24, 2017: Upper Providence Township officials have received several resident inquiries about the 2017 County and Township Real Estate Tax bills that were mailed out this week.'},\n",
       "  {'docid': '5958018',\n",
       "   'title': '-',\n",
       "   'text': 'City of New Providence, NJ Zip Codes. City of New Providence, NJ Demographic Information. * Demographic data is based on information taken from the 2000 Census. City of New Providence, NJ covers 2 Area Codes. City of New Providence, NJ covers 1 Zip Code. 99 Cities within 15 Miles of the City of New Providence, NJ.'},\n",
       "  {'docid': '1103666',\n",
       "   'title': 'Canadaâs Regions',\n",
       "   'text': 'Nova Scotia. Nova Scotia is the most populous Atlantic Province, with a rich history as the gateway to Canada. Known for the worldâ\\x80\\x99s highest tides in the Bay of Fundy, the provinceâ\\x80\\x99s identity is linked to shipbuilding, fisheries and shipping.'},\n",
       "  {'docid': '8179965',\n",
       "   'title': 'PROVIDENCE HOLY CROSS MEDICAL CENTER LBN PROVIDENCE HEALTH SYSTEM - SOUTHERN CALIFORNIA',\n",
       "   'text': 'PROVIDENCE HOLY CROSS MEDICAL CENTER LBN PROVIDENCE HEALTH SYSTEM - SOUTHERN CALIFORNIA General Acute Care Hospital. An acute general hospital is an institution whose primary function is to provide inpatient diagnostic and therapeutic services for a variety of medical conditions, both surgical and non-surgical, to a wide population group.'},\n",
       "  {'docid': '4545354',\n",
       "   'title': '-',\n",
       "   'text': 'Providence Village, Texas: Another masterfully-planned Dallas-Fort Worth community by Huffines Signature Communities.'},\n",
       "  {'docid': '5294366',\n",
       "   'title': 'Tourism Information',\n",
       "   'text': \"Don't Miss the Fundy Isles. A Bay of Fundy vacation is not complete without a visit to the beautiful Fundy Isles. Grand Manan promises extraordinary adventures such as the exhilarating thrill of whale-watching. The incredible tidal currents of the Bay of Fundy create unlimited adventure on Deer Island.\"},\n",
       "  {'docid': '454202',\n",
       "   'title': '-',\n",
       "   'text': 'United States - w/ Julie Rhodes  Best Rock Band - 2015 & 2016 Motif Music Awards The Silks are a rock and roll band out of Providence, Rhode Island. You could say thee from Providence, but then... Fri 4/14 @ 9:00 pm.'},\n",
       "  {'docid': '8765024',\n",
       "   'title': '-',\n",
       "   'text': 'The Bay of Fundy Whales. In Micmac lore, it was a giant whale, who angered the god Glooscap and created such a splash with his mighty tail, that the water sloshes back and forth to this day. In actuality, the story of the tremendous Bay of Fundy tides is no less the stuff of legends.'},\n",
       "  {'docid': '961784',\n",
       "   'title': 'The Thirteen Colonies',\n",
       "   'text': 'Before independence, the thirteen were part of a larger set of colonies in British America. Those in the British West Indies, Canada, and East and West Florida remained loyal to the crown throughout the war, although there was a degree of sympathy with the Patriot cause in several of them.he colonies were: Delaware, Pennsylvania, New Jersey, Georgia, Connecticut, Massachusetts Bay, Maryland, South Carolina, New Hampshire, Virginia, New York, North Carolina, and Rhode Island and Providence.'},\n",
       "  {'docid': '3469091',\n",
       "   'title': 'Rev. Roger Williams, Founder of Rhode Island',\n",
       "   'text': 'Roger Williams (c. 1603 â\\x80\\x93 between January and March 1683) was an English Protestant theologian who was an early proponent of religious freedom and the separation of church and state. In 1636, he began the colony of Providence Plantation, which provided a refuge for religious minorities. Roger Williams National Memorial, established in 1965, is a park in downtown Providence. 2  Roger Williams Park, Providence, Rhode Island, and the Roger Williams Park Zoo within it are named in his honor.'},\n",
       "  {'docid': '8795194',\n",
       "   'title': '-',\n",
       "   'text': 'Proudly servicing members from North Attleboro, Attleboro, Attleboro Falls, Mansfield, Norton, Plainville, Sharon, Foxboro, Canton, Stoughton, and Wrentham, MA as well as Pawtucket, Providence, Cumberland, North Providence, Central Falls and Lincoln, RI. Answer is Fitness is a proud sponsor of the Little North Attleboro league.'},\n",
       "  {'docid': '7506165',\n",
       "   'title': '-',\n",
       "   'text': 'Providence, Rhode Island. Providence is the capital of and most populous city in the U.S. state of Rhode Island, founded in 1636, and one of the oldest cities in the United States. It is located in Providence County and is the third most populous city in New England, after Boston and Worcester. Providence has a city population of 179,154; it is also part of the Providence metropolitan area which extends into southern Massachusetts.'},\n",
       "  {'docid': '5619559',\n",
       "   'title': 'Roger Williams',\n",
       "   'text': 'Consequently, in January 1636 Williams set out for Narragansett Bay, and in the spring, on land purchased from the Narragansett Indians, he founded the town of Providence and the colony of Rhode Island. Providence became a haven for Anabaptists, Quakers, and others whose beliefs were denied public expression.'},\n",
       "  {'docid': '766324',\n",
       "   'title': 'Colleges & Universities in Providence',\n",
       "   'text': 'The University of Rhode Island has a vibrant Providence Campus located in the heart of downtown Providence. The campus houses the College of Continuing Education, which offers nine undergraduate degree and seven graduate level programs, as well as multiple certificate programs.'},\n",
       "  {'docid': '1994565',\n",
       "   'title': 'Silverfish',\n",
       "   'text': 'Silverfish are small insects that are common pests in Providence, Worcester, and throughout Rhode Island, Massachusetts, and New England. Unlike most insects, silverfish only have three stages in their life cycle.'},\n",
       "  {'docid': '7988320',\n",
       "   'title': '-',\n",
       "   'text': 'How long does it take to get from Virginia to Rhode Island by car? MapQuest estimates the driving time between Providence and Richmond as 9 hours and 21 minutes.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset_dict['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7825c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_fn(batch):\n",
    "    batch_query = [query for query in batch['query']]\n",
    "    \n",
    "    batch_num_neg_passage = []\n",
    "    batch_negative_text = []\n",
    "    for passages in batch['negative_passages']:\n",
    "        num_neg_passage = len(passages)\n",
    "        batch_num_neg_passage.append(num_neg_passage)\n",
    "\n",
    "        negative_text = [passage['text'] for passage in passages]\n",
    "        batch_negative_text.append(negative_text)\n",
    "    \n",
    "    # for every positive passage, multiply it with number of negative passage in the same index\n",
    "    queries = []\n",
    "    negative_texts = []\n",
    "    positive_texts = []\n",
    "    for idx, passages in enumerate(batch['positive_passages']):\n",
    "        num_neg_passage = batch_num_neg_passage[idx]\n",
    "        negative_text = batch_negative_text[idx]\n",
    "        query = batch_query[idx]\n",
    "        for passage in passages:\n",
    "            queries += [query] * num_neg_passage\n",
    "            positive_texts += [passage['text']] * num_neg_passage\n",
    "            negative_texts += negative_text\n",
    "\n",
    "    return {'query': queries, 'pos': positive_texts, 'neg': negative_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95837e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a431c0634064143b08a21a06bb91009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/361 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b671c4ae408459fac8e95df39627f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'pos', 'neg'],\n",
       "        num_rows: 11412906\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'pos', 'neg'],\n",
       "        num_rows: 1269038\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_exploded = dataset_dict.map(\n",
    "    explode_fn,\n",
    "    batched=True,\n",
    "    remove_columns=['query_id', 'query', 'positive_passages', 'negative_passages']\n",
    ")\n",
    "dataset_dict_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa69a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what providence is bay of fundy in',\n",
       " 'pos': 'Over the next 150 years, a number of other French settlements and seigneuries were founded in the area occupied by present-day New Brunswick, including along the Saint John River, the upper Bay of Fundy region, in the Tantramar Marshes at Beaubassin, and finally at St. Pierre (site of present-day Bathurst).',\n",
       " 'neg': \"In theology, divine providence, or just providence, is God's intervention in the world. The term Divine Providence is also used as a title of God. A distinction is usually made between general providence, which refers to God's continuous upholding the existence and natural order of the Universe, and special providence, which refers to God's extraordinary intervention in the life of people. Miracles generally fall in the latter category.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_exploded['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147a1f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 11293, 2003, 3016, 1997, 4636, 2100, 1999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "output = tokenizer(example['query'], padding=False, truncation='longest_first', max_length=tokenizer.model_max_length)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2186ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    \n",
    "    query_encoded = tokenizer(\n",
    "        batch['query'],\n",
    "        padding=False,\n",
    "        truncation='longest_first',\n",
    "        max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    # we should check the proportion of documents that are affected \n",
    "    pos_encoded = tokenizer(\n",
    "        batch['pos'],\n",
    "        padding=False,\n",
    "        truncation='longest_first',\n",
    "        max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    \n",
    "    neg_encoded = tokenizer(\n",
    "        batch['neg'],\n",
    "        padding=False,\n",
    "        truncation='longest_first',\n",
    "        max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return {\n",
    "        'query_input_ids': query_encoded['input_ids'],\n",
    "        'query_attention_mask': query_encoded['attention_mask'],\n",
    "        'pos_input_ids': pos_encoded['input_ids'],\n",
    "        'pos_attention_mask': pos_encoded['attention_mask'],\n",
    "        'neg_input_ids': neg_encoded['input_ids'],\n",
    "        'neg_attention_mask': neg_encoded['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75a9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default datasets will try to save tokenization results as int64, which takes\n",
    "# up a lot of necessary space especially for binary tokens such as\n",
    "# attention_mask and token_type_ids\n",
    "# In fact, when features are named appropriately it will use compact types\n",
    "# refer to OptimizedTypedSequence under \n",
    "# https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_writer.py\n",
    "features = datasets.Features({\n",
    "    'query_input_ids': datasets.Sequence(datasets.Value('int32')),\n",
    "    'query_attention_mask': datasets.Sequence(datasets.Value('int8')),\n",
    "    'pos_input_ids': datasets.Sequence(datasets.Value('int32')),\n",
    "    'pos_attention_mask': datasets.Sequence(datasets.Value('int8')),\n",
    "    'neg_input_ids': datasets.Sequence(datasets.Value('int32')),\n",
    "    'neg_attention_mask': datasets.Sequence(datasets.Value('int8'))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d7f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6840bbe4514d4d84c4240c150da9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d74b9760374f798106714df217e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0345b567503446aaa2f61b1ef2ede06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffafc21c23b4b068115defa7ef8b4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165f98b3a0f044cd842d6da31b98a08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33f39d2296b4f3eacb2e2fc2f9d6560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c8f678c69a409d967b35af1f5e08d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748d5bfc06404b098c643251ff0b99e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1427 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cbfe02de6948c988e84e3125a885e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8692b7e4351245929fab1697d8d71c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ebdde408d646ac87131a5cd7f037aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6049f70713a946adb2e62b2d75980e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33981cedd04e4de68dbc143a9c36a10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1e22d663e94c11b66cb3b9cd76b0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ae6dbb29b246a0ba312f2f959af93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ae37b72fd7449aa4e450027cccafd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query_input_ids', 'query_attention_mask', 'pos_input_ids', 'pos_attention_mask', 'neg_input_ids', 'neg_attention_mask'],\n",
       "        num_rows: 11412906\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query_input_ids', 'query_attention_mask', 'pos_input_ids', 'pos_attention_mask', 'neg_input_ids', 'neg_attention_mask'],\n",
       "        num_rows: 1269038\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_tokenized = dataset_dict_exploded.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=['query', 'pos', 'neg'],\n",
    "    features=features\n",
    ")\n",
    "dataset_dict_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0be26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.870764719"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30.982270552\n",
    "dataset_dict_tokenized.data['train'].nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d060e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_input_ids': [101,\n",
       "  2054,\n",
       "  11293,\n",
       "  2003,\n",
       "  3016,\n",
       "  1997,\n",
       "  4636,\n",
       "  2100,\n",
       "  1999,\n",
       "  102],\n",
       " 'query_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'pos_input_ids': [101,\n",
       "  2058,\n",
       "  1996,\n",
       "  2279,\n",
       "  5018,\n",
       "  2086,\n",
       "  1010,\n",
       "  1037,\n",
       "  2193,\n",
       "  1997,\n",
       "  2060,\n",
       "  2413,\n",
       "  7617,\n",
       "  1998,\n",
       "  7367,\n",
       "  23773,\n",
       "  11236,\n",
       "  3111,\n",
       "  2020,\n",
       "  2631,\n",
       "  1999,\n",
       "  1996,\n",
       "  2181,\n",
       "  4548,\n",
       "  2011,\n",
       "  2556,\n",
       "  1011,\n",
       "  2154,\n",
       "  2047,\n",
       "  9192,\n",
       "  1010,\n",
       "  2164,\n",
       "  2247,\n",
       "  1996,\n",
       "  3002,\n",
       "  2198,\n",
       "  2314,\n",
       "  1010,\n",
       "  1996,\n",
       "  3356,\n",
       "  3016,\n",
       "  1997,\n",
       "  4636,\n",
       "  2100,\n",
       "  2555,\n",
       "  1010,\n",
       "  1999,\n",
       "  1996,\n",
       "  9092,\n",
       "  6494,\n",
       "  7849,\n",
       "  19257,\n",
       "  2012,\n",
       "  17935,\n",
       "  22083,\n",
       "  11493,\n",
       "  1010,\n",
       "  1998,\n",
       "  2633,\n",
       "  2012,\n",
       "  2358,\n",
       "  1012,\n",
       "  5578,\n",
       "  1006,\n",
       "  2609,\n",
       "  1997,\n",
       "  2556,\n",
       "  1011,\n",
       "  2154,\n",
       "  21897,\n",
       "  1007,\n",
       "  1012,\n",
       "  102],\n",
       " 'pos_attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'neg_input_ids': [101,\n",
       "  1999,\n",
       "  8006,\n",
       "  1010,\n",
       "  7746,\n",
       "  11293,\n",
       "  1010,\n",
       "  2030,\n",
       "  2074,\n",
       "  11293,\n",
       "  1010,\n",
       "  2003,\n",
       "  2643,\n",
       "  1005,\n",
       "  1055,\n",
       "  8830,\n",
       "  1999,\n",
       "  1996,\n",
       "  2088,\n",
       "  1012,\n",
       "  1996,\n",
       "  2744,\n",
       "  7746,\n",
       "  11293,\n",
       "  2003,\n",
       "  2036,\n",
       "  2109,\n",
       "  2004,\n",
       "  1037,\n",
       "  2516,\n",
       "  1997,\n",
       "  2643,\n",
       "  1012,\n",
       "  1037,\n",
       "  7835,\n",
       "  2003,\n",
       "  2788,\n",
       "  2081,\n",
       "  2090,\n",
       "  2236,\n",
       "  11293,\n",
       "  1010,\n",
       "  2029,\n",
       "  5218,\n",
       "  2000,\n",
       "  2643,\n",
       "  1005,\n",
       "  1055,\n",
       "  7142,\n",
       "  27329,\n",
       "  2075,\n",
       "  1996,\n",
       "  4598,\n",
       "  1998,\n",
       "  3019,\n",
       "  2344,\n",
       "  1997,\n",
       "  1996,\n",
       "  5304,\n",
       "  1010,\n",
       "  1998,\n",
       "  2569,\n",
       "  11293,\n",
       "  1010,\n",
       "  2029,\n",
       "  5218,\n",
       "  2000,\n",
       "  2643,\n",
       "  1005,\n",
       "  1055,\n",
       "  9313,\n",
       "  8830,\n",
       "  1999,\n",
       "  1996,\n",
       "  2166,\n",
       "  1997,\n",
       "  2111,\n",
       "  1012,\n",
       "  17861,\n",
       "  3227,\n",
       "  2991,\n",
       "  1999,\n",
       "  1996,\n",
       "  3732,\n",
       "  4696,\n",
       "  1012,\n",
       "  102],\n",
       " 'neg_attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict_tokenized['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3924969",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82732993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSiamese:\n",
    "    \"\"\"\n",
    "    \n",
    "    https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        query_features = {\n",
    "            'input_ids': [feature['query_input_ids'] for feature in features],\n",
    "            'attention_mask': [feature['query_attention_mask'] for feature in features]\n",
    "        }\n",
    "        pos_features = {\n",
    "            'input_ids': [feature['pos_input_ids'] for feature in features],\n",
    "            'attention_mask': [feature['pos_attention_mask'] for feature in features]\n",
    "        }\n",
    "        neg_features = {\n",
    "            'input_ids': [feature['neg_input_ids'] for feature in features],\n",
    "            'attention_mask': [feature['neg_attention_mask'] for feature in features]\n",
    "        }\n",
    "        query_batch = self.tokenizer.pad(\n",
    "            query_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        pos_batch = self.tokenizer.pad(\n",
    "            pos_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        neg_batch = self.tokenizer.pad(\n",
    "            neg_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            'query_input_ids': query_batch['input_ids'],\n",
    "            'query_attention_mask': query_batch['attention_mask'],\n",
    "            'pos_input_ids': pos_batch['input_ids'],\n",
    "            'pos_attention_mask': pos_batch['attention_mask'],\n",
    "            'neg_input_ids': neg_batch['input_ids'],\n",
    "            'neg_attention_mask': neg_batch['attention_mask'],\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b6603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collate_siamese = DataCollatorForSiamese(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a986a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_input_ids': tensor([[  101,  2054, 11293,  2003,  3016,  1997,  4636,  2100,  1999,   102],\n",
       "         [  101,  2054, 11293,  2003,  3016,  1997,  4636,  2100,  1999,   102]]),\n",
       " 'query_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'pos_input_ids': tensor([[  101,  2058,  1996,  2279,  5018,  2086,  1010,  1037,  2193,  1997,\n",
       "           2060,  2413,  7617,  1998,  7367, 23773, 11236,  3111,  2020,  2631,\n",
       "           1999,  1996,  2181,  4548,  2011,  2556,  1011,  2154,  2047,  9192,\n",
       "           1010,  2164,  2247,  1996,  3002,  2198,  2314,  1010,  1996,  3356,\n",
       "           3016,  1997,  4636,  2100,  2555,  1010,  1999,  1996,  9092,  6494,\n",
       "           7849, 19257,  2012, 17935, 22083, 11493,  1010,  1998,  2633,  2012,\n",
       "           2358,  1012,  5578,  1006,  2609,  1997,  2556,  1011,  2154, 21897,\n",
       "           1007,  1012,   102],\n",
       "         [  101,  2058,  1996,  2279,  5018,  2086,  1010,  1037,  2193,  1997,\n",
       "           2060,  2413,  7617,  1998,  7367, 23773, 11236,  3111,  2020,  2631,\n",
       "           1999,  1996,  2181,  4548,  2011,  2556,  1011,  2154,  2047,  9192,\n",
       "           1010,  2164,  2247,  1996,  3002,  2198,  2314,  1010,  1996,  3356,\n",
       "           3016,  1997,  4636,  2100,  2555,  1010,  1999,  1996,  9092,  6494,\n",
       "           7849, 19257,  2012, 17935, 22083, 11493,  1010,  1998,  2633,  2012,\n",
       "           2358,  1012,  5578,  1006,  2609,  1997,  2556,  1011,  2154, 21897,\n",
       "           1007,  1012,   102]]),\n",
       " 'pos_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]]),\n",
       " 'neg_input_ids': tensor([[  101,  1999,  8006,  1010,  7746, 11293,  1010,  2030,  2074, 11293,\n",
       "           1010,  2003,  2643,  1005,  1055,  8830,  1999,  1996,  2088,  1012,\n",
       "           1996,  2744,  7746, 11293,  2003,  2036,  2109,  2004,  1037,  2516,\n",
       "           1997,  2643,  1012,  1037,  7835,  2003,  2788,  2081,  2090,  2236,\n",
       "          11293,  1010,  2029,  5218,  2000,  2643,  1005,  1055,  7142, 27329,\n",
       "           2075,  1996,  4598,  1998,  3019,  2344,  1997,  1996,  5304,  1010,\n",
       "           1998,  2569, 11293,  1010,  2029,  5218,  2000,  2643,  1005,  1055,\n",
       "           9313,  8830,  1999,  1996,  2166,  1997,  2111,  1012, 17861,  3227,\n",
       "           2991,  1999,  1996,  3732,  4696,  1012,   102],\n",
       "         [  101,  3664, 10853,  1051,  1012,  3449,  2953,  4143,  2587,  2103,\n",
       "           1998,  2110,  2700,  4584,  1010,  3095,  2013,  1996,  2103,  3022,\n",
       "           4041,  1998,  2458,  2533,  1010,  4505,  2013,  1996,  9763,  2479,\n",
       "           2270,  6671,  3691,  1006, 10973,  2696,  1007,  1010,  1996, 11293,\n",
       "           3192,  1010,  1996,  5116, 11293,  6328,  9530,  8043,  6212,  5666,\n",
       "           1006,  1040,  9397,  2278,  1007,  1010,  1998,  1996, 11293,  6328,\n",
       "           2533,  1010,  2004,  2092,  2004,  2449,  4177,  1998,  2451,  1012,\n",
       "           1012,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'neg_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset_dict_tokenized['train'], batch_size=2, collate_fn=data_collate_siamese)\n",
    "example = next(iter(data_loader))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb9f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name: str, pooling_mode: str = 'avg'):\n",
    "        super().__init__()\n",
    "        if pooling_mode not in {'avg', 'cls'}:\n",
    "            raise ValueError(f'{pooling_mode} needs to one of avg, cls')\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling_mode = pooling_mode\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.pooling_mode == 'avg':\n",
    "            pooled = output.last_hidden_state.mean(dim=1)\n",
    "        elif self.pooling_mode == 'cls':\n",
    "            pooled = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0195861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerPooling(\n",
       "  (base_model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerPooling(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e4a91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4427, -0.1523, -0.1497,  ..., -0.1537, -0.0263,  0.2855],\n",
       "        [-0.4427, -0.1523, -0.1497,  ..., -0.1537, -0.0263,  0.2855]],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(\n",
    "    input_ids=example['query_input_ids'].to(device),\n",
    "    attention_mask=example['query_attention_mask'].to(device)\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d042323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name: str, pooling_mode: str = 'avg'):\n",
    "        super().__init__()\n",
    "        self.transformer_model = TransformerPooling(model_name, pooling_mode)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_input_ids,\n",
    "        query_attention_mask,\n",
    "        pos_input_ids,\n",
    "        pos_attention_mask,\n",
    "        neg_input_ids,\n",
    "        neg_attention_mask\n",
    "    ):\n",
    "        query_embedding = self.transformer_model(\n",
    "            input_ids=query_input_ids,\n",
    "            attention_mask=query_attention_mask\n",
    "        )\n",
    "        pos_embedding = self.transformer_model(\n",
    "            input_ids=pos_input_ids,\n",
    "            attention_mask=pos_attention_mask\n",
    "        )\n",
    "        neg_embedding = self.transformer_model(\n",
    "            input_ids=neg_input_ids,\n",
    "            attention_mask=neg_attention_mask\n",
    "        )\n",
    "        doc_embedding = torch.cat([pos_embedding, neg_embedding])\n",
    "        \n",
    "        # cosine similarity\n",
    "        query_norm = F.normalize(query_embedding, p=2, dim=1)\n",
    "        doc_norm = F.normalize(doc_embedding, p=2, dim=1)\n",
    "        scores = torch.mm(query_norm, doc_norm.transpose(0, 1)) # * self.scale\n",
    "\n",
    "        # Example a[i] should match with b[i]\n",
    "        labels = torch.arange(scores.size()[0], device=scores.device)\n",
    "        loss = F.cross_entropy(scores, labels)\n",
    "        return loss, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a16dc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return SiameseModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbc34fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "670d4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/mingyuliu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/mingyuliu/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/mingyuliu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/mingyuliu/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "***** Running training *****\n",
      "  Num examples = 11412906\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 267492\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='267492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   357/267492 02:40 < 33:32:46, 2.21 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1006.00 MiB (GPU 0; 31.75 GiB total capacity; 28.95 GiB already allocated; 229.75 MiB free; 30.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_187694/508743764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#compute_metrics=compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2378\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_187694/3833090769.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input_ids, query_attention_mask, pos_input_ids, pos_attention_mask, neg_input_ids, neg_attention_mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m         neg_embedding = self.transformer_model(\n\u001b[1;32m     28\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m         \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_187694/2855844499.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             )\n\u001b[1;32m    348\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1006.00 MiB (GPU 0; 31.75 GiB total capacity; 28.95 GiB already allocated; 229.75 MiB free; 30.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_train_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "finetuned_checkpoint = f\"{model_name}-siamese\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=finetuned_checkpoint,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=weight_decay,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=True,\n",
    "    #gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    #siamese_model,\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer, \n",
    "    data_collator=data_collate_siamese,\n",
    "    train_dataset=dataset_dict_tokenized['train'],\n",
    "    eval_dataset=dataset_dict_tokenized['test'],\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0d8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
