{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2016-08-14 13:23:19 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.1\n",
      "pymongo 3.3.0\n",
      "tqdm 4.8.2\n",
      "spacy 0.101.0\n",
      "nltk 3.2.1\n",
      "scikit-learn 0.17.1\n",
      "gensim 0.13.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 8, 6 # change default figure size\n",
    "\n",
    "# 1. magic to print version\n",
    "# 2. magic so that the notebook will reload external python modules\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# python -m pip install pymongo\n",
    "import json\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pymongo\n",
    "from tqdm import tqdm_notebook # progress bar\n",
    "\n",
    "# remove stop words built in with sklearn and nltk and punctuation marks; \n",
    "# emoticons are not removed (hopefully)\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# note that string's punctuation only includes english punctuation marks\n",
    "STOPWORDS = set( stopwords.words('english') ).union( set(ENGLISH_STOP_WORDS) )\n",
    "STOPLIST  = STOPWORDS.union( set(string.punctuation) )\n",
    "\n",
    "# loading the spacy nlp english model takes a moment\n",
    "import spacy\n",
    "parser = spacy.load('en')\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,pymongo,tqdm,spacy,nltk,scikit-learn,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.', 'stars': 4, 'date': '2012-08-01', 'type': 'review', 'votes': {'cool': 0, 'useful': 0, 'funny': 0}, 'review_id': 'Ya85v4eqdd6k9Od8HbQjyA', 'business_id': '5UmKMjUEUNdYWqANhGckJw', 'user_id': 'PUFPaY9KxDAcGqfsorJp3Q'}\n"
     ]
    }
   ],
   "source": [
    "FILE = 'yelp_dataset_challenge_academic_dataset'\n",
    "DATASET = 'yelp_academic_dataset_review.json'\n",
    "DATASET_PATH = os.path.join( FILE, DATASET )\n",
    "\n",
    "# print one line of the json file\n",
    "with open(DATASET_PATH) as f:\n",
    "    print( json.loads( f.readline() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_file_len(fname):\n",
    "    \"\"\"\n",
    "    count the total number of lines (minus 1) for a given file\n",
    "    http://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            pass\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def upload_to_mongo( N, dataset_path = DATASET_PATH, stop_list = STOPLIST, parser = parser ):\n",
    "    \"\"\"\n",
    "    gets the reviews from the json file and \n",
    "    upload them to a MongoDB database called 'Yelp'\n",
    "    and into a collection called 'Review'\n",
    "    \n",
    "    N: specify the number of data to read\n",
    "    \"\"\"\n",
    "    \n",
    "    # connect to mongodb and only built the database if it's the first time\n",
    "    conn = pymongo.MongoClient()\n",
    "    db = conn['Yelp']\n",
    "    collection = db['Review']     \n",
    "    if not collection.count():\n",
    "        \n",
    "        # confirm that the number of data to read is\n",
    "        # lower than the total file length, if not\n",
    "        # switch to that\n",
    "        file_len = count_file_len(dataset_path)\n",
    "        if N > file_len:\n",
    "            N = file_len\n",
    "        \n",
    "        with open(dataset_path) as f:\n",
    "            for _ in tqdm_notebook( range(N) ):\n",
    "                line = next(f)\n",
    "                review = json.loads(line)\n",
    "\n",
    "                # clean up the review text using spacy's \n",
    "                # pre-trained english model `parser`; also remove stop words\n",
    "                # and punctuation marks from the pre-specified list\n",
    "                words = []\n",
    "                text = review['text'].strip()\n",
    "                parsed_text = parser(text)                       \n",
    "                for token in parsed_text:\n",
    "\n",
    "                    # lemma_ will access the stemmed, lemmatized \n",
    "                    # and lower-cased version of the token\n",
    "                    token = token.lemma_ \n",
    "                    if token not in stop_list:\n",
    "                        words.append(token)\n",
    "\n",
    "                text_cleaned = ' '.join(words)\n",
    "\n",
    "                collection.insert_one({\n",
    "                    'review_id': review['review_id'],\n",
    "                    'business_id': review['business_id'],\n",
    "                    'stars': review['stars'],\n",
    "                    'text': text,\n",
    "                    'text_cleaned': text_cleaned\n",
    "                })\n",
    "    return db, collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to have a mongodb instance already running by typing \n",
    "\n",
    "```bash\n",
    "mongod\n",
    "```\n",
    "in the command line.\n",
    "\n",
    "We can remove the collection and re-upload the whole thing again using\n",
    "\n",
    "```python\n",
    "db['Review'].drop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confirm that the documents in the collection \n",
    "# matches the specified number\n",
    "number_of_data_to_read = 100000\n",
    "db, collection = upload_to_mongo( N = number_of_data_to_read )\n",
    "assert collection.count() == number_of_data_to_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"\n",
    "    streaming way of building up the gensim corpus\n",
    "    \"\"\"\n",
    "    def __init__( self, cursor, corpora_dict ):\n",
    "        self.cursor = cursor\n",
    "        self.corpora_dict = corpora_dict\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # rewind the cursor so that we can loop over it again\n",
    "        self.cursor.rewind()\n",
    "        for review in self.cursor:\n",
    "            tokens = review['text_cleaned'].split()\n",
    "            yield self.corpora_dict.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens:  38407\n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find()\n",
    "\n",
    "# build the id2word dictionary and the corpus (map the word to id)\n",
    "# and filter out words that appear in less than 2 documents\n",
    "corpora_dict = corpora.Dictionary( review['text_cleaned'].split() for review in cursor )\n",
    "corpora_dict.filter_extremes(no_below = 2)\n",
    "corpora_dict.compactify()\n",
    "print( 'number of unique tokens: ', len(corpora_dict) )\n",
    "\n",
    "# create the corpus using the built dictionary\n",
    "corpus = MyCorpus( cursor, corpora_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building up the corpus, we can use the following piece of code to only keep the number of most frequent words if we want to limit the size of our vocabulary.\n",
    "\n",
    "```python\n",
    "# specify the number for keep_n\n",
    "corpora_dict.filter_extremes(keep_n = 10000)\n",
    "corpora_dict.compactify()\n",
    "```\n",
    "\n",
    "For more info, check the [gensim Dictionary API](https://radimrehurek.com/gensim/corpora/dictionary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_corpus_and_corpora_dict( corpus, corpora_dict, dir_name ):\n",
    "    \"\"\"\n",
    "    Creates a `dir_name` directory and store\n",
    "    corpus and corpora dict in it.\n",
    "    \n",
    "    Returns the corpus's path so we can load it later\n",
    "    for training models\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    \n",
    "    corpora_dict_path = os.path.join( dir_name, 'corpora_dict.dict' )\n",
    "    corpora.Dictionary.save( corpora_dict, corpora_dict_path )\n",
    "    \n",
    "    corpus_path = os.path.join( dir_name, 'corpus.lda-c' )\n",
    "    corpora.BleiCorpus.serialize( corpus_path, corpus )\n",
    "    return corpus_path, corpora_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save and load\n",
    "dir_name = 'model'\n",
    "corpus_path, corpora_dict_path = save_corpus_and_corpora_dict( corpus, corpora_dict, dir_name )\n",
    "corpus_loaded = corpora.BleiCorpus(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 38407)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert gensim corpus to sparse matrix\n",
    "# note that you'll need to transpose the matrix\n",
    "# so that the rows will be each data points\n",
    "X = gensim.matutils.corpus2csc(corpus_loaded).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews that are lower than 3 starts are considered negative, \n",
    "# which are labeled as 0 and positive reviews are labled as 1\n",
    "cursor.rewind()\n",
    "y = np.array([ review['stars'] for review in cursor ])\n",
    "y[ y <= 3 ] = 0\n",
    "y[ y > 3 ] = 1\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 38407)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs = -1)\n",
    "logreg.fit( X_tfidf, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88675000000000004"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_tfidf)\n",
    "accuracy_score( y, y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "\n",
    "# latent semantic (dimension reduction) before clustering\n",
    "from hdbscan import HDBSCAN\n",
    "# clusterer = HDBSCAN()\n",
    "# clusterer.fit()\n",
    "\n",
    "num_topics = 20\n",
    "lda = LdaModel( corpus_loaded, num_topics = num_topics, \n",
    "                id2word = corpora_dict, passes = 10 )\n",
    "\n",
    "lda.show_topics(10)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Blog: Predicting what user reviews are about with LDA and gensim](http://www.vladsandulescu.com/topic-prediction-lda-user-reviews/)\n",
    "- [Github to the blog post above](https://github.com/vladsandulescu/topics)\n",
    "- [Gensim Tutorial: Corpora and Vector Spaces](https://radimrehurek.com/gensim/tut1.html)\n",
    "\n",
    "http://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
