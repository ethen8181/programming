{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    \n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    \n",
    "    return data\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 22054494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# the .data attribute will access the raw data, where\n",
    "# each element is a document\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "\n",
    "# gensim’s Word2vec expects a sequence of sentences as its input,\n",
    "# where each sentence a list of words. We'll be lazy for now\n",
    "# and not perform any sort of text preprocessing\n",
    "vocabulary = [word for doc in newsgroups_train.data for word in doc]\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 3252437\n"
     ]
    }
   ],
   "source": [
    "documents = [doc.strip().split() for doc in newsgroups_train.data]\n",
    "vocabulary = [word.lower() for doc in documents for word in doc]\n",
    "#          if word not in ENGLISH_STOP_WORDS]\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [('subject', 9520), ('use', 8750), (\"'s\", 7893), ('write', 6884), ('know', 6105)]\n",
      "Sample data [15, 0, 56, 39, 23079, 46, 16410, 1074, 697, 246] ['thing', 'subject', 'car', 'nntp_posting', 'host_rac3.wam.umd.edu', 'organization_university', 'maryland_college', 'park', 'lines', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "# vocabulary_size = None\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "#     count = [['UNK', 0]]\n",
    "#     if n_words is None:\n",
    "#         count.extend(collections.Counter(words).most_common(n_words))\n",
    "#     else:\n",
    "#         count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    count = collections.Counter(words).most_common(n_words)\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "            data.append(index)\n",
    "        # else:\n",
    "        #     index = 0  # dictionary['UNK']\n",
    "        #     unk_count += 1\n",
    "        # data.append(index)\n",
    "\n",
    "    # count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_index = 0\n",
    "# batch = np.zeros(batch_size, dtype = np.uint32)\n",
    "# labels = np.zeros((batch_size, 1), dtype = np.uint32)\n",
    "# span = 2 * skip_window + 1  # [skip_window target skip_window]\n",
    "# print(span)\n",
    "# # the buffer holds a maximum of `span` elements and will\n",
    "# # be a moving window of words that samples are drawn from\n",
    "# buffer = collections.deque(maxlen = span)\n",
    "# for _ in range(span):\n",
    "#     buffer.append(data[data_index])\n",
    "#     data_index = (data_index + 1) % len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 subject -> 15 thing\n",
      "0 subject -> 56 car\n",
      "56 car -> 39 nntp_posting\n",
      "56 car -> 0 subject\n",
      "39 nntp_posting -> 23079 host_rac3.wam.umd.edu\n",
      "39 nntp_posting -> 56 car\n",
      "23079 host_rac3.wam.umd.edu -> 39 nntp_posting\n",
      "23079 host_rac3.wam.umd.edu -> 46 organization_university\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "          '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "vocabulary_size = len(dictionary)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 53/100001 [00:00<03:08, 529.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  299.814086914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2092/100001 [00:03<02:28, 660.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2000 :  132.999240431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4130/100001 [00:06<02:22, 672.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  4000 :  64.3089953823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6081/100001 [00:09<02:33, 612.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  6000 :  39.7369674871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8089/100001 [00:12<02:19, 658.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  8000 :  27.7269375744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10103/100001 [00:15<02:15, 664.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  10000 :  20.5734169469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12096/100001 [00:18<02:30, 584.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  12000 :  16.0279527538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14114/100001 [00:21<02:17, 623.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  14000 :  12.7265606122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16089/100001 [00:25<02:07, 656.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  16000 :  11.2583232989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18067/100001 [00:28<02:06, 648.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  18000 :  9.17188687849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20108/100001 [00:31<02:07, 626.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  20000 :  8.22088277483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22070/100001 [00:34<02:00, 648.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  22000 :  6.93328272533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24103/100001 [00:37<01:58, 639.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  24000 :  6.23523056483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26079/100001 [00:40<01:55, 642.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  26000 :  5.63266658413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28127/100001 [00:43<01:51, 644.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  28000 :  5.53037808871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30113/100001 [00:46<01:45, 659.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  30000 :  5.41085318971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32104/100001 [00:50<01:54, 592.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  32000 :  5.22951853299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34093/100001 [00:53<01:41, 649.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  34000 :  5.10790141428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36102/100001 [00:56<01:36, 662.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  36000 :  5.10305028784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38116/100001 [00:59<01:33, 664.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  38000 :  4.96739842135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40068/100001 [01:02<01:29, 671.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  40000 :  4.94769360447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42073/100001 [01:05<01:29, 646.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  42000 :  4.90534448075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44097/100001 [01:08<01:23, 672.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  44000 :  4.80577723062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46104/100001 [01:11<01:20, 669.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  46000 :  4.75936888337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48135/100001 [01:14<01:16, 673.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  48000 :  4.73230387473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50111/100001 [01:17<01:15, 663.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  50000 :  4.84547416949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52074/100001 [01:20<01:12, 663.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  4.7594352026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54107/100001 [01:23<01:08, 672.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  54000 :  4.6888678987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56092/100001 [01:26<01:04, 677.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  56000 :  4.666522982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58110/100001 [01:29<01:04, 650.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  58000 :  4.64724033868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60071/100001 [01:32<00:59, 668.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  60000 :  4.62005870533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62085/100001 [01:35<00:58, 645.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  62000 :  4.60748245668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64059/100001 [01:38<00:57, 622.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  64000 :  4.56079784679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66113/100001 [01:42<00:53, 633.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  66000 :  4.57100452197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68121/100001 [01:45<00:50, 625.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  68000 :  4.51377039814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70092/100001 [01:48<00:44, 667.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  70000 :  4.49332784742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72115/100001 [01:51<00:46, 598.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  72000 :  4.48448897171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74107/100001 [01:54<00:43, 597.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  74000 :  4.48985939038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76129/100001 [01:57<00:35, 675.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  76000 :  4.47161449313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78109/100001 [02:00<00:32, 677.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  78000 :  4.41033694029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80092/100001 [02:03<00:29, 673.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  80000 :  4.44358569908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82071/100001 [02:06<00:26, 677.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  82000 :  4.44948875225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84085/100001 [02:10<00:25, 612.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  84000 :  4.39938140035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86105/100001 [02:13<00:20, 681.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  86000 :  4.37770083308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88088/100001 [02:16<00:17, 672.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  88000 :  4.36674010289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90105/100001 [02:19<00:15, 649.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  90000 :  4.37630050865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92093/100001 [02:22<00:11, 663.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  92000 :  4.33248949313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94085/100001 [02:25<00:09, 646.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  94000 :  4.34902845705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96122/100001 [02:28<00:05, 680.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  96000 :  4.33037853301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98110/100001 [02:31<00:02, 678.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  98000 :  4.31298154807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100001/100001 [02:34<00:00, 648.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  100000 :  4.27476632017\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "from tqdm import trange\n",
    "\n",
    "num_steps = 100001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in trange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         if step % 10000 == 0:\n",
    "#             sim = similarity.eval()\n",
    "#             for i in range(valid_size):\n",
    "#                 valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                 top_k = 8  # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "#                 log_str = 'Nearest to %s:' % valid_word\n",
    "#                 for k in range(top_k):\n",
    "#                     close_word = reverse_dictionary[nearest[k]]\n",
    "#                     log_str = '%s %s,' % (log_str, close_word)\n",
    "#                 print(log_str)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer \n",
      "\n",
      "[ 0.45620858  0.44938023  0.44900897  0.4468715   0.44654727  0.44154607\n",
      "  0.4351962   0.4349767   0.42985046  0.42923429]\n",
      "inverse\n",
      "reducing\n",
      "ay\n",
      "universitet\n",
      "pc\n",
      "maryland_college\n",
      "huge_rock\n",
      "formatter\n",
      "contort\n",
      "public_education\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "# idx = word_index['computer']\n",
    "idx = dictionary['computer']\n",
    "\n",
    "# eval_word = word_index_rev[idx]\n",
    "eval_word = reverse_dictionary[idx]\n",
    "print(eval_word, '\\n')\n",
    "\n",
    "# remember the cdist returns a the cosine distance,\n",
    "# so when doing the argsort, which is sorting by ascending\n",
    "# order, the top k most similar word will be the first k one;\n",
    "# and since the most similar word will always be itself, we\n",
    "# exclude that from the returned result\n",
    "vector = final_embeddings[idx].reshape(1, -1)\n",
    "sim = cdist(final_embeddings, vector, metric = 'cosine').ravel()\n",
    "nearest_indices = np.argsort(sim)[1:(top_k + 1)]\n",
    "print(1 - sim[nearest_indices])\n",
    "\n",
    "for nearest_idx in nearest_indices:\n",
    "    sim_word = reverse_dictionary[nearest_idx]\n",
    "    print(sim_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sentence_no = -1\n",
    "total_words = 0\n",
    "min_reduce = 1\n",
    "vocab = defaultdict(int)\n",
    "checked_string_types = 0\n",
    "\n",
    "for idx, sentence in enumerate(sentences, 1):\n",
    "    for word in sentence:\n",
    "        vocab[word] += 1\n",
    "\n",
    "raw_vocab = vocab\n",
    "corpus_count = sentence_no\n",
    "print('corpus count:', corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors, Vocab\n",
    "\n",
    "wv = KeyedVectors()\n",
    "wv.vocab = {}\n",
    "wv.index2word = []\n",
    "\n",
    "min_count = word2vec.min_count\n",
    "sample = word2vec.sample\n",
    "\n",
    "\n",
    "def keep_vocab_item(word, count, min_count, trim_rule = None):\n",
    "    \"\"\"filter the min word count\"\"\"\n",
    "    default_rule = count >= min_count\n",
    "    if trim_rule is None:\n",
    "        return default_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_total = drop_unique = 0\n",
    "\n",
    "# keep track of the total number of retained\n",
    "# words to perform subsampling later\n",
    "retain_total, retain_words = 0, []\n",
    "\n",
    "for word, count in raw_vocab.items():\n",
    "    if keep_vocab_item(word, count, min_count):\n",
    "        retain_words.append(word)\n",
    "        retain_total += count\n",
    "        wv.vocab[word] = Vocab(count = count, index = len(wv.index2word))\n",
    "        wv.index2word.append(word)\n",
    "    else:\n",
    "        drop_unique += 1\n",
    "        drop_total += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used for some logging information\n",
    "original_unique_total = len(retain_words) + drop_unique\n",
    "retain_unique_pct = len(retain_words) * 100 / original_unique_total\n",
    "original_total = retain_total + drop_total\n",
    "retain_pct = retain_total * 100 / original_total\n",
    "retain_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precalculate each vocabulary item's threshold for sub-sampling\n",
    "threshold_count = sample * retain_total\n",
    "\n",
    "downsample_total, downsample_unique = 0, 0\n",
    "for word in retain_words:\n",
    "    count = raw_vocab[word]\n",
    "    prob = np.sqrt(count / threshold_count + 1) * threshold_count / count\n",
    "    if prob < 1.0:\n",
    "        downsample_unique += 1\n",
    "        downsample_total += prob * count\n",
    "    else:\n",
    "        prob = 1\n",
    "        downsample_total += count\n",
    "    \n",
    "    # ?? * 2 ** 32\n",
    "    wv.vocab[word].sample_int = int(prob * 2 ** 32)\n",
    "    print(word, count, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort vocabulary's index by count\n",
    "wv.index2word.sort(key = lambda word: wv.vocab[word].count, reverse = True)\n",
    "for idx, word in enumerate(wv.index2word):\n",
    "    wv.vocab[word].index = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from temp import build_vocab\n",
    "\n",
    "sentences = [doc.strip().split() for doc in newsgroups_train.data[:30]]\n",
    "vocab, index2word = build_vocab(sentences)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for sentence in sentences:\n",
    "sentence = sentences[0]\n",
    "word_vocabs = [vocab[w] for w in sentence if w in vocab and\n",
    "               vocab[w]['prob'] > np.random.rand()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    \"\"\"Read data into a list of tokens/words\"\"\"\n",
    "    filename = 'text8.zip'\n",
    "    base_url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        call('wget ' + base_url + filename, shell = True)\n",
    "\n",
    "    with ZipFile(filename) as f:\n",
    "        file = f.namelist()[0]\n",
    "\n",
    "        # ensure compatibility each python2 and python3's str type\n",
    "        # https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str\n",
    "        data = tf.compat.as_str(f.read(file)).split()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "words = read_data()\n",
    "print('data:', words[:4])\n",
    "print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(BIGRAM_PATH) as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "print('data:', words[:4])\n",
    "print('data size {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocab_size = None):\n",
    "    \n",
    "    # word_count = [['UNK', -1]]\n",
    "    # word_count.extend(Counter(words).most_common(vocab_size))\n",
    "    word_count = Counter(words).most_common(vocab_size)\n",
    "    word_index = {word: idx for idx, (word, _) in enumerate(word_count)}\n",
    "\n",
    "    # build up word index and replaced the words by its assigned indices\n",
    "    data = []\n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            idx = word_index[word]\n",
    "        # else:\n",
    "        #     idx = 0\n",
    "        #    unknown_count += 1\n",
    "\n",
    "            data.append(idx)\n",
    "\n",
    "    # 'UNK' flag for out of vocabulary word\n",
    "    # unknown = 'UNK', unknown_count\n",
    "    # word_count.append(unknown)\n",
    "    # word_count[0][1] = unknown_count\n",
    "    word_index_rev = {idx: word for word, idx in word_index.items()}\n",
    "    return data, word_count, word_index, word_index_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : ??? do we need to return a 4 element tuple\n",
    "data, word_count, word_index, word_index_rev = build_dataset(words, vocab_size = 20000)\n",
    "\n",
    "print('Most common words', word_count[:5])\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(indexed_words, window):\n",
    "    \"\"\"\n",
    "    Form training pairs according to the skip-gram model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    indexed_words : list\n",
    "        List of index that represents the words, e.g. [5243, 3083, 11],\n",
    "        and 5243 might represent the word \"Today\"\n",
    "        \n",
    "    window : int\n",
    "        Window size of the skip-gram model, where word is sampled before\n",
    "        and after the center word according to this window size\n",
    "    \"\"\"\n",
    "    for index, center in enumerate(indexed_words):\n",
    "        # random integers from `low` (inclusive) to `high` (exclusive)\n",
    "        context = np.random.randint(1, window + 1)\n",
    "\n",
    "        # get a random target before the center word\n",
    "        for target in indexed_words[max(0, index - context):index]:\n",
    "            yield center, target\n",
    "\n",
    "        # get a random target after the center word\n",
    "        for target in indexed_words[(index + 1):(index + 1 + context)]:\n",
    "            yield center, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = generate_sample(indexed_words = data, window = 3)\n",
    "\n",
    "print('original data:', data[:6])\n",
    "print('skip gram sample:')\n",
    "\n",
    "# we start off by using the first word as the center word,\n",
    "# and since there's no word before it, we will not have any\n",
    "# sampled word before it; after that we keep sliding the center\n",
    "# word and generate word pairs\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\"\n",
    "    Group a numerical stream of centered and targeted\n",
    "    word into batches and yield them as numpy arrays\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype = np.int32)\n",
    "        target_batch = np.zeros((batch_size, 1), dtype = np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = 3\n",
    "batch_size = 5\n",
    "iterator = generate_sample(indexed_words = data, window = window)\n",
    "batches = get_batch(iterator, batch_size)\n",
    "\n",
    "# e.g. generate a batch\n",
    "center_batch, target_batch = next(batches)\n",
    "print(center_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Define placeholders for input and output. Input is the center word and output is the target (context) word. Instead of using one-hot vectors, we input the index of those words directly.\n",
    "\n",
    "```python\n",
    "# explicitly naming our operations will make it easier to track them later\n",
    "center_words = tf.placeholder(\n",
    "    tf.int32, shape = [BATCH_SIZE], name = 'center_words')\n",
    "\n",
    "# for target_words:\n",
    "# we will use this with tensorflow's loss function later, and the function\n",
    "# requires rank 2 input, that's why there's an extra dimension in the shape\n",
    "target_words = tf.placeholder(\n",
    "    tf.int32, shape = [BATCH_SIZE, 1], name = 'target_words')\n",
    "```\n",
    "\n",
    "**Step 2:** Define the weight/variable. In this case, the embedding matrix. Each row corresponds to the representation vector of one word. If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will have shape [VOCAB_SIZE, EMBED_SIZE]. We initialize the embedding matrix to value from a random distribution. In this case, let’s choose uniform distribution.\n",
    "\n",
    "```python\n",
    "# word vectors\n",
    "embed_matrix = tf.Variable(\n",
    "    tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name = 'embed_matrix')\n",
    "```\n",
    "\n",
    "**Step 3:** Inference (compute the forward path of the graph). Recall that the hidden layer serves as a lookup table and its purpose is to get the vector representations of words in our dictionary.\n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "i.e. The output of the hidden layer is just the \"word vector\" for the input word. Our embed_matrix has dimension [VOCAB_SIZE x EMBED_SIZE], with each row of the embedding matrix corresponds to the vector representation of the word at that index. So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix. TensorFlow provides a convenient method to do so called `tf.nn.embedding_lookup()`. This method is really useful when it comes to matrix multiplication with one-hot vectors because it saves us from doing a bunch of unnecessary computation that will return 0 anyway.\n",
    "\n",
    "```python\n",
    "# input -> hidden layer\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n",
    "```\n",
    "\n",
    "**Step 4: Define the loss function and optimizer** For nce_loss, we need weights and biases for the hidden layer to calculate negative sampling loss.\n",
    "\n",
    "```python\n",
    "# hidden layer -> output layer's weights\n",
    "output_weight = tf.Variable(\n",
    "    tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev = 1.0 / EMBED_SIZE ** 0.5))\n",
    "\n",
    "output_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "# hidden layer -> output layer + negative sampling loss\n",
    "loss = tf.nn.sampled_softmax_loss(\n",
    "    weights = output_weight, biases = output_bias,\n",
    "    labels = target_words, inputs = embed,\n",
    "    num_sampled = NUM_SAMPLED, num_classes = VOCAB_SIZE)\n",
    "\n",
    "avg_loss = tf.reduce_mean(loss, name = 'loss')\n",
    "\n",
    "# choose an optimizer to perform the heavy lifting\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "optimize = optimizer.minimize(avg_loss)\n",
    "```\n",
    "\n",
    "After defining the operations we will create the session to execute the computation, including feeding in the inputs, running the optimizer to minimize the objective function we just defined and fetch the loss value so we can check convergence. The following code chunk pretty much dumped everything into one giant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf_word2vec_no_frill import tf_word2vec, build_vocab\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 20000  # 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "WINDOW_SIZE = 5  # the context window \n",
    "NUM_SAMPLED = 64  # Number of negative examples to sample\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 10000\n",
    "TENSORBOARD = './graphs/no_frills/'\n",
    "word2vec_param = {'vocab_size': VOCAB_SIZE, 'batch_size': BATCH_SIZE,\n",
    "                  'embed_size': EMBED_SIZE, 'num_sampled': NUM_SAMPLED,\n",
    "                  'learning_rate': LEARNING_RATE, 'epochs': EPOCHS,\n",
    "                  'tensorboard': TENSORBOARD, 'window_size': WINDOW_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words = read_data()\n",
    "# words = [word.lower() \n",
    "#          for doc in documents\n",
    "#          for word in doc\n",
    "#          if word not in ENGLISH_STOP_WORDS]\n",
    "# with open(BIGRAM_PATH) as f:\n",
    "#     words = f.read().split()\n",
    "\n",
    "# data, word_count, word_index, word_index_rev = build_dataset(words, VOCAB_SIZE)\n",
    "# iterator = generate_sample(indexed_words = data, window = WINDOW_SIZE)\n",
    "# batch_gen = get_batch(iterator, BATCH_SIZE)\n",
    "\n",
    "# actual model training\n",
    "# word_vectors, history = tf_word2vec(batch_gen, **word2vec_param)\n",
    "# word_vectors, history = tf_word2vec(data, **word2vec_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf_word2vec_no_frill import tf_word2vec, build_vocab\n",
    "\n",
    "\n",
    "# VOCAB_SIZE = 20000  # 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "WINDOW_SIZE = 5  # the context window \n",
    "NUM_SAMPLED = 64  # Number of negative examples to sample\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100000\n",
    "TENSORBOARD = './graphs/no_frills/'\n",
    "word2vec_param = {'batch_size': BATCH_SIZE,\n",
    "                  'embed_size': EMBED_SIZE, 'num_sampled': NUM_SAMPLED,\n",
    "                  'learning_rate': LEARNING_RATE, 'epochs': EPOCHS,\n",
    "                  'tensorboard': TENSORBOARD, 'window_size': WINDOW_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(BIGRAM_PATH) as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.split())\n",
    "\n",
    "\n",
    "vocab, index2word = build_vocab(sentences)\n",
    "word_vectors, history = tf_word2vec(sentences, vocab, **word2vec_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the convergence or course\n",
    "# we can also do this within tensorboard\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title('Convergence Plot')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = 500\n",
    "\n",
    "embedding = word_vectors[:top]\n",
    "\n",
    "metadata_file = 'top_vocab.tsv'\n",
    "top_words = pd.DataFrame([word_index_rev[i] for i in range(top)])\n",
    "top_words.to_csv(metadata_file, sep = '\\t', index = False, header = False)\n",
    "top_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To terminate the tensorboard visualization in jupyter notebook we can go to the dropdown menu at the top: `Kernel -> Interrupt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
