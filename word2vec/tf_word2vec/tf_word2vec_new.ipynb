{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word2vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tensorflow-Word2vec\" data-toc-modified-id=\"Tensorflow-Word2vec-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tensorflow Word2vec</a></span></li><li><span><a href=\"#Gensim-Word2vec\" data-toc-modified-id=\"Gensim-Word2vec-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Gensim Word2vec</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2019-06-05 19:00:27 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 7.5.0\n",
      "\n",
      "numpy 1.16.3\n",
      "pandas 0.24.2\n",
      "sklearn 0.20.3\n",
      "matplotlib 3.0.3\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# change default style figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class MovieDataUtils:\n",
    "    \"\"\"\n",
    "    Utility class that downloads the polarity data to disk and load it into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    MOVIE_FOLDER_NAME = 'rt-polaritydata'\n",
    "    POS_FILE_NAME = 'rt-polarity.pos'\n",
    "    NEG_FILE_NAME = 'rt-polarity.neg'\n",
    "    MOVIE_DATA_URL = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "\n",
    "    def load_data(self, save_folder_name: str='data'):\n",
    "        movie_folder_path = os.path.join(save_folder_name, MovieDataUtils.MOVIE_FOLDER_NAME)\n",
    "        pos_file_path = os.path.join(movie_folder_path, MovieDataUtils.POS_FILE_NAME)\n",
    "        neg_file_path = os.path.join(movie_folder_path, MovieDataUtils.NEG_FILE_NAME)\n",
    "        \n",
    "        if not os.path.exists(save_folder_name):\n",
    "            os.makedirs(save_folder_name, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(movie_folder_path):\n",
    "            self.download_and_extract_tar_gz_data(save_folder_name)\n",
    "\n",
    "        with open(pos_file_path, encoding='latin-1') as f:\n",
    "            pos_data = f.readlines()\n",
    "\n",
    "        with open(neg_file_path, encoding='latin-1') as f:\n",
    "            neg_data = f.readlines()\n",
    "\n",
    "        texts = pos_data + neg_data\n",
    "        target = [1] * len(pos_data) + [0] * len(neg_data)\n",
    "        return texts, target\n",
    "\n",
    "    def download_and_extract_tar_gz_data(self, save_folder_name: str):\n",
    "        import tarfile\n",
    "        import requests\n",
    "        response = requests.get(MovieDataUtils.MOVIE_DATA_URL, stream=True)\n",
    "\n",
    "        tar_gz_file = os.path.join(save_folder_name, MovieDataUtils.MOVIE_FOLDER_NAME + '.tar.gz')\n",
    "        with open(tar_gz_file, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        tar = tarfile.open(tar_gz_file, 'r:gz')\n",
    "        tar.extractall(path=save_folder_name)\n",
    "        tar.close()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample size: 10662\n",
      "example text:  the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "corresponding target: 1\n"
     ]
    }
   ],
   "source": [
    "movie_data_utils = MovieDataUtils()\n",
    "texts, target = movie_data_utils.load_data()\n",
    "print('total sample size:', len(texts))\n",
    "print('example text: ', texts[0])\n",
    "print('corresponding target:', target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that normalizes/cleans the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def normalize_texts(texts, stop_words=None, min_token_len=2, remove_digits=True, remove_punctuation=True):\n",
    "    \n",
    "    if stop_words is None:\n",
    "        stop_words = set()\n",
    "\n",
    "    punc_and_digit = set()\n",
    "    if remove_digits:\n",
    "        punc_and_digit |= set(string.digits)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        punc_and_digit |= set(string.punctuation)\n",
    "\n",
    "    normed_texts = []\n",
    "    for text in texts:\n",
    "        normed_tokens = []\n",
    "        tokens = text.lower().strip().split(' ')\n",
    "\n",
    "        for token in tokens:\n",
    "            if (token not in stop_words and\n",
    "                token not in punc_and_digit and\n",
    "                len(token) >= min_token_len):\n",
    "                normed_tokens.append(token)\n",
    "\n",
    "        normed_text = ' '.join(normed_tokens)\n",
    "        normed_texts.append(normed_text)\n",
    "\n",
    "    return normed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example normalized text:  rock destined 21st century's new conan he's going make splash even greater arnold schwarzenegger jean-claud van damme steven segal\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops_words = stopwords.words('english')\n",
    "texts = normalize_texts(texts, stops_words)\n",
    "print('example normalized text: ', texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50 \n",
    "embedding_size = 200 \n",
    "vocab_size = 10000 \n",
    "generations = 50000 \n",
    "print_loss_every = 500 \n",
    "num_sampled = batch_size // 2\n",
    "window_size = 2  \n",
    "print_valid_every = 2000 \n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts: list[str]\n",
    "\n",
    "    vocab_size : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word2index : dict[str, int]\n",
    "        Each distinct word in the corpus gets map to a numeric index.\n",
    "        e.g. {'UNK': 0, 'film': 1}\n",
    "    \"\"\"\n",
    "    # list[str] convert list of sentences to list of words\n",
    "    words = [token for text in texts for token in text.split()]\n",
    "\n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    word_count = [('UNK', -1)]\n",
    "    word_count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    index2word = []\n",
    "    word2index = {}\n",
    "    for word, _ in word_count:\n",
    "        word2index[word] = len(word2index)\n",
    "        index2word.append(word)\n",
    "\n",
    "    return word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_count):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts: list[str]\n",
    "\n",
    "    min_count : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word2index : dict[str, int]\n",
    "        Each distinct word in the corpus gets map to a numeric index.\n",
    "        e.g. {'UNK': 0, 'film': 1}\n",
    "    \"\"\"\n",
    "    # list[str] convert list of sentences to list of words\n",
    "    words = [token for text in texts for token in text.split()]\n",
    "    \n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "    index2word = ['UNK']\n",
    "    word2index = {'UNK': 0}\n",
    "    for word, count in word_count.items():\n",
    "        if count >= min_count:\n",
    "            word2index[word] = len(word2index)\n",
    "            index2word.append(word)\n",
    "\n",
    "    return word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_index(texts, word2index):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    texts_index : list[list[int]]\n",
    "        e.g. [[0, 2], [3, 1]]\n",
    "        each element in the outer list is the sentence, e.g. [0, 2]\n",
    "        and each element in the inner list is each word represented in numeric index.\n",
    "    \"\"\"\n",
    "    texts_index = []\n",
    "    for text in texts:\n",
    "        text_index = [word2index.get(token, 0) for token in text.split()]\n",
    "        texts_index.append(text_index)\n",
    "\n",
    "    return texts_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  4296\n"
     ]
    }
   ],
   "source": [
    "word2index, index2word = build_vocab(texts, min_count=5)\n",
    "indexed_texts = texts_to_index(texts, word2index)\n",
    "print('vocabulary size: ', len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: check how the batch data are generated !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_and_label(indexed_texts, window_size, i):\n",
    "    start_slice = max(i - window_size, 0)\n",
    "    end_slice = i + window_size + 1\n",
    "    window = indexed_texts[start_slice:end_slice]\n",
    "    label = i if i < window_size else window_size\n",
    "    return window, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indexed_texts = [148, 4, 101, 29, 53, 6956, 9, 207, 518]\n",
    "window, label = create_window_and_label(rand_indexed_texts, window_size=2, i=0)\n",
    "assert window == [148, 4, 101]\n",
    "assert label == 0\n",
    "\n",
    "window, label = create_window_and_label(rand_indexed_texts, window_size=2, i=3)\n",
    "assert window == [4, 101, 29, 53, 6956]\n",
    "assert label == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(indexed_texts, batch_size, window_size, method='skip_gram'):\n",
    "    batch_data = []\n",
    "    batch_label = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # list[int]\n",
    "        rand_indexed_texts = np.random.choice(indexed_texts)\n",
    "\n",
    "        # print(rand_indexed_texts)\n",
    "        for i in range(len(rand_indexed_texts)):\n",
    "            window, label = create_window_and_label(rand_indexed_texts, window_size, i)\n",
    "            # print(window, label)\n",
    "            center, context = window[label], window[:label] + window[(label + 1):]\n",
    "            center = [center] * len(context)\n",
    "            if method == 'skip_gram':\n",
    "                batch_data.extend(center)\n",
    "                batch_label.extend(context)\n",
    "            elif method == 'cbow':\n",
    "                batch_data.extend(context)\n",
    "                batch_label.extend(center)\n",
    "\n",
    "    # trim batch and label at the end and convert to numpy array\n",
    "    batch_data = np.array(batch_data[:batch_size])\n",
    "    batch_label = np.array(batch_label[:batch_size]).reshape(-1, 1)\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1644 1644  789  789  789 1052 1052 1052 1052  391  391  391  391   27\n",
      "   27   27   27  657  657  657  657 4066 4066 4066 4066  197  197  197\n",
      "  197  489  489  489]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data, batch_label = generate_batch_data(indexed_texts, 32, 2)\n",
    "print(batch_data)\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "method = 'skip_gram'\n",
    "epochs = 10000\n",
    "\n",
    "batch_size = 32 \n",
    "embed_size = 100 \n",
    "vocab_size = len(word2index) \n",
    "num_neg_samples = 5\n",
    "window_size = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:15<00:00, 132.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_word2vec.TfWord2vec at 0x1395b7ef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_word2vec import TfWord2vec\n",
    "\n",
    "model_tf_word2vec = TfWord2vec(batch_size, embed_size, vocab_size, window_size,\n",
    "                               num_neg_samples, epochs, learning_rate, method)\n",
    "model_tf_word2vec.fit(indexed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.9388548),\n",
       " ('UNK', 0.93558717),\n",
       " ('film', 0.93402964),\n",
       " ('like', 0.9303813),\n",
       " ('one', 0.9191068),\n",
       " ('story', 0.91847056),\n",
       " ('--', 0.89264494),\n",
       " ('much', 0.8769109),\n",
       " ('may', 0.8745483),\n",
       " ('funny', 0.8658905)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_word2vec import most_similar\n",
    "\n",
    "\n",
    "most_similar(model_tf_word2vec.embed_in_, word2index, index2word, ['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('around', 0.4175861),\n",
       " ('dig', 0.3923855),\n",
       " ('edge', 0.3674501),\n",
       " ('ease', 0.35598972),\n",
       " ('allen', 0.33458835),\n",
       " ('[the', 0.3266014),\n",
       " ('cartoons', 0.31436068),\n",
       " ('attractive', 0.3120032),\n",
       " (\"moore's\", 0.31183684),\n",
       " ('interaction', 0.31126958)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(model_tf_word2vec.embed_out_, word2index, index2word, ['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.utils import save_as_line_sentence\n",
    "\n",
    "corpus_file = 'polaritydata.txt'\n",
    "corpus = [text.split(' ') for text in texts]\n",
    "save_as_line_sentence(corpus, corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  4295\n",
      "elapse time:  2.1541221141815186\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from joblib import cpu_count\n",
    "\n",
    "start_time = time.time()\n",
    "model_word2vec = Word2Vec(corpus_file=corpus_file, iter=10, size=100, alpha=0.05, sg=0,\n",
    "                          workers=cpu_count())\n",
    "elapse_time = time.time() - start_time\n",
    "\n",
    "print('vocabulary size: ', len(model_word2vec.wv.vocab))\n",
    "print('elapse time: ', elapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyuliu/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('idea', 0.8201919794082642),\n",
       " ('intentions', 0.7895775437355042),\n",
       " ('job', 0.7637943029403687),\n",
       " ('pretty', 0.7432918548583984),\n",
       " ('except', 0.7155461311340332),\n",
       " ('sign', 0.7142113447189331),\n",
       " ('talent', 0.7096766233444214),\n",
       " ('playing', 0.6989737749099731),\n",
       " ('girl', 0.6977887153625488),\n",
       " ('funnier', 0.6914594173431396)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec.wv.most_similar(positive=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  4295\n",
      "elapse time:  6.305377006530762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyuliu/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('intentions', 0.5170398950576782),\n",
       " ('ops', 0.48557227849960327),\n",
       " ('clean', 0.45620492100715637),\n",
       " ('club', 0.429222971200943),\n",
       " ('deliver', 0.42575153708457947),\n",
       " ('idea', 0.4245274066925049),\n",
       " ('rousing', 0.41510117053985596),\n",
       " ('tell', 0.4081522524356842),\n",
       " ('dynamic', 0.40439286828041077),\n",
       " ('necessarily', 0.4020853042602539)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_word2vec = Word2Vec(corpus_file=corpus_file, iter=30, size=100, alpha=0.05, sg=1,\n",
    "                          workers=cpu_count())\n",
    "elapse_time = time.time() - start_time\n",
    "\n",
    "print('vocabulary size: ', len(model_word2vec.wv.vocab))\n",
    "print('elapse time: ', elapse_time)\n",
    "model_word2vec.wv.most_similar(positive=['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cbow: similar to skipgram except we are predicting a single target word from a surrounding window of context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Jupyter Notebook: Gensim Docs - 2Vec File-based Training: API Tutorial](https://nbviewer.jupyter.org/github/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
