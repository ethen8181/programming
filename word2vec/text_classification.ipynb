{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# install fastText\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastText\n",
    "make\n",
    "\n",
    "# upgrade tensorflow to version 1.2.1 for python3 on mac\n",
    "pip install --upgrade \\\n",
    " https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.1-py3-none-any.whl\n",
    " \n",
    "pip install dask_searchcv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-07-17 16:11:40 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.4.1\n",
      "\n",
      "numpy 1.13.1\n",
      "pandas 0.20.2\n",
      "sklearn 0.18.1\n",
      "joblib 0.11\n",
      "matplotlib 2.0.2\n",
      "gensim 2.2.0\n",
      "xgboost 0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "from joblib import cpu_count\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,joblib,matplotlib,gensim,xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"download Reuters' text categorization benchmarks from its url\"\"\"\n",
    "    \n",
    "    train_data = 'r8-train-no-stop.txt'\n",
    "    test_data = 'r8-test-no-stop.txt'\n",
    "    concat_data = 'r8-no-stop.txt'\n",
    "    base_url = 'http://www.cs.umb.edu/~smimarog/textmining/datasets/'\n",
    "    \n",
    "    # brew install wget\n",
    "    # on a mac if you don't have it\n",
    "    if not os.path.isfile(train_data):\n",
    "        call('wget ' + base_url + train_data, shell = True)\n",
    "\n",
    "    if not os.path.isfile(test_data):\n",
    "        call('wget ' + base_url + test_data, shell = True)\n",
    "\n",
    "    if not os.path.isfile(concat_data):\n",
    "        # concatenate train and test files, we'll make our own train-test splits\n",
    "        # the > piping symbol directs the concatenated file to a new file, it\n",
    "        # will replace the file if it already exists; on the other hand, the >> symbol\n",
    "        # will append if it already exists\n",
    "        call('cat r8-*-no-stop.txt > ' + concat_data, shell = True)\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_glove():\n",
    "    \"\"\"download GloVe word vector representations, this step may take a while\"\"\"\n",
    "\n",
    "    # bunch of small embeddings - trained on 6B tokens - 822 MB download, 2GB unzipped\n",
    "    glove_small = 'glove.6B.zip'\n",
    "    base_url = 'http://nlp.stanford.edu/data/'\n",
    "    if not os.path.isfile(glove_small):\n",
    "        call('wget ' + base_url + glove_small, shell = True)\n",
    "        call('unzip ' + glove_small, shell = True)\n",
    "\n",
    "    # and a single behemoth - trained on 840B tokens - 2GB compressed, 5GB unzipped\n",
    "    # glove_big = 'glove.840B.300d.zip'\n",
    "    # if not os.path.isfile(glove_big):\n",
    "    #    call('wget ' + base_url + glove_big, shell = True)\n",
    "    #    call('unzip ' + glove_big, shell = True)\n",
    "\n",
    "        \n",
    "download_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"r8-no-stop.txt\"\n",
    "GLOVE_6B_50D_PATH = \"glove.6B.50d.txt\"\n",
    "# GLOVE_840B_300D_PATH = \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 7674\n"
     ]
    }
   ],
   "source": [
    "# TODO : include spacy ???\n",
    "\n",
    "X, y = [], []\n",
    "with open(DATA_PATH) as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        # texts are already tokenized, just split on space\n",
    "        # in a real case we would use e.g. spaCy for tokenization\n",
    "        # and maybe remove stopwords etc.\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "\n",
    "\n",
    "X, y = np.asarray(X), np.asarray(y)\n",
    "print('total examples {}'.format(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 1234, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# workers = cpu_count()\n",
    "# word2vec = Word2Vec(\n",
    "#     X_train_text, size = 100, window = 5, min_count = 5, workers = workers)\n",
    "\n",
    "# # If we’re finished training a model (i.e. no more updates, only querying)\n",
    "# # we can store the wordvectors and delete the model to trim unneeded model memory\n",
    "# word_vectors = word2vec.wv\n",
    "# del word2vec\n",
    "\n",
    "\n",
    "# w2v = {w: vec for w, vec in zip(word_vectors.index2word, word_vectors.syn0)}\n",
    "# len(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec model consists of a feature vector for each word in the vocabulary, stored in a numpy array called `syn0`. The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector (dimensionality of the feature vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18935"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH) as f:\n",
    "    for line in f:\n",
    "        splitted = line.split()\n",
    "        word = splitted[0]\n",
    "        vector = [float(x) for x in splitted[1:]]\n",
    "        if word in all_words:\n",
    "            glove_small[word] = np.array(vector)\n",
    "            \n",
    "len(glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dask_searchcv as dcv\n",
    "# from main import Word2Vectorizer\n",
    "# from scipy.stats import randint, uniform\n",
    "\n",
    "# # specify the pipeline and the parameter that's going\n",
    "# # be tuned in the pipeline\n",
    "# word2vec = Word2Vectorizer(\n",
    "#     size = 100, window = 5, min_count = 5)\n",
    "\n",
    "# # for xgboost, set number of estimator to a large number\n",
    "# # and the learning rate to be a small number, we'll simply \n",
    "# # let early stopping decide when to stop training;\n",
    "\n",
    "# # word2vec approach was to reduce the dimensionality of the problem\n",
    "# # so that those superior methods can be effectively used\n",
    "# xgb = XGBClassifier(learning_rate = 0.05, n_estimators = 80, n_jobs = -1)\n",
    "# w2v_xgb = Pipeline([\n",
    "#     ('w2v', word2vec), \n",
    "#     ('xgb', xgb)\n",
    "# ])\n",
    "\n",
    "# # when setting parameters of the various steps in a Pipeline,\n",
    "# # we can use the name (first element in the tuple) and \n",
    "# # the parameter name separated by '__'\n",
    "\n",
    "# # subsample, colsample_bytree and max_depth are presumably the most\n",
    "# # common way to control under/overfitting for tree-based models\n",
    "# w2v_xgb_params = {'w2v__tfidf': [True, False],\n",
    "#                   'xgb__max_depth': randint(low = 3, high = 12),\n",
    "#                   'xgb__colsample_bytree': uniform(loc = 0.8, scale = 0.2),\n",
    "#                   'xgb__subsample': uniform(loc = 0.8, scale = 0.2)}\n",
    "\n",
    "# name, model, params = ('w2v_xgb', w2v_xgb, w2v_xgb_params)\n",
    "\n",
    "# # a drop-in replacement for scikit-learn's RandomSearchCV that's\n",
    "# # more efficient at doing Pipeline parameter tuning\n",
    "# # http://dask-searchcv.readthedocs.io/en/latest/;\n",
    "\n",
    "# # note that in scikit-learn 0.19 the Pipeline also has a parameter\n",
    "# # that allows you to cache the intermediate process, which might\n",
    "# # reduce the need of this extra package\n",
    "# # http://scikit-learn.org/dev/modules/pipeline.html#caching-transformers-avoid-repeated-computation\n",
    "# random_search = dcv.RandomizedSearchCV(model, params, cv = 3)\n",
    "# random_search.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify all possible pipelines\n",
    "from main import Word2Vectorizer\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# TODO: ??? chi2 test for controlling max_features\n",
    "tfidf = TfidfVectorizer(analyzer = lambda x: x,\n",
    "                        stop_words = 'english')\n",
    "\n",
    "# the word2vec approach was to reduce the dimensionality of\n",
    "# the problem so that those superior methods can be effectively used\n",
    "word2vec = Word2Vectorizer(size = 100, window = 5, min_count = 5)\n",
    "\n",
    "# SVM is particularly well suited for problems with very \n",
    "# highly dimensional, sparse feature vectors, such as text data\n",
    "svm = LinearSVC()\n",
    "\n",
    "# logistic regression is also known for its interpretability and\n",
    "# fast training time\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# for xgboost, set number of estimator to a large number\n",
    "# and the learning rate to be a small number, we'll simply \n",
    "# let early stopping decide when to stop training;\n",
    "xgb = XGBClassifier(learning_rate = 0.05, n_estimators = 80, n_jobs = -1)\n",
    "\n",
    "tfidf_svm = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('svm', svm)\n",
    "])\n",
    "\n",
    "tfidf_logistic = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('logistic', logistic)\n",
    "])\n",
    "\n",
    "w2v_xgb = Pipeline([\n",
    "    ('w2v', word2vec), \n",
    "    ('xgb', xgb)\n",
    "])\n",
    "\n",
    "w2v_svm = Pipeline([\n",
    "    ('w2v', word2vec), \n",
    "    ('svm', svm)\n",
    "])\n",
    "\n",
    "w2v_logistic = Pipeline([\n",
    "    ('w2v', word2vec), \n",
    "    ('logistic', logistic)\n",
    "])\n",
    "\n",
    "glove_pretrained = Word2Vectorizer(w2v = GLOVE_6B_50D_PATH)\n",
    "glove_pretrained_xgb = Pipeline([\n",
    "    ('w2v', glove_pretrained), \n",
    "    ('xgb', xgb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the parameter that's going\n",
    "# be tuned in all the pipelines\n",
    "\n",
    "# hyperparameters' dictionary:\n",
    "# when setting parameters of the various steps in a Pipeline,\n",
    "# we can use the name (first element in the tuple) and \n",
    "# the parameter name separated by '__'\n",
    "tfidf_params = {'tfidf__sublinear_tf': [True, False],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
    "logistic_params = {'logistic__C': uniform(loc = 1, scale = 0.5)}\n",
    "svm_params = {'svm__loss': ['hinge', 'squared_hinge']}\n",
    "w2v_params = {'w2v__tfidf': [True, False]}\n",
    "\n",
    "# subsample, colsample_bytree and max_depth are presumably the most\n",
    "# common way to control under/overfitting for tree-based models\n",
    "xgb_params = {'xgb__max_depth': randint(low = 3, high = 12),\n",
    "              'xgb__colsample_bytree': uniform(loc = 0.8, scale = 0.2),\n",
    "              'xgb__subsample': uniform(loc = 0.8, scale = 0.2)}\n",
    "\n",
    "tfidf_svm_params = {**tfidf_params, **svm_params}\n",
    "tfidf_logistic_params = {**tfidf_params, **logistic_params}\n",
    "w2v_svm_params = {**w2v_params, **svm_params}\n",
    "w2v_xgb_params = {**w2v_params, **xgb_params}\n",
    "w2v_logistic_params = {**w2v_params, **logistic_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "      <th>estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_svm</td>\n",
       "      <td>0.972960</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_logistic</td>\n",
       "      <td>0.942336</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2v_svm</td>\n",
       "      <td>0.943476</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v_xgb</td>\n",
       "      <td>0.934680</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v_logistic</td>\n",
       "      <td>0.911386</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove_pretrained_xgb</td>\n",
       "      <td>0.924906</td>\n",
       "      <td>RandomizedSearchCV(cache_cv=True, cv=3, error_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name     score  \\\n",
       "0             tfidf_svm  0.972960   \n",
       "1        tfidf_logistic  0.942336   \n",
       "2               w2v_svm  0.943476   \n",
       "3               w2v_xgb  0.934680   \n",
       "4          w2v_logistic  0.911386   \n",
       "5  glove_pretrained_xgb  0.924906   \n",
       "\n",
       "                                           estimator  \n",
       "0  RandomizedSearchCV(cache_cv=True, cv=3, error_...  \n",
       "1  RandomizedSearchCV(cache_cv=True, cv=3, error_...  \n",
       "2  RandomizedSearchCV(cache_cv=True, cv=3, error_...  \n",
       "3  RandomizedSearchCV(cache_cv=True, cv=3, error_...  \n",
       "4  RandomizedSearchCV(cache_cv=True, cv=3, error_...  \n",
       "5  RandomizedSearchCV(cache_cv=True, cv=3, error_...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask_searchcv as dcv\n",
    "\n",
    "cv = 3\n",
    "n_iter = 3\n",
    "all_models = [\n",
    "    ('tfidf_svm', tfidf_svm, tfidf_svm_params),\n",
    "    ('tfidf_logistic', tfidf_logistic, tfidf_logistic_params),\n",
    "    ('w2v_svm', w2v_svm, w2v_svm_params),\n",
    "    ('w2v_xgb', w2v_xgb, w2v_xgb_params),\n",
    "    ('w2v_logistic', w2v_logistic, w2v_logistic_params),\n",
    "    ('glove_pretrained_xgb', glove_pretrained_xgb, w2v_xgb_params)\n",
    "]\n",
    "\n",
    "all_models_info = []\n",
    "for name, model, params in all_models:\n",
    "    # a drop-in replacement for scikit-learn's RandomSearchCV that's\n",
    "    # more efficient at doing Pipeline parameter tuning\n",
    "    # http://dask-searchcv.readthedocs.io/en/latest/\n",
    "\n",
    "    # note that in scikit-learn 0.19 the Pipeline also has a parameter\n",
    "    # that allows you to cache the intermediate process, which might\n",
    "    # reduce the need of this extra package\n",
    "    # http://scikit-learn.org/dev/modules/pipeline.html#caching-transformers-avoid-repeated-computation\n",
    "    random_search = dcv.RandomizedSearchCV(model, params, cv = cv, n_iter = n_iter)\n",
    "    random_search.fit(X_train_text, y_train)\n",
    "    info = name, random_search.best_score_, random_search\n",
    "    all_models_info.append(info)\n",
    "\n",
    "results = pd.DataFrame(all_models_info, columns = ['model_name', 'score', 'estimator'])\n",
    "results = (results\n",
    "           .sort_values('score', ascending = False)\n",
    "           .reset_index(drop = True))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle popcorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join('data', 'labeledTrainData.tsv')\n",
    "data = pd.read_csv(data_path, delimiter = '\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_col = 'sentiment'\n",
    "feature_col = 'review'\n",
    "\n",
    "X_train, X_val = train_test_split(data, test_size = 0.2, \n",
    "                                  random_state = 1234,\n",
    "                                  stratify = data[label_col])\n",
    "\n",
    "X_train_text, X_val_text = X_train[feature_col], X_val[feature_col]\n",
    "y_train, y_val = X_train[label_col].values, X_val[label_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9823     Five years after the original Creepshow, anoth...\n",
       "15954    OK - you want to test somebody on how comforta...\n",
       "21970    Larry Burrows has the distinct feeling he's mi...\n",
       "4005     I don't know much about film-making, but good ...\n",
       "22769    I really have problems rating this movie. It i...\n",
       "13377    This \\film\\\" attempts to follow the genre of l...\n",
       "6757     Belushi at his most ingratiating and Courtney ...\n",
       "6772     I rented Zero Day from the local video store l...\n",
       "18205    I saw 'New York: I Love You' today and loved i...\n",
       "5105     This is full of major spoilers, so beware.<br ...\n",
       "7019     Why do I watch movies like this ? - other than...\n",
       "12886    Hal Hartley's Henry Fool was an independent fi...\n",
       "9129     I'm an incorrigible skeptic and agnostic and w...\n",
       "11594    i found it highly intellectual and artistic in...\n",
       "3731     The best Laurel and Hardy shorts are filled to...\n",
       "15297    The exploding zeppelins crashing down upon 'Sk...\n",
       "18701    With the releasing of \\Farligt förflutet\\\" Swe...\n",
       "6606     This, for lack of a better term, movie is lous...\n",
       "21648    (spoilers?)<br /><br />while the historical ac...\n",
       "14861    Honestly, I went to see the movie, not because...\n",
       "15955    In the opinion of several of my friends and fa...\n",
       "3353     God Bless 80's slasher films. This is a fun, f...\n",
       "121      Yes, I am just going to tell you about this on...\n",
       "5286     pokemon the movie was a terrible film. unlike ...\n",
       "5762     I loves this movie,because it showed that they...\n",
       "1674     I thought this movie seemed like a case study ...\n",
       "5108     This is a poor film by any standard. The story...\n",
       "3283     Now, I won't deny that when I purchased this o...\n",
       "10991    This movie is gorgeous. It's real and down to ...\n",
       "13190    I took part in a little mini production of thi...\n",
       "                               ...                        \n",
       "12909    I loved the original. It was brilliant and alw...\n",
       "6279     I have an awful pan-and-scan videotape of \\Boo...\n",
       "20241    I definitely recommend reading the book prior ...\n",
       "2759     Although the director tried(the filming was ma...\n",
       "19146    I first heard about White Noise when I saw the...\n",
       "8094     This is a small film , few characters ,theatri...\n",
       "21660    This movie was portrayed in the trailer as a c...\n",
       "6129     What can I say? An excellent end to an excelle...\n",
       "2960     No,<br /><br />Basically your watching somethi...\n",
       "14211    When the scientist and family man Matt Winslow...\n",
       "4818     Wow. What a terrible adaptation of a beautiful...\n",
       "8729     I am a MAJOR fan of the horror genre! I LOVE h...\n",
       "22296    A must see by all - if you have to borrow your...\n",
       "5240     This long winded film turns out to be less abo...\n",
       "737      I bought this film on DVD so I could get an ep...\n",
       "6342     Saying this movie is extremely hard to follow ...\n",
       "45       I loved the episode but seems to me there shou...\n",
       "11325    I saw this recently on a cable channel. The mo...\n",
       "3401     My comments on this movie have been deleted tw...\n",
       "12784    The planning episodes were a bit dull, but whe...\n",
       "6469     yeah, it's a bit of a silly film, so if you ar...\n",
       "3754     If my memory is correct, when this movie was r...\n",
       "18435    The Forest isn't just your everyday standard s...\n",
       "15881    They probably could have skipped some of the b...\n",
       "2561     This movie is one reason IMDB should allow a v...\n",
       "9988     I guess this movie will only work on people wh...\n",
       "1772     After seeing the credits with only one name th...\n",
       "12175    I just saw this movie for the first time ever ...\n",
       "11762    I ordered this extremely rare and highly overr...\n",
       "6592     This movie fails miserably on every level. I h...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x40000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2173373 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(\n",
    "    stop_words = 'english', max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)\n",
    "\n",
    "X_train = vect.fit_transform(X_train_text)\n",
    "X_val = vect.transform(X_val_text)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic AUC train: 0.98719432\n",
      "logistic AUC val: 0.95929152\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "logistic_proba_train = logistic.predict_proba(X_train)[:, 1]\n",
    "logistic_auc_train = roc_auc_score(y_train, logistic_proba_train)\n",
    "logistic_proba_val = logistic.predict_proba(X_val)[:, 1]\n",
    "logistic_auc_val = roc_auc_score(y_val, logistic_proba_val)\n",
    "\n",
    "print('logistic AUC train:', logistic_auc_train)\n",
    "print('logistic AUC val:', logistic_auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_path = os.path.join('data', 'testData.tsv')\n",
    "df_test = pd.read_csv(testdata_path, delimiter = '\\t')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vect.transform(df_test[feature_col])\n",
    "logistic_proba_val = logistic.predict_proba(X_test)[:, 1]\n",
    "df_test['sentiment'] = logistic_proba_val\n",
    "df_test = df_test.drop(feature_col, axis = 1)\n",
    "df_test.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
