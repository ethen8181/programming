{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-07-05 20:14:52 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from graphviz import Source\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eli5 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "feature_names = boston.feature_names\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 14)\n",
    "\n",
    "model_xgb = XGBClassifier(n_estimators = 1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "accuracy_score(y_test, model_xgb.predict(X_test))\n",
    "\n",
    "# model_xgb = XGBRegressor(n_estimators = 30)\n",
    "# model_xgb.fit(X_train, y_train)\n",
    "# r2_score(y_test, model_xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_feature_imp(model_xgb, feature_names = None, importance_type = 'gain'):\n",
    "    # XGBClassifier is a scikit-learn like\n",
    "    # wrapper around the actual underlying model\n",
    "    booster = model_xgb.get_booster()\n",
    "    \n",
    "    # get the normalized feature importance (sum up to 1)\n",
    "    score = booster.get_score(importance_type = importance_type)\n",
    "    all_features_score = np.array(\n",
    "        [score.get(f, 0.) for f in booster.feature_names], dtype = np.float32)\n",
    "    \n",
    "    normed_score = all_features_score / np.sum(all_features_score)\n",
    "    \n",
    "    # construct a dataframe with the feature name mapping with the score\n",
    "    if feature_names is None:\n",
    "        feature_names = booster.feature_names\n",
    "\n",
    "    feature_imp = {'weight': normed_score, 'feature': feature_names}\n",
    "    df_feature_imp = pd.DataFrame(feature_imp)[['feature', 'weight']]\n",
    "    df_feature_imp = (df_feature_imp\n",
    "                      .sort_values('feature')\n",
    "                      .reset_index(drop = True))\n",
    "    return df_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.442693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.557307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature    weight\n",
       "0  petal length (cm)  0.442693\n",
       "1   petal width (cm)  0.557307\n",
       "2  sepal length (cm)  0.000000\n",
       "3   sepal width (cm)  0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_imp = xgb_feature_imp(model_xgb, feature_names = feature_names)\n",
    "df_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import Booster, XGBRegressor, XGBClassifier\n",
    "\n",
    "def _check_booster_args(xgb):\n",
    "    if isinstance(xgb, Booster):\n",
    "        booster = xgb\n",
    "    else:\n",
    "        booster = xgb.get_booster()\n",
    "        regression = isinstance(xgb, XGBRegressor)\n",
    "\n",
    "    return booster, regression\n",
    "\n",
    "\n",
    "booster, regression = _check_booster_args(model_xgb)\n",
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb regression n_targets\n",
    "n_targets = 1\n",
    "if isinstance(model_xgb, XGBClassifier):\n",
    "    n_targets = 1 if model_xgb.n_classes_ == 2 else model_xgb.n_classes_\n",
    "\n",
    "n_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['y']\n",
    "if not regression:\n",
    "    names = model_xgb.classes_\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.atleast_2d(X_train[0])\n",
    "\n",
    "# if regression:\n",
    "# proba = model_xgb.predict_proba(X)[0]\n",
    "# proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:[f2<2.3] yes=1,no=2,missing=1,gain=54.04,cover=53.3333\\n\\t1:leaf=0.141176,cover=16\\n\\t2:leaf=-0.0730435,cover=37.3333\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_dumps = booster.get_dump(with_stats = True)\n",
    "tree_dumps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16.0, 'leaf': 0.141176, 'node_id': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'node_id': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'node_id': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _parse_dump_line(line):\n",
    "    # match the branch pattern, e.g.\n",
    "    # \\t1:[f2<2.3] yes=3,no=4,missing=3,gain=34.829,cover=35.5556'\n",
    "    # may have 0 or more than 1 '\\t' at the beginning, which is \n",
    "    # used for indicating depth of branch when printed\n",
    "    branch_match = re.match(\n",
    "        '^(\\t*)(\\d+):\\[(.+)<(.+)\\] '\n",
    "        'yes=(\\d+),no=(\\d+),missing=(\\d+),'\n",
    "        'gain=(.+),cover=(.+)$', line)\n",
    "    \n",
    "    if branch_match is not None:\n",
    "        matched = branch_match.groups()\n",
    "        n_tabs = matched[0]\n",
    "        depth = len(n_tabs)\n",
    "        branch_info = {'depth': depth,\n",
    "                       'node_id': int(matched[1]),\n",
    "                       'split': matched[2],\n",
    "                       'split_condition': float(matched[3]),\n",
    "                       'yes': int(matched[4]),\n",
    "                       'no': int(matched[5]),\n",
    "                       'missing': int(matched[6]),\n",
    "                       'gain': float(matched[7]),\n",
    "                       'cover': float(matched[8])}\n",
    "        return depth, branch_info\n",
    "\n",
    "    # if it's not a branch, then it has to be a leaf node\n",
    "    # match the leaf pattern, e.g.\n",
    "    # \\t1:leaf=0.141176,cover=16\n",
    "    leaf_match = re.match('^(\\t*)(\\d+):leaf=(.+),cover=(.+)$', line)\n",
    "    n_tabs, node_id, value, cover = leaf_match.groups()\n",
    "    depth = len(n_tabs)\n",
    "    leaf_info = {'node_id': int(node_id),\n",
    "                 'leaf': float(value),\n",
    "                 'cover': float(cover)}\n",
    "    return depth, leaf_info\n",
    "\n",
    "\n",
    "def _parse_tree_dump(text_dump):\n",
    "    \"\"\" Parse text tree dump (one item of a list returned by Booster.get_dump())\n",
    "    into json format that will be used by next XGBoost release.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    stack = []  # type: List[Dict]\n",
    "    for line in text_dump.split('\\n'):\n",
    "        if line:\n",
    "            depth, node = _parse_dump_line(line)\n",
    "            if depth == 0:\n",
    "                assert not stack\n",
    "                result = node\n",
    "                stack.append(node)\n",
    "            elif depth > len(stack):\n",
    "                raise ValueError('Unexpected dump structure')\n",
    "            else:\n",
    "                if depth < len(stack):\n",
    "                    stack = stack[:depth]\n",
    "                stack[-1].setdefault('children', []).append(node)\n",
    "                stack.append(node)\n",
    "    return result\n",
    "\n",
    "text_dump = tree_dumps[0]\n",
    "result = _parse_tree_dump(text_dump)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# XGBClassifier does not have pred_leaf argument as of now, so use booster\n",
    "dmatrix = DMatrix(X, missing = model_xgb.missing)\n",
    "leaf_ids = booster.predict(dmatrix, pred_leaf = True)[0]\n",
    "xgb_feature_names = {f: i for i, f in enumerate(booster.feature_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = booster.get_dump(with_stats = True, dump_format = 'json')\n",
    "len(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'children': [{'cover': 16, 'leaf': -0.0705882, 'nodeid': 3},\n",
       "    {'children': [{'cover': 16.8889, 'leaf': 0.141615, 'nodeid': 7},\n",
       "      {'cover': 2.66667, 'leaf': -3.25116e-09, 'nodeid': 8}],\n",
       "     'cover': 19.5556,\n",
       "     'depth': 2,\n",
       "     'gain': 4.65416,\n",
       "     'missing': 7,\n",
       "     'no': 8,\n",
       "     'nodeid': 4,\n",
       "     'split': 'f2',\n",
       "     'split_condition': 4.95,\n",
       "     'yes': 7}],\n",
       "   'cover': 35.5556,\n",
       "   'depth': 1,\n",
       "   'gain': 34.829,\n",
       "   'missing': 3,\n",
       "   'no': 4,\n",
       "   'nodeid': 1,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 2.3,\n",
       "   'yes': 3},\n",
       "  {'children': [{'cover': 1.33333, 'leaf': -2.55448e-09, 'nodeid': 5},\n",
       "    {'cover': 16.4444, 'leaf': -0.0707006, 'nodeid': 6}],\n",
       "   'cover': 17.7778,\n",
       "   'depth': 1,\n",
       "   'gain': 0.619154,\n",
       "   'missing': 5,\n",
       "   'no': 6,\n",
       "   'nodeid': 2,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 4.85,\n",
       "   'yes': 5}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 12.9454,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f3',\n",
       " 'split_condition': 1.75,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tree_id = 1\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result2 = json.loads(t)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_id = 0\n",
    "\n",
    "# leaf_ids : the leaf id of that example for each tree\n",
    "leaf_id = leaf_ids[tree_id]\n",
    "print(leaf_id)\n",
    "\n",
    "# parse the tree dump into json format\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result1 = json.loads(t)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 0, 'f1': 1, 'f2': 2, 'f3': 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node undefined = -2\n",
    "xgb_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_tree_paths(result):\n",
    "    if 'leaf' not in result:\n",
    "        node_id = result['nodeid']\n",
    "        left_child, right_child = result['children']\n",
    "        left_paths = _get_tree_paths(left_child)\n",
    "        right_paths = _get_tree_paths(right_child)\n",
    "\n",
    "        for path in left_paths:\n",
    "            path.append(node_id)\n",
    "\n",
    "        for path in right_paths:\n",
    "            path.append(node_id)\n",
    "\n",
    "        paths = left_paths + right_paths\n",
    "    else:\n",
    "        node_id = result['nodeid']\n",
    "        paths = [[node_id]]\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0], [2, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = _get_tree_paths(result1)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [0, 1], 2: [0, 2]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_count = -1\n",
    "\n",
    "leaf_to_path = {}\n",
    "for path in paths:\n",
    "    path.reverse()\n",
    "    node = path[-1]\n",
    "    leaf_to_path[node] = path\n",
    "    \n",
    "    if node > node_count:\n",
    "        node_count = node\n",
    "        \n",
    "node_count += 1\n",
    "        \n",
    "print(node_count)   \n",
    "leaf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the feature, value, where index i holds the feature\n",
    "# and value for node i \n",
    "\n",
    "def _get_feature(result, feature, xgb_feature_names):\n",
    "    node_id = result['nodeid']\n",
    "    if 'leaf' not in result:\n",
    "        feature[node_id] = xgb_feature_names[result['split']]\n",
    "        left_child, right_child = result['children']\n",
    "        _get_feature(left_child, feature, xgb_feature_names)\n",
    "        _get_feature(right_child, feature, xgb_feature_names)\n",
    "    else:\n",
    "        feature[node_id] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -2, -2], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = np.zeros(node_count, dtype = np.int32)\n",
    "_get_feature(result1, feature, xgb_feature_names)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parent_value(children):\n",
    "    \"\"\"\n",
    "    Value of the parent node: a weighted sum of child values.\n",
    "    \"\"\"\n",
    "    covers = np.array([child['cover'] for child in children])\n",
    "    covers /= np.sum(covers)\n",
    "    leafs = np.array([child['leaf'] for child in children])\n",
    "    return np.sum(leafs * covers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.4,  1.5,  0.2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38218513,  0.30924702,  0.30856785]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       " {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_parent(result):\n",
    "    if 'leaf' not in result:\n",
    "        left_child, right_child = result['children']\n",
    "        _get_parent(left_child)\n",
    "        _get_parent(right_child)\n",
    "    else:\n",
    "        result['leaf'] = _parent_value(result['children'])\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-717031172d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_get_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-f62c0d20d9a3>\u001b[0m in \u001b[0;36m_get_parent\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'leaf'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mleft_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0m_get_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0m_get_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-f62c0d20d9a3>\u001b[0m in \u001b[0;36m_get_parent\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0m_get_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leaf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parent_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'children'"
     ]
    }
   ],
   "source": [
    "_get_parent(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'nodeid': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0087776098338186448"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parent_value(result1['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16, 'leaf': -0.0705882, 'nodeid': 3},\n",
       "  {'children': [{'cover': 16.8889, 'leaf': 0.141615, 'nodeid': 7},\n",
       "    {'cover': 2.66667, 'leaf': -3.25116e-09, 'nodeid': 8}],\n",
       "   'cover': 19.5556,\n",
       "   'depth': 2,\n",
       "   'gain': 4.65416,\n",
       "   'missing': 7,\n",
       "   'no': 8,\n",
       "   'nodeid': 4,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 4.95,\n",
       "   'yes': 7}],\n",
       " 'cover': 35.5556,\n",
       " 'depth': 1,\n",
       " 'gain': 34.829,\n",
       " 'missing': 3,\n",
       " 'no': 4,\n",
       " 'nodeid': 1,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 3}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left, right = result2['children']\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 0], [7, 4, 1, 0], [8, 4, 1, 0], [5, 2, 0], [6, 2, 0]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = _get_tree_paths(result2)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: [0, 1, 3], 5: [0, 2, 5], 6: [0, 2, 6], 7: [0, 1, 4, 7], 8: [0, 1, 4, 8]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_count = -1\n",
    "\n",
    "leaf_to_path = {}\n",
    "for path in paths:\n",
    "    path.reverse()\n",
    "    node = path[-1]\n",
    "    leaf_to_path[node] = path\n",
    "    if node > node_count:\n",
    "        node_count = node\n",
    "        \n",
    "node_count += 1\n",
    "        \n",
    "print(node_count)   \n",
    "leaf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'cover': 16,\n",
       "  'leaf': 0.141176,\n",
       "  'nodeid': 1,\n",
       "  'parent': {'children': [{...},\n",
       "    {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2, 'parent': {...}}],\n",
       "   'cover': 53.3333,\n",
       "   'depth': 0,\n",
       "   'gain': 54.04,\n",
       "   'leaf': -0.0087776098338186448,\n",
       "   'missing': 1,\n",
       "   'no': 2,\n",
       "   'nodeid': 0,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 2.3,\n",
       "   'yes': 1}},\n",
       " 2: {'cover': 37.3333,\n",
       "  'leaf': -0.0730435,\n",
       "  'nodeid': 2,\n",
       "  'parent': {'children': [{'cover': 16,\n",
       "     'leaf': 0.141176,\n",
       "     'nodeid': 1,\n",
       "     'parent': {...}},\n",
       "    {...}],\n",
       "   'cover': 53.3333,\n",
       "   'depth': 0,\n",
       "   'gain': 54.04,\n",
       "   'leaf': -0.0087776098338186448,\n",
       "   'missing': 1,\n",
       "   'no': 2,\n",
       "   'nodeid': 0,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 2.3,\n",
       "   'yes': 1}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _indexed_leafs(parent):\n",
    "    \"\"\" Return a leaf nodeid -> node dictionary with\n",
    "    \"parent\" and \"leaf\" (average child \"leaf\" value) added to all nodes.\n",
    "    \"\"\"\n",
    "    if not parent.get('children'):\n",
    "        return {parent['nodeid']: parent}\n",
    "    indexed = {}\n",
    "    for child in parent['children']:\n",
    "        child['parent'] = parent\n",
    "        if 'leaf' in child:\n",
    "            indexed[child['nodeid']] = child\n",
    "        else:\n",
    "            indexed.update(_indexed_leafs(child))\n",
    "    parent['leaf'] = _parent_value(parent['children'])\n",
    "    return indexed\n",
    "\n",
    "def _parent_value(children):\n",
    "    \"\"\" Value of the parent node: a weighted sum of child values.\n",
    "    \"\"\"\n",
    "    covers = np.array([child['cover'] for child in children])\n",
    "    covers /= np.sum(covers)\n",
    "    leafs = np.array([child['leaf'] for child in children])\n",
    "    return np.sum(leafs * covers)\n",
    "\n",
    "tree_id = 0\n",
    "\n",
    "# leaf_ids : the leaf id of that example for each tree\n",
    "leaf_id = leaf_ids[tree_id]\n",
    "print(leaf_id)\n",
    "\n",
    "# parse the tree dump into json format\n",
    "t = booster.get_dump(with_stats = True, dump_format = 'json')[tree_id]\n",
    "result = json.loads(t)\n",
    "result\n",
    "\n",
    "\n",
    "indexed = _indexed_leafs(result)\n",
    "indexed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': [{'cover': 16.0, 'leaf': 0.141176, 'node_id': 1},\n",
       "  {'cover': 37.3333, 'leaf': -0.0730435, 'node_id': 2}],\n",
       " 'cover': 53.3333,\n",
       " 'depth': 0,\n",
       " 'gain': 54.04,\n",
       " 'missing': 1,\n",
       " 'no': 2,\n",
       " 'node_id': 0,\n",
       " 'split': 'f2',\n",
       " 'split_condition': 2.3,\n",
       " 'yes': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cover': 16,\n",
       " 'leaf': 0.141176,\n",
       " 'nodeid': 1,\n",
       " 'parent': {'children': [{...},\n",
       "   {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2, 'parent': {...}}],\n",
       "  'cover': 53.3333,\n",
       "  'depth': 0,\n",
       "  'gain': 54.04,\n",
       "  'leaf': -0.0087776098338186448,\n",
       "  'missing': 1,\n",
       "  'no': 2,\n",
       "  'nodeid': 0,\n",
       "  'split': 'f2',\n",
       "  'split_condition': 2.3,\n",
       "  'yes': 1}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf = indexed[leaf_id]\n",
    "leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'children': [{'cover': 16, 'leaf': 0.141176, 'nodeid': 1, 'parent': {...}},\n",
       "   {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2, 'parent': {...}}],\n",
       "  'cover': 53.3333,\n",
       "  'depth': 0,\n",
       "  'gain': 54.04,\n",
       "  'leaf': -0.0087776098338186448,\n",
       "  'missing': 1,\n",
       "  'no': 2,\n",
       "  'nodeid': 0,\n",
       "  'split': 'f2',\n",
       "  'split_condition': 2.3,\n",
       "  'yes': 1},\n",
       " {'cover': 16,\n",
       "  'leaf': 0.141176,\n",
       "  'nodeid': 1,\n",
       "  'parent': {'children': [{...},\n",
       "    {'cover': 37.3333, 'leaf': -0.0730435, 'nodeid': 2, 'parent': {...}}],\n",
       "   'cover': 53.3333,\n",
       "   'depth': 0,\n",
       "   'gain': 54.04,\n",
       "   'leaf': -0.0087776098338186448,\n",
       "   'missing': 1,\n",
       "   'no': 2,\n",
       "   'nodeid': 0,\n",
       "   'split': 'f2',\n",
       "   'split_condition': 2.3,\n",
       "   'yes': 1}}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0.0\n",
    "score += leaf['leaf']\n",
    "path = [leaf]\n",
    "while 'parent' in path[-1]:\n",
    "    path.append(path[-1]['parent'])\n",
    "path.reverse()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_id = leaf_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '0', 'f3', '1.75', '1', '2', '1', '12.9454', '53.3333')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match the branch pattern, e.g.\n",
    "# \\t1:[f2<2.3] yes=3,no=4,missing=3,gain=34.829,cover=35.5556'\n",
    "# may have 0 or more than 1 '\\t' at the beginning, which is \n",
    "# used for indicating depth of branch when printed\n",
    "tree_dump = tree_dumps[1].split('\\n')[0]\n",
    "\n",
    "\n",
    "import re\n",
    "line = tree_dump.split('\\n')[0]\n",
    "branch_match = re.match(\n",
    "    '^(\\t*)(\\d+):\\[(.+)<(.+)\\] '\n",
    "    'yes=(\\d+),no=(\\d+),missing=(\\d+),'\n",
    "    'gain=(.+),cover=(.+)$', line)\n",
    "matched = branch_match.groups()\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _target_feature_weights(leaf_ids, tree_dumps, feature_names, xgb_feature_names):\n",
    "    feature_weights = np.zeros(len(feature_names))\n",
    "    # All trees in XGBoost give equal contribution to the prediction:\n",
    "    # it is equal to sum of \"leaf\" values in leafs\n",
    "    # before applying loss-specific function\n",
    "    # (e.g. logistic for \"binary:logistic\" loss).\n",
    "    score = 0\n",
    "    for text_dump, leaf_id in zip(tree_dumps, leaf_ids):\n",
    "        leaf = _indexed_leafs(_parse_tree_dump(text_dump))[leaf_id]\n",
    "        score += leaf['leaf']\n",
    "        path = [leaf]\n",
    "        while 'parent' in path[-1]:\n",
    "            path.append(path[-1]['parent'])\n",
    "        path.reverse()\n",
    "        # Check how each split changes \"leaf\" value\n",
    "        for node, child in zip(path, path[1:]):\n",
    "            idx = xgb_feature_names[node['split']]\n",
    "            feature_weights[idx] += child['leaf'] - node['leaf']\n",
    "        # Root \"leaf\" value is interpreted as bias\n",
    "        feature_weights[feature_names.bias_idx] += path[0]['leaf']\n",
    "    return score, feature_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3,\n",
       "       1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3,\n",
       "       3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1,\n",
       "       3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 4, 1, 3, 3, 1, 3, 4, 1, 3, 4, 1, 3, 1,\n",
       "       1, 3, 4, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 4, 1, 3, 1, 1, 3, 3, 1, 3,\n",
       "       1, 1, 3, 3, 1, 3, 3, 1, 5, 7, 1, 3, 1, 0, 5, 3, 0, 5, 3, 0, 5, 3, 0,\n",
       "       5, 3, 0, 5, 3, 0, 3, 3, 0, 5, 3, 0, 5, 2, 0, 3, 3, 0, 5, 2, 0, 3, 3,\n",
       "       0, 3, 1, 0, 5, 1, 0, 3, 1, 0, 5, 2, 0, 3, 1, 0, 5, 2, 0, 3, 1, 0, 3,\n",
       "       1, 0, 5, 1, 0, 3, 1, 0, 1, 1, 0, 5, 3, 0, 1, 1, 0, 1, 1, 0, 3, 3, 0,\n",
       "       1, 1, 0, 1, 1, 0, 3, 3, 0, 1, 1, 0, 1, 1, 0, 5, 3, 0, 1, 1, 0, 1, 1,\n",
       "       0, 5, 3, 0, 1, 1, 0, 5, 1, 0, 1, 3, 0, 1, 1, 0, 5, 3, 0, 1, 1, 0, 5,\n",
       "       1], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import DMatrix\n",
    "\n",
    "output_margin=False\n",
    "ntree_limit=0\n",
    "\n",
    "# each record indicating the predicted leaf index of each sample in each tree\n",
    "leaf_preds = booster.predict(dmatrix,\n",
    "    output_margin=output_margin,\n",
    "    ntree_limit=ntree_limit,\n",
    "    pred_leaf=True)[0]\n",
    "\n",
    "leaf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  3.2012167 ,  0.02203865,  0.00565705,\n",
       "       -0.31680772,  0.        , -1.92371082, -0.03058295,  0.63473058,\n",
       "       -0.77690864, -0.21795982, -2.3625524 , -0.24004447,  0.73383176], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrib_preds = booster.predict(dmatrix,\n",
    "    output_margin=output_margin,\n",
    "    ntree_limit=ntree_limit,\n",
    "    pred_contribs=True)[0]\n",
    "\n",
    "contrib_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:[f2<2.3] yes=1,no=2,missing=1,gain=54.04,cover=53.3333\\n\\t1:leaf=0.141176,cover=16\\n\\t2:leaf=-0.0730435,cover=37.3333\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_dumps = booster.get_dump(with_stats=True)\n",
    "tree_dumps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 3, ..., 0, 5, 1],\n",
       "       [2, 7, 3, ..., 0, 5, 1],\n",
       "       [2, 7, 3, ..., 0, 6, 4],\n",
       "       ..., \n",
       "       [1, 3, 3, ..., 0, 5, 1],\n",
       "       [2, 7, 3, ..., 0, 5, 1],\n",
       "       [2, 6, 6, ..., 0, 2, 4]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.02203865,  0.        ,  0.63473058, -2.3625524 ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = contrib_preds[0]\n",
    "temp[0::n_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7057831"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9350071"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.369698"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[2::n_targets].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "            <thead>\n",
       "            <tr style=\"border: none;\">\n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "                <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "            \n",
       "                <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "                    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                        0.6886\n",
       "                        \n",
       "                    </td>\n",
       "                    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                        f2\n",
       "                    </td>\n",
       "                </tr>\n",
       "            \n",
       "                <tr style=\"background-color: hsl(120, 100.00%, 89.69%); border: none;\">\n",
       "                    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                        0.2672\n",
       "                        \n",
       "                    </td>\n",
       "                    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                        f3\n",
       "                    </td>\n",
       "                </tr>\n",
       "            \n",
       "                <tr style=\"background-color: hsl(120, 100.00%, 97.95%); border: none;\">\n",
       "                    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                        0.0265\n",
       "                        \n",
       "                    </td>\n",
       "                    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                        f0\n",
       "                    </td>\n",
       "                </tr>\n",
       "            \n",
       "                <tr style=\"background-color: hsl(120, 100.00%, 98.46%); border: none;\">\n",
       "                    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                        0.0176\n",
       "                        \n",
       "                    </td>\n",
       "                    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                        f1\n",
       "                    </td>\n",
       "                </tr>\n",
       "            \n",
       "            \n",
       "            </tbody>\n",
       "        </table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\\n       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\\n       silent=True, subsample=1)\", description='\\nXGBoost feature importances; values are numbers 0 <= x <= 1;\\nall values sum to 1.\\n', error=None, method='feature importances', is_regression=False, targets=None, feature_importances=FeatureImportances(importances=[FeatureWeight(feature='f2', weight=0.68861258, std=None, value=None), FeatureWeight(feature='f3', weight=0.26723063, std=None, value=None), FeatureWeight(feature='f0', weight=0.026507288, std=None, value=None), FeatureWeight(feature='f1', weight=0.017649557, std=None, value=None)], remaining=0), decision_tree=None, highlight_spaces=None, transition_features=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eli5 import explain_weights\n",
    "explain_weights(model_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.dump_model('temp.txt')\n",
    "booster = model_xgb.get_booster()\n",
    "original_feature_names = booster.feature_names\n",
    "\n",
    "features_names = iris.feature_names\n",
    "booster.feature_names = features_names\n",
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[petal length (cm)<2.3] yes=1,no=2,missing=1\n",
      "\t1:leaf=0.141176\n",
      "\t2:leaf=-0.0730435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgdump = booster.get_dump()\n",
    "xgdump[0]\n",
    "print(booster.get_dump()[0])\n",
    "# recover original feature names\n",
    "booster.feature_names = original_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Feature Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = boston.data[[300, 309]]\n",
    "print(\"Instance 0 prediction:\", model_tree_reg.predict([instances[0]]))\n",
    "print(\"Instance 1 prediction:\", model_tree_reg.predict([instances[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_to_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contributions = []\n",
    "# for row, leaf in enumerate(leaves):\n",
    "#     path = leaf_to_path[leaf]\n",
    "\n",
    "# leaf = leaves[0]\n",
    "path = leaf_to_path[leaf]\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = list(model_tree.tree_.feature)\n",
    "\n",
    "path_features = set()\n",
    "path_features_dict = {}\n",
    "\n",
    "for depth in range(len(path) - 1):\n",
    "    path_feature = feature[path[depth]]\n",
    "    path_features.add(path_feature)\n",
    "    contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "    \n",
    "    joint_features = tuple(sorted(path_features))\n",
    "    contrib += path_features_dict.get(joint_features, 0)\n",
    "    path_features_dict[joint_features] = contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias - 2.8953765912305585 - 4.9119953416149027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions = []\n",
    "for leaf in leaves:\n",
    "    path = leaf_to_path[leaf]\n",
    "    path_features = set()\n",
    "    path_features_dict = {}\n",
    "\n",
    "    for depth in range(len(path) - 1):\n",
    "        path_feature = feature[path[depth]]\n",
    "        path_features.add(path_feature)\n",
    "        contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "\n",
    "        joint_features = tuple(sorted(path_features))\n",
    "        contrib += path_features_dict.get(joint_features, 0)\n",
    "        path_features_dict[joint_features] = contrib\n",
    "    \n",
    "    contributions.append(path_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contributions = []\n",
    "\n",
    "for leaf in leaves:\n",
    "    # for each leaf, check which is the path\n",
    "    # that it took to get to the leaf\n",
    "    for path in paths:\n",
    "        if leaf == path[-1]:\n",
    "            break\n",
    "    \n",
    "    # compute the contribution of each feature \n",
    "    # for a given observation\n",
    "    contribs = np.zeros(line_shape)\n",
    "    for depth in range(len(path) - 1):\n",
    "        contrib = values[path[depth + 1]] - values[path[depth]]\n",
    "        feature_idx = feature[path[depth]]\n",
    "        contribs[feature_idx] += contrib\n",
    "    \n",
    "    contributions.append(contribs)\n",
    "    \n",
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_predict_tree(model, X, joint_contribution=joint_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tree_explainer import TreeExplainer\n",
    "\n",
    "tree_explain = TreeExplainer(model_tree, iris.feature_names)\n",
    "best_idx, prediction, df_explained = tree_explain.explain(X_train[0])\n",
    "\n",
    "# style the contribution weight\n",
    "# https://pandas.pydata.org/pandas-docs/stable/style.html#Builtin-Styles\n",
    "# http://seaborn.pydata.org/tutorial/color_palettes.html#custom-diverging-palettes-with-diverging-palette\n",
    "cmap = sns.diverging_palette(10, 133, s = 85, l = 60, n = 4, as_cmap = True)\n",
    "df_explained = df_explained.style.background_gradient(cmap = cmap, subset = 'contrib')\n",
    "\n",
    "print('predicted class: ', best_idx)\n",
    "print('prediction: ', prediction)\n",
    "df_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 500)\n",
    "rf.fit(X_train, y_train)\n",
    "accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(X_train, feature_names = iris.feature_names, \n",
    "                                 class_names = iris.target_names, discretize_continuous = True)\n",
    "\n",
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], rf.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.available_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table = True, show_all = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Interpreting random forests](http://blog.datadive.net/interpreting-random-forests/)\n",
    "- [Blog: Random forest interpretation with scikit-learn](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)\n",
    "- [Blog: Random forest interpretation  conditional feature contributions](http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
