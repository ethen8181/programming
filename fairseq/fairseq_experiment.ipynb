{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fairseq-Load-Dataset\" data-toc-modified-id=\"Fairseq-Load-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fairseq Load Dataset</a></span></li><li><span><a href=\"#Fairseq-Scoring\" data-toc-modified-id=\"Fairseq-Scoring-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Fairseq Scoring</a></span></li><li><span><a href=\"#Fairseq-Generate-Flow\" data-toc-modified-id=\"Fairseq-Generate-Flow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fairseq Generate Flow</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairseq Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 1.21MB [00:02, 431kB/s]                             \n",
      "validation.tar.gz: 49.2kB [00:01, 40.5kB/s]                            \n",
      "mmt16_task1_test.tar.gz: 49.2kB [00:01, 41.0kB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en': 'A man in an orange hat starring at something.',\n",
       " 'de': 'Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchnlp.datasets import multi30k_dataset\n",
    "\n",
    "directory = 'data/multi30k'\n",
    "datasets = multi30k_dataset(directory, train=True, dev=True, test=True)\n",
    "datasets[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmt16_task1_test.tar.gz train.de                val.de\r\n",
      "test.de                 train.en                val.en\r\n",
      "test.en                 training.tar.gz         validation.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"\n",
    "    Runs a shell command that returns the output message if any.\n",
    "    Should the command fail, i.e. exit code isn't 0, the program will exit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmd : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : str\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n",
    "                               stderr=subprocess.PIPE, encoding='utf-8', shell=True)\n",
    "    (output, error) = process.communicate()\n",
    "    status = process.returncode\n",
    "    if status != 0:\n",
    "        print(\"ERROR: \" + error)\n",
    "        print(\"ERROR: failed to run command - \" + cmd)\n",
    "        exit(status)\n",
    "\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fairseq-preprocess     --source-lang de     --target-lang en     --trainpref data/multi30k/train     --validpref data/multi30k/val     --testpref data/multi30k/test     --destdir data/multi30k/data     --dataset-impl raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Namespace(alignfile=None, cpu=False, criterion='cross_entropy', dataset_impl='raw', destdir='data/multi30k/data', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='de', srcdict=None, target_lang='en', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='data/multi30k/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='data/multi30k/train', user_dir=None, validpref='data/multi30k/val', workers=1)\\n| Wrote preprocessed data to data/multi30k/data\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lang = 'de'\n",
    "target_lang = 'en'\n",
    "fairseq_preprocess_format = {\n",
    "    'source_lang': source_lang,\n",
    "    'target_lang': target_lang,\n",
    "    'destdir': 'data/multi30k/data'# 'data/multi30k.de-en'\n",
    "}\n",
    "\n",
    "cmd = \"\"\"\n",
    "fairseq-preprocess \\\n",
    "    --source-lang {source_lang} \\\n",
    "    --target-lang {target_lang} \\\n",
    "    --trainpref data/multi30k/train \\\n",
    "    --validpref data/multi30k/val \\\n",
    "    --testpref data/multi30k/test \\\n",
    "    --destdir {destdir} \\\n",
    "    --dataset-impl raw\n",
    "\"\"\".format(**fairseq_preprocess_format)\n",
    "print(cmd)\n",
    "\n",
    "run_command(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [de] dictionary: 24896 types\n",
      "| [en] dictionary: 15464 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fairseq.tasks.translation.TranslationTask at 0x13be549b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    task = 'translation'\n",
    "    data = 'data/multi30k/data'\n",
    "    \n",
    "    left_pad_source = True\n",
    "    left_pad_target = False\n",
    "    \n",
    "    source_lang = None\n",
    "    target_lang = None\n",
    "    \n",
    "    upsample_primary = 1\n",
    "    dataset_impl = 'raw'\n",
    "    max_source_positions = 1024\n",
    "    max_target_positions = 1024\n",
    "\n",
    "    gen_subset = 'test'\n",
    "    \n",
    "\n",
    "\n",
    "args = Args()\n",
    "task = tasks.setup_task(args)\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fairseq.data import indexed_dataset\n",
    "\n",
    "\n",
    "def split_exists(split, src, tgt, lang, data_path, dataset_impl):\n",
    "    filename = os.path.join(data_path, '{}.{}-{}.{}'.format(split, src, tgt, lang))\n",
    "    return indexed_dataset.dataset_exists(filename, impl=dataset_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/multi30k/data/test.de-en.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = args.data\n",
    "src = args.source_lang\n",
    "tgt = args.target_lang\n",
    "split = args.gen_subset\n",
    "dataset_impl = args.dataset_impl\n",
    "\n",
    "if split_exists(split, src, tgt, src, data_path, dataset_impl):\n",
    "    prefix = os.path.join(data_path, '{}.{}-{}.'.format(split, src, tgt))\n",
    "elif split_exists(split, tgt, src, src, data_path, dataset_impl):\n",
    "    prefix = os.path.join(data_path, '{}.{}-{}.'.format(split, tgt, src))\n",
    "else:\n",
    "    raise ValueError('Dataset not found: {} ({})'.format(split, data_path))\n",
    "\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.indexed_dataset.IndexedRawTextDataset at 0x13bddf208>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset = indexed_dataset.make_dataset(prefix + src, impl=dataset_impl,\n",
    "                                           fix_lua_indexing=True, dictionary=src_dict)\n",
    "tgt_dataset = indexed_dataset.make_dataset(prefix + tgt, impl=dataset_impl,\n",
    "                                           fix_lua_indexing=True, dictionary=tgt_dict)\n",
    "\n",
    "tgt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.datasets[split] = load_langpair_dataset(\n",
    "#     data_path, split, src, self.src_dict, tgt, self.tgt_dict,\n",
    "#     combine=combine, dataset_impl=self.args.dataset_impl,\n",
    "#     upsample_primary=self.args.upsample_primary,\n",
    "#     left_pad_source=self.args.left_pad_source,\n",
    "#     left_pad_target=self.args.left_pad_target,\n",
    "#     max_source_positions=self.args.max_source_positions,\n",
    "#     max_target_positions=self.args.max_target_positions,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairseq Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairseq Generate Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from fairseq import utils, tasks, checkpoint_utils\n",
    "from fairseq.meters import StopwatchMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [noisy] dictionary: 212296 types\n",
      "| [original] dictionary: 121200 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fairseq.tasks.translation.TranslationTask at 0x13cc7fb70>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    task = 'translation'\n",
    "    left_pad_source = True\n",
    "    left_pad_target = False\n",
    "    \n",
    "    source_lang = None\n",
    "    target_lang = None\n",
    "    cpu = False\n",
    "    replace_unk = None\n",
    "    \n",
    "    no_beamable_mm = False # ??\n",
    "    skip_invalid_size_inputs_valid_test = False # ??\n",
    "    num_shards = 1\n",
    "    num_workers = 1\n",
    "    shard_id = 0\n",
    "\n",
    "    upsample_primary = 1\n",
    "    max_source_positions = 1024\n",
    "    max_target_positions = 1024\n",
    "    max_tokens = 12000\n",
    "    max_sentences = 10\n",
    "    required_batch_size_multiple = 8\n",
    "    prefix_size = 0\n",
    "\n",
    "    # input parameters\n",
    "    data = '/Users/mingyuliu/Desktop/20191021_224310/data'\n",
    "    gen_subset = 'new'\n",
    "    dataset_impl = 'raw'\n",
    "    path = '/Users/mingyuliu/Desktop/20191021_224310/models/checkpoint_best.pt'\n",
    "    beam = 1\n",
    "    print_alignment = False\n",
    "    fp16 = False\n",
    "    batch_size = 2 # ??\n",
    "    remove_bpe = None\n",
    "    quiet = False\n",
    "    nbest = 1\n",
    "\n",
    "\n",
    "args = Args()\n",
    "task = tasks.setup_task(args)\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| /Users/mingyuliu/Desktop/20191021_224310/data new noisy-original 200 examples\n"
     ]
    }
   ],
   "source": [
    "task.load_dataset(args.gen_subset)\n",
    "\n",
    "# Set dictionaries\n",
    "try:\n",
    "    src_dict = getattr(task, 'source_dictionary', None)\n",
    "except NotImplementedError:\n",
    "    src_dict = None\n",
    "\n",
    "tgt_dict = task.target_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loading model(s) from /Users/mingyuliu/Desktop/20191021_224310/models/checkpoint_best.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FConvModel(\n",
       "   (encoder): FConvEncoder(\n",
       "     (embed_tokens): Embedding(212296, 256, padding_idx=1)\n",
       "     (embed_positions): LearnedPositionalEmbedding(1024, 256, padding_idx=1)\n",
       "     (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (projections): ModuleList(\n",
       "       (0): None\n",
       "       (1): None\n",
       "       (2): None\n",
       "       (3): None\n",
       "     )\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
       "       (1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
       "       (2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
       "       (3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
       "     )\n",
       "     (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "   )\n",
       "   (decoder): FConvDecoder(\n",
       "     (embed_tokens): Embedding(121200, 256, padding_idx=1)\n",
       "     (embed_positions): LearnedPositionalEmbedding(1024, 256, padding_idx=1)\n",
       "     (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (projections): ModuleList(\n",
       "       (0): None\n",
       "       (1): None\n",
       "       (2): None\n",
       "     )\n",
       "     (convolutions): ModuleList(\n",
       "       (0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
       "       (1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
       "       (2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
       "     )\n",
       "     (attention): ModuleList(\n",
       "       (0): AttentionLayer(\n",
       "         (in_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "         (out_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "       )\n",
       "       (1): AttentionLayer(\n",
       "         (in_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "         (out_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "       )\n",
       "       (2): AttentionLayer(\n",
       "         (in_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "         (out_projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "     (fc3): Linear(in_features=256, out_features=121200, bias=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ensemble\n",
    "print('| loading model(s) from {}'.format(args.path))\n",
    "models, _model_args = checkpoint_utils.load_model_ensemble(\n",
    "    args.path.split(':'),\n",
    "    task=task\n",
    ")\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Optimize ensemble for generation\n",
    "use_cuda = torch.cuda.is_available() and not args.cpu\n",
    "\n",
    "for model in models:\n",
    "    model.make_generation_fast_(\n",
    "        beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n",
    "        need_attn=args.print_alignment,\n",
    "    )\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    if use_cuda:\n",
    "        model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment dictionary for unknown word replacement\n",
    "# (None if no unknown word replacement, empty if no path to align dictionary)\n",
    "align_dict = utils.load_align_dict(args.replace_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.iterators.CountingIterator at 0x1416dc668>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (possibly sharded)\n",
    "itr = task.get_batch_iterator(\n",
    "    dataset=task.dataset(args.gen_subset),\n",
    "    max_tokens=args.max_tokens,\n",
    "    max_sentences=args.max_sentences,\n",
    "    max_positions=utils.resolve_max_positions(\n",
    "        task.max_positions(),\n",
    "        *[model.max_positions() for model in models]\n",
    "    ),\n",
    "    ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n",
    "    required_batch_size_multiple=args.required_batch_size_multiple,\n",
    "    num_shards=args.num_shards,\n",
    "    shard_id=args.shard_id,\n",
    "    num_workers=args.num_workers,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([108, 116, 117, 120, 154, 112,  74, 147]),\n",
       " 'nsentences': 8,\n",
       " 'ntokens': 16,\n",
       " 'net_input': {'src_tokens': tensor([[     3,      2],\n",
       "          [     3,      2],\n",
       "          [  5845,      2],\n",
       "          [107341,      2],\n",
       "          [     3,      2],\n",
       "          [     3,      2],\n",
       "          [     3,      2],\n",
       "          [ 53762,      2]]),\n",
       "  'src_lengths': tensor([2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "  'prev_output_tokens': tensor([[     2,   2519],\n",
       "          [     2, 113263],\n",
       "          [     2,   5255],\n",
       "          [     2,   1986],\n",
       "          [     2,   7599],\n",
       "          [     2,      3],\n",
       "          [     2,  37025],\n",
       "          [     2,  38250]])},\n",
       " 'target': tensor([[  2519,      2],\n",
       "         [113263,      2],\n",
       "         [  5255,      2],\n",
       "         [  1986,      2],\n",
       "         [  7599,      2],\n",
       "         [     3,      2],\n",
       "         [ 37025,      2],\n",
       "         [ 38250,      2]])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(itr))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['net_input']['src_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "gen_timer = StopwatchMeter()\n",
    "generator = task.build_generator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
    "# if 'net_input' not in sample:\n",
    "#     continue\n",
    "\n",
    "prefix_tokens = None\n",
    "if args.prefix_size > 0:\n",
    "    prefix_tokens = sample['target'][:, :args.prefix_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'tokens': tensor([3, 2]),\n",
       "   'score': -0.15852700173854828,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.3162, -0.0009])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.15852700173854828,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.3162, -0.0009])}],\n",
       " [{'tokens': tensor([5255,    2]),\n",
       "   'score': -0.26697462797164917,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.5308, -0.0031])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.29383397102355957,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.5864, -0.0013])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.15852700173854828,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.3162, -0.0009])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.15852700173854828,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.3162, -0.0009])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.15852700173854828,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-0.3162, -0.0009])}],\n",
       " [{'tokens': tensor([3, 2]),\n",
       "   'score': -0.5243253707885742,\n",
       "   'attention': None,\n",
       "   'alignment': None,\n",
       "   'positional_scores': tensor([-1.0468, -0.0018])}]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypos = task.inference_step(generator, models, sample, prefix_tokens)\n",
    "num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)\n",
    "hypos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sample_id = sample['id'][i]\n",
    "\n",
    "\n",
    "has_target = sample['target'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2])\n",
      "tensor([2519,    2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "src_tokens = utils.strip_pad(sample['net_input']['src_tokens'][i, :], tgt_dict.pad())\n",
    "target_tokens = None\n",
    "if has_target:\n",
    "    target_tokens = utils.strip_pad(sample['target'][i, :], tgt_dict.pad()).int().cpu()\n",
    "    \n",
    "print(src_tokens)\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if align_dict is not None:\n",
    "    src_str = task.dataset(args.gen_subset).src.get_original_text(sample_id)\n",
    "    target_str = task.dataset(args.gen_subset).tgt.get_original_text(sample_id)\n",
    "else:\n",
    "    if src_dict is not None:\n",
    "        src_str = src_dict.string(src_tokens, args.remove_bpe)\n",
    "    else:\n",
    "        src_str = \"\"\n",
    "    if has_target:\n",
    "        target_str = tgt_dict.string(target_tokens, args.remove_bpe, escape_unk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-108\t<unk>\n",
      "T-108\tslippers\n"
     ]
    }
   ],
   "source": [
    "if not args.quiet:\n",
    "    if src_dict is not None:\n",
    "        print('S-{}\\t{}'.format(sample_id, src_str))\n",
    "    if has_target:\n",
    "        print('T-{}\\t{}'.format(sample_id, target_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H-108\t-0.15852700173854828\t<unk>\n",
      "P-108\t-0.3162 -0.0009\n"
     ]
    }
   ],
   "source": [
    "for j, hypo in enumerate(hypos[i][:min(len(hypos), args.nbest)]):\n",
    "    hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
    "        hypo_tokens=hypo['tokens'].int().cpu(),\n",
    "        src_str=src_str,\n",
    "        alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,\n",
    "        align_dict=align_dict,\n",
    "        tgt_dict=tgt_dict,\n",
    "        remove_bpe=args.remove_bpe,\n",
    "    )\n",
    "\n",
    "    if not args.quiet:\n",
    "        print('H-{}\\t{}\\t{}'.format(sample_id, hypo['score'], hypo_str))\n",
    "        print('P-{}\\t{}'.format(\n",
    "            sample_id,\n",
    "            ' '.join(map(\n",
    "                lambda x: '{:.4f}'.format(x),\n",
    "                hypo['positional_scores'].tolist(),\n",
    "            ))\n",
    "        ))\n",
    "\n",
    "        if args.print_alignment:\n",
    "            print('A-{}\\t{}'.format(\n",
    "                sample_id,\n",
    "                ' '.join(map(lambda x: str(utils.item(x)), alignment))\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypo_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-108\t<unk>\n",
      "T-108\tslippers\n",
      "H-108\t-0.15852700173854828\t<unk>\n",
      "P-108\t-0.3162 -0.0009\n",
      "S-116\t<unk>\n",
      "T-116\tgipiemme\n",
      "H-116\t-0.15852700173854828\t<unk>\n",
      "P-116\t-0.3162 -0.0009\n",
      "S-117\tprep\n",
      "T-117\tprep\n",
      "H-117\t-0.26697462797164917\tprep\n",
      "P-117\t-0.5308 -0.0031\n",
      "S-120\tthemometer\n",
      "T-120\tthermometer\n",
      "H-120\t-0.29383397102355957\t<unk>\n",
      "P-120\t-0.5864 -0.0013\n",
      "S-154\t<unk>\n",
      "T-154\tsurfboard\n",
      "H-154\t-0.15852700173854828\t<unk>\n",
      "P-154\t-0.3162 -0.0009\n",
      "S-112\t<unk>\n",
      "T-112\t<<unk>>\n",
      "H-112\t-0.15852700173854828\t<unk>\n",
      "P-112\t-0.3162 -0.0009\n",
      "S-74\t<unk>\n",
      "T-74\tritchey\n",
      "H-74\t-0.15852700173854828\t<unk>\n",
      "P-74\t-0.3162 -0.0009\n",
      "S-147\tbontrager\n",
      "T-147\tbontrager\n",
      "H-147\t-0.5243253707885742\t<unk>\n",
      "P-147\t-1.0468 -0.0018\n"
     ]
    }
   ],
   "source": [
    "for i, sample_id in enumerate(sample['id'].tolist()):\n",
    "    has_target = sample['target'] is not None\n",
    "\n",
    "    # Remove padding\n",
    "    src_tokens = utils.strip_pad(sample['net_input']['src_tokens'][i, :], tgt_dict.pad())\n",
    "    target_tokens = None\n",
    "    if has_target:\n",
    "        target_tokens = utils.strip_pad(sample['target'][i, :], tgt_dict.pad()).int().cpu()\n",
    "\n",
    "    # Either retrieve the original sentences or regenerate them from tokens.\n",
    "    if align_dict is not None:\n",
    "        src_str = task.dataset(args.gen_subset).src.get_original_text(sample_id)\n",
    "        target_str = task.dataset(args.gen_subset).tgt.get_original_text(sample_id)\n",
    "    else:\n",
    "        if src_dict is not None:\n",
    "            src_str = src_dict.string(src_tokens, args.remove_bpe)\n",
    "        else:\n",
    "            src_str = \"\"\n",
    "        if has_target:\n",
    "            target_str = tgt_dict.string(target_tokens, args.remove_bpe, escape_unk=True)\n",
    "\n",
    "    if not args.quiet:\n",
    "        if src_dict is not None:\n",
    "            print('S-{}\\t{}'.format(sample_id, src_str))\n",
    "        if has_target:\n",
    "            print('T-{}\\t{}'.format(sample_id, target_str))\n",
    "\n",
    "    # Process top predictions\n",
    "    for j, hypo in enumerate(hypos[i][:min(len(hypos), args.nbest)]):\n",
    "        hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
    "            hypo_tokens=hypo['tokens'].int().cpu(),\n",
    "            src_str=src_str,\n",
    "            alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,\n",
    "            align_dict=align_dict,\n",
    "            tgt_dict=tgt_dict,\n",
    "            remove_bpe=args.remove_bpe,\n",
    "        )\n",
    "\n",
    "        if not args.quiet:\n",
    "            print('H-{}\\t{}\\t{}'.format(sample_id, hypo['score'], hypo_str))\n",
    "            print('P-{}\\t{}'.format(\n",
    "                sample_id,\n",
    "                ' '.join(map(\n",
    "                    lambda x: '{:.4f}'.format(x),\n",
    "                    hypo['positional_scores'].tolist(),\n",
    "                ))\n",
    "            ))\n",
    "\n",
    "            if args.print_alignment:\n",
    "                print('A-{}\\t{}'.format(\n",
    "                    sample_id,\n",
    "                    ' '.join(map(lambda x: str(utils.item(x)), alignment))\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dict.unk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dict[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dict.index('<unk>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
