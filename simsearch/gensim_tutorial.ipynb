{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/RaRe-Technologies/gensim/blob/d7f4c2c46aa7a16d2493d25f5830cdf267e573f4/docs/notebooks/Corpora_and_Vector_Spaces.ipynb\n",
    "- https://github.com/RaRe-Technologies/gensim/blob/d7f4c2c46aa7a16d2493d25f5830cdf267e573f4/docs/notebooks/Topics_and_Transformations.ipynb\n",
    "- https://github.com/RaRe-Technologies/gensim/blob/d7f4c2c46aa7a16d2493d25f5830cdf267e573f4/docs/notebooks/Similarity_Queries.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove common words and tokenize# remov \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = 'tmp'\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(checkpoint_path, 'deerwester.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)\n",
    "\n",
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(checkpoint_path, 'deerwester.mm'), corpus)  # store to disk, for later use\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n",
      "computer\n",
      "human\n",
      "interface\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(checkpoint_path, 'deerwester.dict')):\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(checkpoint_path, 'deerwester.dict'))\n",
    "    corpus = corpora.MmCorpus(os.path.join(checkpoint_path, 'deerwester.mm'))\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")\n",
    "    \n",
    "print(dictionary[0])\n",
    "print(dictionary[1])\n",
    "print(dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x10cce74a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "corpus_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.07910475117444804), (1, -0.5732835243079396)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x1104570f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "# index.save(os.path.join(TEMP_FOLDER, 'deerwester.index'))\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999408), (1, 0.9946708), (2, 0.9999427), (3, 0.99987906), (4, 0.99935204), (5, -0.08804217), (6, -0.051574208), (7, -0.023664715), (8, 0.1938726)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9999427), (0, 0.9999408), (3, 0.99987906), (4, 0.99935204), (1, 0.9946708), (8, 0.1938726), (7, -0.023664715), (6, -0.051574208), (5, -0.08804217)]\n"
     ]
    }
   ],
   "source": [
    "# The EPS user interface management system\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec1_tfidf = ksearch.getTfidfForDoc(73)\n",
    "# vec2_tfidf = ksearch.getTfidfForDoc(results[0][0])    \n",
    "\n",
    "# Interpret the top match.\n",
    "# search.interpretMatch(vec1_tfidf, vec2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0899264 , -0.72418606])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sparse2dense(sparse_vec, length):\n",
    "    vec = np.zeros(length)\n",
    "    for i in range(0, len(sparse_vec)):\n",
    "        j = sparse_vec[i][0]\n",
    "        value = sparse_vec[i][1]\n",
    "        vec[j] = value\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "vec1_tfidf = corpus_tfidf[2]\n",
    "sparse2dense(lsi[vec1_tfidf], lsi.num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.5710059809418182),\n",
       " (5, 0.4170757362022777),\n",
       " (7, 0.4170757362022777),\n",
       " (8, 0.5710059809418182)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1_tfidf = corpus_tfidf[2]\n",
    "vec2_tfidf = tfidf[vec_bow]\n",
    "print(vec2_tfidf)\n",
    "vec1_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0899264  -0.72418606]\n",
      "[ 0.05593551 -0.40537267]\n",
      "[0.         0.         0.57100598 0.         0.         0.41707574\n",
      " 0.         0.41707574 0.57100598 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# len(dictionary.keys())\n",
    "vocab_size = len(dictionary.token2id)\n",
    "vec1_lsi = sparse2dense(lsi[vec1_tfidf], lsi.num_topics)\n",
    "vec2_lsi = sparse2dense(lsi[vec2_tfidf], lsi.num_topics)\n",
    "vec1_tfidf = sparse2dense(vec1_tfidf, vocab_size)\n",
    "print(vec1_lsi)\n",
    "print(vec2_lsi)\n",
    "print(vec1_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.25762474,  0.        ,  0.        ,\n",
       "        0.26537098,  0.        ,  0.21555183,  0.2613603 , -0.        ,\n",
       "       -0.        ,  0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.linalg.norm(vec1_lsi) * np.linalg.norm(vec2_lsi)    \n",
    "                \n",
    "# Create a vector to hold the similarity contribution of each word.\n",
    "word_sims = np.zeros(vocab_size)\n",
    "\n",
    "# For each word in the vocabulary...\n",
    "for word_id in range(vocab_size):\n",
    "\n",
    "    # Get the weights vector for this word. This vector has one weight\n",
    "    # for each topic\n",
    "    word_weights = np.asarray(lsi.projection.u[word_id, :]).flatten()\n",
    "\n",
    "    # Calculate the contribution of this word in doc1 to the total similarity.\n",
    "    word_sims[word_id] = vec1_tfidf[word_id] * np.dot(word_weights, vec2_lsi) / norms\n",
    "\n",
    "word_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.projection.u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.25762474,  0.        ,  0.        ,\n",
       "        0.26537098,  0.        ,  0.21555183,  0.2613603 , -0.        ,\n",
       "       -0.        ,  0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = np.linalg.norm(vec1_lsi) * np.linalg.norm(vec2_lsi) \n",
    "word_sims = vec1_tfidf * np.dot(lsi.projection.u, vec2_lsi) / norms\n",
    "word_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.2653709808317419),\n",
       " (8, 0.26136029647405956),\n",
       " (2, 0.25762473899614763),\n",
       " (7, 0.21555182985077137),\n",
       " (0, 0.0),\n",
       " (1, 0.0),\n",
       " (3, 0.0),\n",
       " (4, 0.0),\n",
       " (6, 0.0),\n",
       " (9, -0.0),\n",
       " (10, -0.0),\n",
       " (11, 0.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sims = sorted(enumerate(word_sims), key=lambda item: -item[1])\n",
    "word_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printWordSims(word_sims, dictionary, topn=10, min_pos=0.1, max_neg=0.01):\n",
    "    \"\"\"\n",
    "    Internal function used by `interpretMatch` to display the contributing\n",
    "    words.\n",
    "    \"\"\"\n",
    "    # TODO - First create the list of results in interpretMatch, then\n",
    "    #        in this function just do the printing, and adapt the column\n",
    "    #        width to the maximum word length in the results...\n",
    "\n",
    "    # Build up the table of results to display.        \n",
    "    tableStr = ''\n",
    "    for i in range(0, topn):\n",
    "        pos_word_id, pos_word_val = word_sims[i]\n",
    "        neg_word_id, neg_word_val = word_sims[-(i + 1)]\n",
    "\n",
    "        pos_word = dictionary[pos_word_id]\n",
    "        neg_word = dictionary[neg_word_id]                       \n",
    "\n",
    "        # If neither words pass the thresholds, break.\n",
    "        if ((pos_word_val <= min_pos) and (neg_word_val >= max_neg)):\n",
    "            break\n",
    "\n",
    "        # Only display the positive word if the value passes the threshold.\n",
    "        if (pos_word_val > min_pos):\n",
    "            tableStr += '  %15s  +%.3f' % (pos_word, pos_word_val)\n",
    "        # Otherwise add empty space.\n",
    "        else:\n",
    "            # e.g.,     '          freedom  +0.440'\n",
    "            tableStr += '                         '\n",
    "\n",
    "        # Only display the negative word if the value passes the threshold.\n",
    "        if (neg_word_val < max_neg):\n",
    "            tableStr += '    %15s  %.3f\\n' % (neg_word, neg_word_val)\n",
    "        # Otherwise just end the line.\n",
    "        else:\n",
    "            tableStr += '\\n'\n",
    "\n",
    "    print(tableStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           system  +0.265             minors  0.000\n",
      "              eps  +0.261              graph  -0.000\n",
      "        interface  +0.258              trees  -0.000\n",
      "             user  +0.216               time  0.000\n",
      "                                      survey  0.000\n",
      "                                    response  0.000\n",
      "                                       human  0.000\n",
      "                                    computer  0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printWordSims(word_sims, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
