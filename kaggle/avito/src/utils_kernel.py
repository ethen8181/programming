import os
import numpy as np
import pandas as pd
from string import punctuation
from scipy.stats import randint
from nltk.corpus import stopwords
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer


def read_and_clean_data(input_dir, ids_col, label_col, cat_cols):
    activation_col = 'activation_date'
    read_csv_params = {
        'index_col': ids_col,
        'parse_dates': [activation_col]
    }
    train_path = os.path.join(input_dir, 'train.csv')
    train = pd.read_csv(train_path, **read_csv_params)
    label = train[label_col].copy()
    train = train.drop(label_col, axis=1)
    train_index = train.index
    test_path = os.path.join(input_dir, 'test.csv')
    test = pd.read_csv(test_path, **read_csv_params)
    test_index = test.index
    df = pd.concat([train, test], axis=0)
    del train, test

    # extract time-based features
    week_day_col = 'week_day'
    week_of_year_col = 'week_of_year'
    day_of_month_col = 'day_of_month'
    df[week_day_col] = df[activation_col].dt.weekday
    df[week_of_year_col] = df[activation_col].dt.week
    df[day_of_month_col] = df[activation_col].dt.day
    # TODO we aren't really using image column, we don't even have to read it in
    df = df.drop([activation_col, 'image'], axis=1)

    # TODO the original kernel filled the price with the mean
    # treat unknown price as a distinct category as oppose of imputing it with median
    # and un-skew its distribution
    na_flag = -999
    price_col = 'price'
    df[price_col] = np.log(df[price_col] + 0.001)
    df[price_col] = df[price_col].fillna(na_flag)

    # merge user aggregated stats generated by generate_user_aggregated_features.py
    user_agg_features_path = os.path.join(input_dir, 'aggregated_user_features.parquet')
    if os.path.isfile(user_agg_features_path):
        df_user_agg_features = pd.read_parquet(user_agg_features_path)
        agg_cols = ['days_up_sum_mean', 'days_up_sum_std',
                    'times_put_up_mean', 'times_put_up_std', 'user_item_count']
        # when merging on columns, pandas will drop the index, in this case this would mean losing
        # our item id, thus we convert the item id into a column and convert it back to index
        # once the merging is done
        df = df.reset_index().merge(df_user_agg_features, on='user_id', how='left').set_index(ids_col)
        df[agg_cols] = df[agg_cols].fillna(na_flag)

    # TODO replace with reading it in as category type
    label_encoder = LabelEncoder()
    for col in cat_cols:
        df[col].fillna('Unknown')
        df[col] = label_encoder.fit_transform(df[col].astype(str))

    # text feature engineering
    description_col = 'description'
    df[description_col + '_num_punc'] = df[description_col].apply(
        lambda x: len([c for c in str(x) if c in punctuation]))

    text_features = [description_col, 'title']
    for col in text_features:
        df[col] = df[col].astype(str).fillna('missing')
        # Lowercase all text, so that capitalized words don't get treated differently
        df[col] = df[col].str.lower()

        # TODO not sure if there's a way to easily speed up this calculation, as using apply is a bit slow
        df[col + '_num_words'] = df[col].apply(lambda text: len(text.split()))
        df[col + '_num_unique_words'] = df[col].apply(lambda text: len(set(w for w in text.split())))

        # add epsilon to denominator to prevent division error
        df[col + '_words_vs_unique'] = df[col + '_num_unique_words'] / (df[col + '_num_words'] + 1e-4)

    return df, train_index, test_index, label, text_features


def create_vectorizer_pipeline(text_features):
    stop_words = set(stopwords.words('russian'))
    tfidf_params = {
        'norm': 'l2',
        'analyzer': 'word',
        'smooth_idf': False,
        'sublinear_tf': True,
        'ngram_range': (1, 2),
        'token_pattern': r'\w{1,}',
        'stop_words': stop_words
    }

    pipelines = []
    for feature_name in text_features:
        # the cap on the number of features for the description column,
        # doesn't seem to have any effect on the performance
        if feature_name == 'description':
            vectorizer = TfidfVectorizer(max_features=17000, **tfidf_params)
        else:
            vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words)

        pipeline = Pipeline([
            ('extractor', ColumnExtractor(col=feature_name)),
            ('vectorizer', vectorizer)
        ])
        pipelines.append((feature_name, pipeline))

    return FeatureUnion(pipelines)


def get_vectorizer_feature_names(vectorizer_pipeline):
    vectorizer_feature_names = []
    for _, pipeline in vectorizer_pipeline.transformer_list:
        vectorizer = pipeline.named_steps['vectorizer']
        vectorizer_feature_names.extend(vectorizer.get_feature_names())

    return vectorizer_feature_names


class ColumnExtractor(BaseEstimator, TransformerMixin):
    """
    Extracts a single column for a given DataFrame, this
    is mainly used for integrating with scikit-learn pipeline [1]_.

    Parameters
    ----------
    col : str
        A single column name in the given DataFrame.

    References
    ----------
    .. [1] `Custom feature selection in sklearn pipeline
            <https://stackoverflow.com/questions/25250654/how-can-i-use-a-custom-feature-selection-function-in-scikit-learns-pipeline>`_
    """

    def __init__(self, col):
        self.col = col

    def fit(self, data, y=None):
        """
        Performs no operations at fitting time.

        Parameters
        ----------
        data : DataFrame, shape [n_samples, n_features]
            Input data.

        y : default None
            Ignore, argument required for constructing sklearn Pipeline.

        Returns
        -------
        self
        """
        return self

    def transform(self, data):
        """
        Extract the specified single column.

        Parameters
        ----------
        data : DataFrame, shape [n_samples, n_features]
            Input data.

        Returns
        -------
        column : pd.Series
            Extracted column.
        """
        column = data[self.col]
        return column


class ModelPipeline:
    """
    Build a RandomSearchCV regression model that optimizes for rmse.
    """

    def __init__(self, n_iter=2, cv=10, random_state=1234):
        self.cv = cv
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y, feature_name, categorical_feature, validation_split=0.2):

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=validation_split, random_state=self.random_state)

        # for gbm, set number of estimator to a large number
        # and the learning rate to be a small number, we'll simply
        # let early stopping decide when to stop training
        # https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html
        lgb_fixed_params = {
            'n_jobs': -1,
            'silent': False,
            'metric': 'rmse',
            'objective': 'rmse',  # equivalent to regression
            'learning_rate': 0.018,
            'num_leaves': 270,
            'n_estimators': 3000,  # 1380,
            'colsample_bytree': 0.5,  # equivalent to feature_fraction
            'subsample': 0.75  # equivalent to bagging_fraction
        }
        lgb = LGBMRegressor(**lgb_fixed_params)

        eval_set = [(X_train, y_train), (X_val, y_val)]
        lgb_fit_params = {
            'verbose': 100,
            'eval_set': eval_set,
            'early_stopping_rounds': 5,
            'feature_name': feature_name,
            'categorical_feature': categorical_feature
        }

        # set up random search hyperparameters:
        # subsample, colsample_bytree and max_depth are presumably the most
        # common way to control under/overfitting for tree-based models
        lgb_tuned_params = {
            'num_leaves': randint(low=150, high=300),
            'colsample_bytree': [0.5, 0.6, 0.7, 0.8],
            'subsample': [0.7, 0.75, 0.8, 0.85]
        }

        # return_train_score = False
        # computing the scores on the training set can be computationally
        # expensive and is not strictly required to select the parameters
        # that yield the best generalization performance.
        model_tuned = RandomizedSearchCV(
            estimator=lgb,
            param_distributions=lgb_tuned_params,
            cv=self.cv,
            n_iter=self.n_iter,
            n_jobs=-1,
            verbose=1,
            refit=False,
            scoring='neg_mean_squared_error',
            random_state=self.random_state,
            return_train_score=False
        ).fit(X_train, y_train, **lgb_fit_params)
        del X_train, y_train, X_val, y_val

        lgb = model_tuned.best_estimator_
        lgb_fixed_params.update(lgb.best_params_)
        lgb_fixed_params['n_estimators'] = lgb.best_iteration_
        del lgb_fit_params['eval_set'], lgb_fit_params['early_stopping_rounds']
        lgb = LGBMRegressor(**lgb_fixed_params)
        lgb.fit(X, y, **lgb_fit_params)

        # eval_set = [(X_train, y_train), (X_val, y_val)]
        # lgb_fit_params = {
        #     'verbose': 100,
        #     'eval_set': eval_set,
        #     'early_stopping_rounds': 5,
        #     'feature_name': feature_name,
        #     'categorical_feature': categorical_feature
        # }
        # lgb.fit(X_train, y_train, **lgb_fit_params)
        # del X_train, y_train, X_val, y_val
        #
        # lgb_fixed_params['n_estimators'] = lgb.best_iteration_
        # del lgb_fit_params['eval_set'], lgb_fit_params['early_stopping_rounds']
        # lgb = LGBMRegressor(**lgb_fixed_params)
        # lgb.fit(X, y, **lgb_fit_params)
        self.lgb_ = lgb
        return self

    def get_feature_importance(self, threshold=1e-3):
        """
        Sort the feature importance based on decreasing order of the
        normalized gain. Features that has a normalized gain smaller
        than the specified ``threshold`` will not be returned.
        """
        # booster = self.model_tuned.best_estimator_.booster_
        booster = self.lgb_.booster_
        importance = booster.feature_importance(importance_type='gain')
        importance /= importance.sum()
        feature_name = np.array(booster.feature_name())

        mask = importance > threshold
        importance = importance[mask]
        feature_name = feature_name[mask]
        idx = np.argsort(importance)[::-1]
        return list(zip(feature_name[idx], np.round(importance[idx], 4)))

    def predict(self, X):
        # best = self.model_tuned.best_estimator_
        # return best.predict(data, num_iteration=best.best_iteration_)
        best = self.lgb_.booster_
        return best.predict(X)

    def score(self, X, y):
        prediction = self.predict(X)
        rmse = np.sqrt(mean_squared_error(y, prediction))
        return rmse


def create_submission(prediction, ids, ids_col, label_col, current_model_dir):
    submission = pd.DataFrame({
        ids_col: ids,
        label_col: prediction
    }, columns=[ids_col, label_col])
    submission[label_col] = submission[label_col].clip(0.0, 1.0)
    submission_path = os.path.join(current_model_dir, 'submission.csv')
    submission.to_csv(submission_path, index=False, header=True)
