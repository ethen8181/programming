{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "title: Test Driving Elasticsearch Learning to Rank with a Linear Model\n",
    "author: doug-turnbull\n",
    "excerpt: \"This blog post let's the simple model we derived in a previous post and make it usable via the Elasticsearch Learning to Rank plugin.\"\n",
    "categories:\n",
    "  - Relevancy\n",
    "  - Learning-to-rank\n",
    "  - Machine-Learning\n",
    "image: /images/relevant-search/relevant-search-cover-sm.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Last time](http://opensourceconnections.com/blog/2017/04/01/learning-to-rank-linear-models/), I created a simple linear model using three ranking signals. Using [TMDB movie data](http://themoviedb.org), I came up with a naive model that computed a relevance score from a title field's TF\\*IDF score, an overview field's TF\\*IDF score, and the user rating of the movie. Our model learned the weight to apply to each score when summing -- similar to the boosts you apply when doing manual relevance tuning.\n",
    "\n",
    "What I didn't tell you was I was using Elasticsearch to compute those ranking signals. This blog post I want to take the simple model we derived and make it usable via the [Elasticsearch Learning to Rank plugin](https://github.com/o19s/elasticsearch-learning-to-rank). In a future blog post, we'll load the same model into Solr. This will give you a chance to see a very simple 101 model in action with the two search engine's learning to rank plugins.\n",
    "\n",
    "## The ranking signals...\n",
    "\n",
    "If you recall, learning to rank learns a ranking function as a function of ranking-time signals. Classically these are referred to as \"features\" when discussing machine learning models. But I like to use *signals* to denote that they're signaling us something about the *relationship* between a query and document. Plus, selfishly, it's what we call ranking-time information in [Relevant Search](http://manning.com/books/relevant-search) to differentiate between the features that exist purely on content or derived from queries.\n",
    "\n",
    "In this blog post, we'll use the Python Elasticsearch client library, but mostly I'll just be showing off the basic queries I use to derive the signals. I've already loaded the TMDB movie data locally, if you'd like to have this data at your disposal follow the directions in the [Learning to Rank demo README](https://github.com/o19s/elasticsearch-learning-to-rank/tree/master/demo#download-tmdb-data)\n",
    "\n",
    "Onto the action. Below, you'll see our three queries we use to generate the signal values: `titleSearch`, `overviewSearch`, and `ratingSearch`. The first two are straight-forward [match queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html). The latter is a [function score query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html) that just returns a movie's rating which has no relationship to the search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "keywords=\"rambo\"\n",
    "\n",
    "titleSearch = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"title\": keywords\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "overviewSearch = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"overview\": keywords\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "ratingSearch = {\n",
    "    \"query\": {\n",
    "        \"function_score\": {\n",
    "        \n",
    "            \"functions\": [\n",
    "                {\"field_value_factor\": {\n",
    "                    \"field\": \"vote_average\",\n",
    "                    \"missing\": -1   \n",
    "                }}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es = Elasticsearch()\n",
    "es.search(index='tmdb', doc_type='movie', body=titleSearch)\n",
    "es.search(index='tmdb', doc_type='movie', body=overviewSearch)\n",
    "es.search(index='tmdb', doc_type='movie', body=ratingSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you recall, these three features were gathered for a set of judgments. Judgments let us know how relevant a document is for a query. So Rambo is a \"4\" (exact match) for the keyword search \"rambo.\" Conversely \"Rocky and Bullwininkle\" is a 0 (not at all relevant) for a \"Rambo\" query. With enough judgments, we logged the relevance scores of the above queries for the documents that were judged. This gave us a training set that looked like:\n",
    "\n",
    "```\n",
    "titleScore,overviewScore,movieRating,comment\n",
    "4,12.28,9.82,6.40,# 7555\trambo@Rambo\n",
    "0,0.00,10.76,7.10,# 1368\trambo@First Blood\n",
    "```\n",
    "\n",
    "In that blog post, we used `sk-learn` to run linear regression to learn which signals best predicted the resulting relevance grade. We came up with a model with a weight for each and a y-intercept. This model was:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "coefs = [ 0.04999419,  0.22958357,  0.00573909] # each signals weight\n",
    "yIntercept = 0.97040804634516986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Uploading our Linear model to Elasticsearch\n",
    "\n",
    "The Elasticsearch learning to rank plugin uses a scripting format known as `ranklib` to encode models. Following the [documentation for the ranklib scripting language](https://docs.google.com/document/d/1DL_Z40eGG3r_BVOoVYpBRb3k2qWONRf_w02FfORtiSU/edit#) we know we can encode a linear model that looks like:\n",
    "\n",
    "```\n",
    "## Linear Regression\n",
    "0:<y-intercept> 1:<coef1> 2:<coef2> 3:<coef3> ...\n",
    "```\n",
    "\n",
    "So in Python code, we can format our model above in that format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "linearModel = \"\"\" ## Linear Regression\n",
    "0:0.97040804634516986 1:0.04999419 2:0.229585357 3:0.00573909\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Following the [documentation for the Learning to Rank Plugin]() we can upload this model as a ranklib script, and give it a name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.put_script(lang='ranklib', id='our_silly_model', body={'script': linearModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Elasticsearch has acknowledged our upload. Great! Now we should be able to execute a simple query!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do we construct a query that uses the model?\n",
    "\n",
    "You almost always want to run a learning to rank model in a rescore query, but the TMDB data set isn't huge. We can use it directly with only a few hundred milliseconds to evaluate over the whole corpus. This is fun, because it let's us informally evaluate how well our model is doing.\n",
    "\n",
    "To query with the model, we create a function that runs the ltr query. Remember the model we built computes a relevance score for a document from three inputs that relate to the query and document:\n",
    "\n",
    "- The keyword's title TF\\*IDF score\n",
    "- The keyword's overview TF\\*IDF score\n",
    "- The movie's rating\n",
    "\n",
    "To compute the first two, we need to run the `titleSearch` and `overviewSearch` above for our current keywords. So we need to pass our model a version of these queries with the current keywords. That's what happens first in the function below. We inject our keywords into the inputs that are query-dependent. Then we add our `ratingSearch` that's only document dependent.\n",
    "\n",
    "These three queries are scored per document are then fed into the linear model. Remember [last time](http://opensourceconnections.com/blog/2017/04/01/learning-to-rank-linear-models/) this model is simple: each coefficient is just a weight on each signal's score. The model is simply a weighted sum of the scores of `titleSearch`, `overviewSearch`, and `ratingSearch` using coefficients as the weight!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runLtrQuery(keywords):\n",
    "\n",
    "    # Plugin our keywords\n",
    "    titleSearch['query']['match']['title'] = keywords\n",
    "    overviewSearch['query']['match']['overview'] = keywords\n",
    "    \n",
    "    # Format query    \n",
    "    ltrQuery = {\n",
    "        \"query\": {\n",
    "            \"ltr\": {\n",
    "                \"model\": {\n",
    "                    \"stored\": \"our_silly_model\"\n",
    "                },\n",
    "                \"features\": [titleSearch['query'], overviewSearch['query'], ratingSearch['query']]\n",
    "\n",
    "            }\n",
    "        },\n",
    "        \"size\": 3\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Search and print results!\n",
    "    results = es.search(index='tmdb', doc_type='movie', body=ltrQuery)\n",
    "    for result in results['hits']['hits']:\n",
    "        if 'title' in result['_source']:\n",
    "            print(result['_source']['title'])\n",
    "        else:\n",
    "            print(\"Movie %s (unknown title)\" % result['_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Taking the model on a test spin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With this function in place, let's run some searches! First a simple title search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forrest Gump\n",
      "Dead Men Don't Wear Plaid\n",
      "Maniac Cop\n"
     ]
    }
   ],
   "source": [
    "runLtrQuery('Forrest Gump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hey, not too shabby! How long will our luck go, let's try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robin Hood: Men in Tights\n",
      "Welcome to Sherwood! The Story of 'The Adventures of Robin Hood'\n",
      "Robin Hood\n"
     ]
    }
   ],
   "source": [
    "runLtrQuery('Robin Hood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wow lucky again. It's almost like these aren't lucky guesses but rather a prescient author is selecting examples that they know will look good! Ok, now let's try something closer to home: a Stallone Movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rambo III\n",
      "Rambo\n",
      "Rambo: First Blood Part II\n"
     ]
    }
   ],
   "source": [
    "runLtrQuery('Rambo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Err, not bad, but a bit off the mark... Well this is actually a case, like we talked about before, involving the nuance that the linear model can fail to capture. The linear model just knows \"more title score is good!\". But in *this* case, First Blood should be closer to the top. It *was* the original Rambo movie! Moreover Rambo III shouldn't really before just Rambo. \n",
    "\n",
    "Still, not bad for 40 odd examples of training data!\n",
    "\n",
    "Let's try something where we *don't* know the title. Like an example from Relevant Search `basketball with cartoon aliens`. Here we're grasping at straws. Hoping \"Space Jam\", a movie where Michael Jordan saves the world by playing aliens at basketball, comes up first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliens in the Attic\n",
      "Above the Rim\n",
      "Meet Dave\n"
     ]
    }
   ],
   "source": [
    "runLtrQuery('basketball with cartoon aliens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sadly, Not even close! To be fair,  we didn't show our training process examples of this use case. Most of our examples were direct, known-title navigational searches. This just goes to show you how it's important to get a broad set of representative samples across how your users search for learning to rank to work well.\n",
    "\n",
    "It also continues to demonstrate how the linear model struggles with nuance. Other models like gradient boosting (ala LambdaMART) can grok nuance faster, and aren't constrained to our boring linear definitions of functions. We'll see how these models work in future blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Next up - Solr!\n",
    "\n",
    "One of my colleagues will be taking Solr learning to rank out for a test spin for you. A simple linear model is very easy to understand, so it's fun for these little test-spins.\n",
    "\n",
    "I'd love to hear from you. If you'd like our help evaluating a learning to rank solution for your business, please [get in touch](mailto:TalkToUs@opensourceconnections.com)! And I'm always eager for feedback on these posts. Please let me know if I can learn a few things from you! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
