{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tuning-Analyzer-to-Improve-Recall\" data-toc-modified-id=\"Tuning-Analyzer-to-Improve-Recall-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tuning Analyzer to Improve Recall</a></span></li><li><span><a href=\"#Example-of-Creating-Index-that-uses-English-Analyzer\" data-toc-modified-id=\"Example-of-Creating-Index-that-uses-English-Analyzer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example of Creating Index that uses English Analyzer</a></span></li><li><span><a href=\"#Dealing-with-Delimiters-(Acronyms-&amp;-Phone-Numbers)\" data-toc-modified-id=\"Dealing-with-Delimiters-(Acronyms-&amp;-Phone-Numbers)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dealing with Delimiters (Acronyms &amp; Phone Numbers)</a></span></li><li><span><a href=\"#Capturing-Meaning-and-Modeling-Specificity-with-Synonyms\" data-toc-modified-id=\"Capturing-Meaning-and-Modeling-Specificity-with-Synonyms-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Capturing Meaning and Modeling Specificity with Synonyms</a></span></li><li><span><a href=\"#Modeling-Specificity-with-Paths\" data-toc-modified-id=\"Modeling-Specificity-with-Paths-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling Specificity with Paths</a></span></li><li><span><a href=\"#Tokenize-the-World\" data-toc-modified-id=\"Tokenize-the-World-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Tokenize the World</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature creation happens on both the query and document. When an analysis is properly performed, we can greatly improve the relevancy of our search results. Note that meaningful features doesn't have to be just text, it can be geographic locations, images, etc.\n",
    "\n",
    "\n",
    "There are three major stages to an analyzer:\n",
    "\n",
    "- Character filtering: This gives us the change to modify the entire piece of text, e.g. HTML tag filtering\n",
    "- Tokenization: This step chops the original text into a stream of tokens, e.g. using whitespace\n",
    "- Token filtering: Modifies the token stream by modifying, removing or inserting tokens, e.g. perform stemming, removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Analyzer to Improve Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                # create an analyzer called \"standard_clone\"\n",
    "                'standard_clone': {\n",
    "                    'tokenizer': 'standard',\n",
    "                    'filter': ['lowercase', 'stop']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bomb',\n",
       " 'dr',\n",
       " 'how',\n",
       " 'i',\n",
       " 'learned',\n",
       " 'love',\n",
       " 'stop',\n",
       " 'strangelove',\n",
       " 'worrying'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'analyzer': 'standard_clone',\n",
    "    'text': 'Dr. Strangelove: Or How I Learned to Stop Worrying and Love the Bomb'\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "# apart from the token, it also returns information such as offset and position\n",
    "# but here we are only interested in the token\n",
    "result = json.loads(response.text)\n",
    "tokens1 = set(token['token'] for token in result['tokens'])\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no match between the two queries\n",
    "data = {\n",
    "    'analyzer': 'standard_clone',\n",
    "    'text': \"mr. weirdlove: don't worry, I'm learning to start loving bombs\"\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "result = json.loads(response.text)\n",
    "tokens2 = set(token['token'] for token in result['tokens'])\n",
    "tokens2 & tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard analyzer can lead to a high precision but poor recall problem as the user's query must use the exact same words that occurred in the document. Let's see how we can avoid that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "\n",
    "            # syntax for accessing the available stemmer\n",
    "            # https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html\n",
    "            'filter': {\n",
    "                # normalize the words, e.g. walking, walked -> walk\n",
    "                # the stemmer uses heuristic for mapping words to its root form.\n",
    "                # Stemming is often times desirable as it makes the word represent\n",
    "                # their meaning by collapsing multiple representation of the same word\n",
    "                # into a single form. By performing stemming, our search engine's recall\n",
    "                # usually improves by quite a bit\n",
    "                # but there can be times when the heuristic leads to undesirable\n",
    "                # results, e.g. the word Main (a state in U.S.) will get normalized\n",
    "                # to the \"main\", we can avoid this by specifying a list of protected keyword\n",
    "                'english_stemmer': {\n",
    "                    'type': 'stemmer',\n",
    "                    'name': 'english'\n",
    "                },\n",
    "                # removes trailing s from words\n",
    "                'english_possessive_stemmer': {\n",
    "                    'type': 'stemmer',\n",
    "                    'name': 'possessive_english'\n",
    "                },\n",
    "                # remove a list of built-in english stop words,\n",
    "                # '_english_' is a keyword to specify use the built-in version\n",
    "                'english_stop': {\n",
    "                    'type': 'stop',\n",
    "                    'stopwords': '_english_'\n",
    "                },\n",
    "                # protect words from being modified by downstream stemmer\n",
    "                # https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-marker-tokenfilter.html\n",
    "                'english_keywords': {\n",
    "                    'type': 'keyword_marker',\n",
    "                    'keywords': ['maine']\n",
    "                }\n",
    "            },\n",
    "\n",
    "            'analyzer': {\n",
    "                'english_clone': {\n",
    "                    'type': 'custom',\n",
    "                    'tokenizer': 'standard',\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'english_possessive_stemmer',\n",
    "                        'english_stop',\n",
    "                        'english_keywords',  # note the ordering matters, define the keyword before stemming\n",
    "                        'english_stemmer'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[flower],[flower],[flower],[flower]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the stemming worked\n",
    "data = {\n",
    "    'analyzer': 'english_clone',\n",
    "    'text': 'flowers flower flowered flower'\n",
    "}\n",
    "\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "result = json.loads(response.text)\n",
    "','.join('[' + token['token'] + ']' for token in result['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main', 'maine'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see the keywords are protected\n",
    "data = {\n",
    "    'analyzer': 'english_clone',\n",
    "    'text': 'maine main'\n",
    "}\n",
    "\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "result = json.loads(response.text)\n",
    "set(token['token'] for token in result['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bomb', 'learn', 'love', 'worri'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, check to see if there's a match between the two queries\n",
    "data = {\n",
    "    'analyzer': 'english_clone',\n",
    "    'text': 'Dr. Strangelove: Or How I Learned to Stop Worrying and Love the Bomb'\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "# apart from the token, it also returns information such as offset and position\n",
    "# but here we are only interested in the token\n",
    "result = json.loads(response.text)\n",
    "tokens1 = set(token['token'] for token in result['tokens'])\n",
    "\n",
    "data = {\n",
    "    'analyzer': 'english_clone',\n",
    "    'text': \"mr. weirdlove: don't worry, I'm learning to start loving bombs\"\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "result = json.loads(response.text)\n",
    "tokens2 = set(token['token'] for token in result['tokens'])\n",
    "tokens2 & tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Creating Index that uses English Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.delete('http://localhost:9200/my_library')\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'number_of_shards': 1,\n",
    "        'number_of_replicas': 1,\n",
    "        'index': {\n",
    "            'analysis': {\n",
    "                'analyzer': {\n",
    "                    'default': {\n",
    "                        'type': 'english'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.put('http://localhost:9200/my_library', data=json.dumps(settings), headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index some sample documents,\n",
    "# some documents are very much about apples,\n",
    "# some are apple-ish, while some rarely mentions it\n",
    "documents = [\n",
    "    {'title': 'apples apple'},\n",
    "    {'title': 'apple apple apple apple apple'},\n",
    "    {'title': 'apple apple apple banana banana'},\n",
    "    {'title': 'apple banana blueberry coconut'}\n",
    "]\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    url = 'http://localhost:9200/my_library/_doc/%s' % (idx + 1)\n",
    "    response = requests.put(url, data=json.dumps(document), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num\tRelevance Score\tTitle\n",
      "1\t1.0476142\tapple apple apple banana banana\n",
      "2\t0.7985077\tapple banana blueberry coconut\n",
      "3\t0.18038376\tapple apple apple apple apple\n",
      "4\t0.16857682\tapples apple\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    url = 'http://localhost:9200/my_library/_doc/_search'\n",
    "    response = requests.get(url, data=json.dumps(query), headers=headers)\n",
    "    search_hits = json.loads(response.text)['hits']\n",
    "\n",
    "    print('Num\\tRelevance Score\\tTitle')\n",
    "    for idx, hit in enumerate(search_hits['hits']):\n",
    "        print('%s\\t%s\\t%s' % (idx + 1, hit['_score'], hit['_source']['title']))\n",
    "\n",
    "\n",
    "user_search = 'apple banana'\n",
    "query = {\n",
    "    'query': {\n",
    "        'match': {\n",
    "            'title': user_search\n",
    "        }\n",
    "    }\n",
    "}\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Delimiters (Acronyms & Phone Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acronyms are cases where dealing with delimiters inappropriately can\n",
    "# lead to poor results\n",
    "# e.g. I.B.M versus IBM\n",
    "# our analyzer should normalize various acrynoms so that\n",
    "# the resulting tokens remains the same\n",
    "\n",
    "# word_delimiter splits words into subwords and performs\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-tokenfilter.html\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'filter': {\n",
    "                'acronyms': {\n",
    "                    'type': 'word_delimiter',\n",
    "                    'generate_word_parts': False,\n",
    "                    'generate_number_parts': False,\n",
    "                    'catenate_all': True,\n",
    "                    # the preserve_original problem look-outs for cases\n",
    "                    # where acrynoms are mixed with actual english words\n",
    "                    # e.g. N.E.W -> new\n",
    "                    'preserve_original': True\n",
    "                }\n",
    "            },\n",
    "            'analyzer': {\n",
    "                'standard_with_acronyms': {\n",
    "                    'type': 'custom',\n",
    "                    'tokenizer': 'standard',\n",
    "                    'filter': ['lowercase', 'acronyms']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "requests.delete('http://localhost:9200/my_library')\n",
    "requests.put('http://localhost:9200/my_library', data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[i.b.m],[ibm],[ibm],[ibm]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'analyzer': 'standard_with_acronyms',\n",
    "    'text': 'I.B.M. IBM ibm'\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "# the resulting token has both i.b.m and ibm,\n",
    "# i.b.m exists since we specify the preserve_original argument to True\n",
    "result = json.loads(response.text)\n",
    "','.join('[' + token['token'] + ']' for token in result['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for phone numbers 1-800-86705309,\n",
    "# we wish to preserve the last 7 digits (the local number),\n",
    "# and the last 10 digits (long-distance number), so a user\n",
    "# can search for the phone number using any of the patterns\n",
    "\n",
    "# to a regex to emit a token for every captured group in the specified regex,\n",
    "# the link provides an example using emails\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-capture-tokenfilter.html\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'filter': {\n",
    "                'phone_num_filter': {\n",
    "                    'type': 'word_delimiter',\n",
    "                    'catenate_all': True,\n",
    "                    'generate_number_parts': False\n",
    "                },\n",
    "                'phone_num_parts': {\n",
    "                    'type': 'pattern_capture',\n",
    "                    'patterns': ['(\\\\d{7}$)', '(\\\\d{10}$)'],\n",
    "                    'preserve_original': True\n",
    "                }\n",
    "            },\n",
    "            'analyzer': {\n",
    "                'phone_num': {\n",
    "                    # here, we use the keyword tokenizer, the use-case\n",
    "                    # is where we're dealing with a phone number field,\n",
    "                    # instead of a text field that happens to contain phone numbers\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-tokenizer.html\n",
    "                    'tokenizer': 'keyword',\n",
    "                    'filter': ['phone_num_filter', 'phone_num_parts']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "requests.delete('http://localhost:9200/my_library')\n",
    "requests.put('http://localhost:9200/my_library', data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[18008675309],[8008675309],[8675309]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'analyzer': 'phone_num',\n",
    "    'text': '1(800)867-5309'\n",
    "}\n",
    "response = requests.get('http://localhost:9200/my_library/_analyze', \n",
    "                        data=json.dumps(data), headers=headers)\n",
    "\n",
    "# with phone numbers, capturing the meaningful subset of numbers is\n",
    "# one way of trying to capturing the user's intent. Doing this is\n",
    "# essentially ackowledging the fact that user will search for numbers\n",
    "# by entering local numbers or national numbers\n",
    "result = json.loads(response.text)\n",
    "','.join('[' + token['token'] + ']' for token in result['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Meaning and Modeling Specificity with Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. dress shoes, whenever the term dress immediately precedes shoes,\n",
    "# it has a specific concept\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/search-analyzer.html\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'filter': {\n",
    "                'english_stop': {\n",
    "                    'type': 'stop',\n",
    "                    'stopwords': '_english_'\n",
    "                },\n",
    "                'english_stemmer': {\n",
    "                    'type': 'stemmer',\n",
    "                    'name': 'english'\n",
    "                },\n",
    "                'english_possessive_stemmer': {\n",
    "                    'type': 'stemmer',\n",
    "                    'name': 'possessive_english'\n",
    "                },\n",
    "                # whenever we see either dress shoe or dress shoes, map that to\n",
    "                # the tokens specified at the right hand side of the =>\n",
    "                # And here, we are defining the index time and query time\n",
    "                # analyzer separately, since when we search for shoe, we want\n",
    "                # both shoe and dress shoe to show up, however, when we search\n",
    "                # for dress shoe, we only want dress shoe to show up. In other words,\n",
    "                # this analysis is asymmetric\n",
    "                'retail_syn_filter_index': {\n",
    "                    'type': 'synonym',\n",
    "                    'synonyms': ['dress shoe, dress shoes => dress_shoe, shoe']\n",
    "                },\n",
    "                'retail_syn_filter_search': {\n",
    "                    'type': 'synonym',\n",
    "                    'synonyms': ['dress shoe, dress shoes => dress_shoe']\n",
    "                }\n",
    "            },\n",
    "            'analyzer': {\n",
    "                'retail_analyzer_index': {\n",
    "                    'tokenizer': 'standard',\n",
    "                    # important to place the synonym before stemming, which is a more\n",
    "                    # drastic form of normalization and after lowercasing and possessive stemmer\n",
    "                    # so that words such as Dress shoes will still get matched\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'english_possessive_stemmer',\n",
    "                        'english_stop',\n",
    "                        'retail_syn_filter_index',\n",
    "                        'english_stemmer'\n",
    "                    ]\n",
    "                },\n",
    "                'retail_analyzer_search': {\n",
    "                    'tokenizer': 'standard',\n",
    "                    'filter': [\n",
    "                        'lowercase',\n",
    "                        'english_possessive_stemmer',\n",
    "                        'english_stop',\n",
    "                        'retail_syn_filter_search',\n",
    "                        'english_stemmer'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        '_doc': {\n",
    "            'properties': {\n",
    "                # for the 'desc', description field\n",
    "                'desc': {\n",
    "                    'type': 'text',\n",
    "                    'analyzer': 'retail_analyzer_index',\n",
    "                    'search_analyzer': 'retail_analyzer_search'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "requests.delete('http://localhost:9200/my_library')\n",
    "requests.put('http://localhost:9200/my_library', data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {'desc': 'bob brand dress shoes are the bomb'},  # dress shoe\n",
    "    {'desc': 'this little black dress is sure to impress'},  # dress\n",
    "    {'desc': 'tennis shoes... you know, for tennis'}  # tennis shoe\n",
    "]\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    url = 'http://localhost:9200/my_library/_doc/%s' % (idx + 1)\n",
    "    response = requests.put(url, data=json.dumps(document), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "searched for: dress shoes\n",
      "bob brand dress shoes are the bomb\n",
      "\n",
      "searched for: shoes\n",
      "bob brand dress shoes are the bomb\n",
      "tennis shoes... you know, for tennis\n"
     ]
    }
   ],
   "source": [
    "# the search gives us the pertinent result, where it returns\n",
    "# only dress shoes when searching for dress shoes while returning\n",
    "# both dress shoes and tennis shoes when searching for shoes.\n",
    "user_searches = ['dress shoes', 'shoes']\n",
    "\n",
    "url = 'http://localhost:9200/my_library/_search'\n",
    "for user_search in user_searches:\n",
    "    print('\\nsearched for:', user_search)\n",
    "    query = {\n",
    "        'query': {\n",
    "            'match': {\n",
    "                'desc': user_search\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.get(url, data=json.dumps(query), headers=headers)\n",
    "    search_hits = json.loads(response.text)['hits']['hits']\n",
    "    for search_hit in search_hits:\n",
    "        print(search_hit['_source']['desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the knowledge above and use it in the case of specificity e.g. we can index `fuji => fuji, apple, fruit`, in this case we are linking a topic to its \"parent\" topic, i.e. fuji is one kind of apple, so when a user searches for apple, it will match not only apple documents, but also fuji documents. This pattern is a tradeoff that we are making to improve recall at the expensive of decreasing precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Specificity with Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a filesystem search engine, when a user is searching for a document\n",
    "# in the path 'fruit/apples', the search result should return documents\n",
    "# from the children directory such as 'fruit/apples/fugi', 'fruit/apples/gala'\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html\n",
    "settings = {\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'path_analyzer': {\n",
    "                    'tokenizer': 'path_hierarchy'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        '_doc': {\n",
    "            'properties': {\n",
    "                'inventory_dir' : {\n",
    "                    'type': 'text',\n",
    "                    'analyzer': 'path_analyzer'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "requests.delete('http://localhost:9200/my_library')\n",
    "requests.put('http://localhost:9200/my_library', data=json.dumps(settings), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    # because of the path hierarchy tokenizer, the\n",
    "    # '/fruit/apples/fuji' will emit the following three\n",
    "    # terms '/fruit', '/fruit/apples' and '/fruit/apples/fuji'\n",
    "    {'desc': 'crisp, sweet-flavored, long shelf-life',\n",
    "     'inventory_dir': '/fruit/apples/fuji'},\n",
    "    {'desc': 'sweat, pleasant apple',\n",
    "     'inventory_dir': '/fruit/apples/gala'},\n",
    "    {'desc': 'edible, seed-bearing portion of plants',\n",
    "     'inventory_dir': '/fruit'}\n",
    "]\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    url = 'http://localhost:9200/my_library/_doc/%s' % (idx + 1)\n",
    "    response = requests.put(url, data=json.dumps(document), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fruit/apples/gala\n",
      "sweat, pleasant apple\n",
      "/fruit/apples/fuji\n",
      "crisp, sweet-flavored, long shelf-life\n"
     ]
    }
   ],
   "source": [
    "# query filter answers a yes or no question, thus no scores are\n",
    "# computed for this type of query\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html\n",
    "query = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'filter': [\n",
    "                {'term': {'inventory_dir': '/fruit/apples'}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "url = 'http://localhost:9200/my_library/_search'\n",
    "response = requests.get(url, data=json.dumps(query), headers=headers)\n",
    "\n",
    "search_hits = json.loads(response.text)['hits']['hits']\n",
    "for search_hit in search_hits:\n",
    "    source = search_hit['_source']\n",
    "    print(source['inventory_dir'])\n",
    "    print(source['desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply search on anything where we can extract meaningful and discrete features from the data that flow through the analysis/analyzer process. Apart from text, we can also consider tokenizing geographic information or images, etc. to turn our search engine into a more general purpose similarity system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
