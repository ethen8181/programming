{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-to-Rank\" data-toc-modified-id=\"Learning-to-Rank-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Learning to Rank</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Model Evaluation</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2020-03-28 22:06:34 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 7.9.0\n",
      "\n",
      "numpy 1.16.5\n",
      "pandas 0.25.0\n",
      "sklearn 0.21.2\n",
      "xgboost 0.81\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# prevent scientific notations\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://s3-us-west-2.amazonaws.com/xgboost-examples/MQ2008.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unrar x MQ2008.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: rename MQ2008/Fold1/*.txt to ./*.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# mv -f MQ2008/Fold1/*.txt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mMQ2008\u001b[m\u001b[m              MQ2008.rar.1        model.xgb           train.txt\r\n",
      "MQ2008.rar          learn_to_rank.ipynb test.txt            vali.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll print out the first few line of the raw data to understand that are some of the preprocessing steps required to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 qid:10002 1:0.007477 2:0.000000 3:1.000000 4:0.000000 5:0.007470 6:0.000000 7:0.000000 8:0.000000 9:0.000000 10:0.000000 11:0.471076 12:0.000000 13:1.000000 14:0.000000 15:0.477541 16:0.005120 17:0.000000 18:0.571429 19:0.000000 20:0.004806 21:0.768561 22:0.727734 23:0.716277 24:0.582061 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.780495 30:0.962382 31:0.999274 32:0.961524 33:0.000000 34:0.000000 35:0.000000 36:0.000000 37:0.797056 38:0.697327 39:0.721953 40:0.582568 41:0.000000 42:0.000000 43:0.000000 44:0.000000 45:0.000000 46:0.007042 #docid = GX008-86-4444840 inc = 1 prob = 0.086622\n",
      "\n",
      "0 qid:10002 1:0.603738 2:0.000000 3:1.000000 4:0.000000 5:0.603175 6:0.000000 7:0.000000 8:0.000000 9:0.000000 10:0.000000 11:0.000000 12:0.000000 13:0.122130 14:0.000000 15:0.000000 16:0.998377 17:0.375000 18:1.000000 19:0.000000 20:0.998128 21:0.000000 22:0.000000 23:0.154578 24:0.555676 25:0.000000 26:0.000000 27:0.000000 28:0.000000 29:0.071711 30:0.000000 31:0.000000 32:0.000000 33:0.000000 34:0.000000 35:0.000000 36:0.000000 37:0.000000 38:0.000000 39:0.117399 40:0.560607 41:0.000000 42:0.280000 43:0.000000 44:0.003708 45:0.333333 46:1.000000 #docid = GX037-06-11625428 inc = 0.0031586555555558 prob = 0.0897452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'train.txt'\n",
    "\n",
    "with open(input_path) as f:\n",
    "    for _ in range(2):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the [documentation](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/#!letor-4-0): each row is a query-document pair. The first column is relevance label of this pair, the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. The larger the relevance label, the more relevant the query-document pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code chunk parses out the query ids, input features, relevance label and group information. The group information stores the number of instances in each group. For example if our group list looks like `[5, 8, 7]`, that means, the first 5 items belongs to the first group, the next 8 items belongs to the second group and so on. This grouping information is needed by XGBoost to determine which group of items are to be compared against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_data(input_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    query_ids = []\n",
    "\n",
    "    n_group = 0\n",
    "    current_group = ''\n",
    "    with open(input_path) as f:\n",
    "        for line in f:\n",
    "            # filter out comment about the pair\n",
    "            if '#' in line:\n",
    "                line = line[:line.index('#')]\n",
    "\n",
    "            splits = line.strip().split(' ')\n",
    "\n",
    "            feature = np.array([float(feature_str.split(':')[1]) for feature_str in splits[2:]])\n",
    "            data.append(feature)\n",
    "\n",
    "            label = int(splits[0])\n",
    "            labels.append(label)\n",
    "\n",
    "            query_id = splits[1]\n",
    "            query_ids.append(query_id)\n",
    "            \n",
    "            # keep accumulating the number of items in a group until a new\n",
    "            # query id is encountered\n",
    "            if current_group == '':\n",
    "                current_group = query_id\n",
    "                n_group += 1\n",
    "            elif current_group == query_id:\n",
    "                n_group += 1\n",
    "            else:\n",
    "                groups.append(n_group)\n",
    "                current_group = query_id\n",
    "                n_group = 1\n",
    "\n",
    "        # make sure to append the last group\n",
    "        groups.append(n_group)\n",
    "\n",
    "    return np.array(data), np.array(labels), np.array(query_ids), np.array(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the data to do some quick inspection. As the group stores the number of records in each group, summing all the numbers up should give us the total number of records, this is a quick sanity check to make sure we parse the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample group:\n",
      "[  8   8   8   8   8  16   8 118  16   8   8   8   7   8  16   8  16   8\n",
      "  32   8]\n",
      "total items:  9630\n",
      "data dimension:\n",
      "(9630, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.007477, 0.      , 1.      , ..., 0.      , 0.      , 0.007042],\n",
       "       [0.603738, 0.      , 1.      , ..., 0.003708, 0.333333, 1.      ],\n",
       "       [0.214953, 0.      , 0.      , ..., 1.      , 1.      , 0.021127],\n",
       "       ...,\n",
       "       [1.      , 0.      , 0.      , ..., 0.060915, 0.454545, 0.      ],\n",
       "       [0.259641, 0.6     , 0.      , ..., 0.051975, 0.090909, 0.      ],\n",
       "       [0.791031, 0.      , 0.      , ..., 0.001754, 0.181818, 0.      ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = 'train.txt'\n",
    "X_train, y_train, query_train, group_train = parse_raw_data(input_path)\n",
    "\n",
    "print('sample group:')\n",
    "print(group_train[:20])\n",
    "print('total items: ', np.sum(group_train))\n",
    "\n",
    "print('data dimension:')\n",
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7820, 1223,  587])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of the relevance label\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample group:\n",
      "[15  8  7 31  7  7  8 15  8  8 16  8 30 16 16 16 31  8  8 31]\n",
      "data dimension:\n",
      "(2707, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.      , 0.      , 0.      , ..., 0.197431, 0.5     , 0.      ],\n",
       "       [0.003315, 0.      , 1.      , ..., 0.212859, 0.25    , 0.214286],\n",
       "       [0.093923, 0.      , 0.      , ..., 0.309468, 1.      , 0.357143],\n",
       "       ...,\n",
       "       [0.018219, 0.5     , 0.      , ..., 1.      , 1.      , 0.25    ],\n",
       "       [0.006073, 0.      , 0.      , ..., 0.111722, 0.      , 0.125   ],\n",
       "       [1.      , 0.5     , 0.      , ..., 0.006428, 0.      , 0.      ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = 'vali.txt'\n",
    "X_val, y_val, query_val, group_val = parse_raw_data(input_path)\n",
    "\n",
    "print('sample group:')\n",
    "print(group_val[:20])\n",
    "\n",
    "print('data dimension:')\n",
    "print(X_val.shape)\n",
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the learning to rank model is very similar to training regression or classification model, except the objective is different, and we also need to pass in the group information to the Ranker class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval_0-ndcg:0.837363\teval_1-ndcg:0.785951\n",
      "Multiple eval metrics have been passed: 'eval_1-ndcg' will be used for early stopping.\n",
      "\n",
      "Will train until eval_1-ndcg hasn't improved in 5 rounds.\n",
      "[10]\teval_0-ndcg:0.861817\teval_1-ndcg:0.805565\n",
      "Stopping. Best iteration:\n",
      "[13]\teval_0-ndcg:0.861784\teval_1-ndcg:0.808128\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bytree=1, gamma=1.0, learning_rate=0.01, max_delta_step=0,\n",
       "          max_depth=6, min_child_weight=0.1, missing=None, n_estimators=100,\n",
       "          n_jobs=-1, nthread=None, objective='rank:ndcg', random_state=0,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "          subsample=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'rank:ndcg',\n",
    "    'learning_rate': 0.01,\n",
    "    'gamma': 1.0,\n",
    "    'min_child_weight': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    'verbose': 10,\n",
    "    'early_stopping_rounds': 5,\n",
    "    'eval_metric': 'ndcg',\n",
    "    'eval_set': [(X_train, y_train), (X_val, y_val)],\n",
    "    'eval_group': [group_train, group_val]\n",
    "}\n",
    "\n",
    "model = xgb.sklearn.XGBRanker(**params)\n",
    "model.fit(X_train, y_train, group_train, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick reminder that when using predict, the default behavior is to use all the trees to to the prediction, if we leverage the early stopping functionality to determine the best amount of trees required, then we should either specify `None` or the use `best_ntree_limit` to generate the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53620577, 0.33488166, 0.46941793, ..., 0.33488166, 0.3867974 ,\n",
       "       0.5271703 ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default behaviour is to use all trees\n",
    "pred_val = model.predict(X_val)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5332428 , 0.3766683 , 0.46111014, ..., 0.3766683 , 0.4103794 ,\n",
       "       0.53116554], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use None is use only up to the best iteration determined\n",
    "pred_val = model.predict(X_val, ntree_limit=None)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5332428 , 0.3766683 , 0.46111014, ..., 0.3766683 , 0.4103794 ,\n",
       "       0.53116554], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or use the best_ntree_limit attribute, which is available when using\n",
    "# the early stopping functionality\n",
    "pred_val = model.predict(X_val, ntree_limit=model.best_ntree_limit)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon determining the best tree limit, we can also refit the model on the entire training and validation set using that tree number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([X_train, X_val])\n",
    "y = np.hstack([y_train, y_val])\n",
    "group = np.hstack([group_train, group_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.get_params()\n",
    "\n",
    "# by directly accessing best_ntree_limit attribute, we assume\n",
    "# the model always uses early_stopping_rounds in the fit parameter\n",
    "if params['n_estimators'] != model.best_ntree_limit:\n",
    "\n",
    "    # We over-ride the current n_estimators with the best_ntree_limit and refit the model again\n",
    "    params['n_estimators'] = model.best_ntree_limit\n",
    "    best_model = xgb.sklearn.XGBRanker(**params)\n",
    "\n",
    "    del fit_params['early_stopping_rounds']\n",
    "    best_model.fit(X, y, group, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model evaluation part is where things becomes a little bit different. As we were ranking the items within each group, we need to evaluate the model on how well it is ranking the items within each group.\n",
    "\n",
    "We'll use NDCG as the evaluation metric in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(y_true, y_score, k=None):\n",
    "    actual = dcg_at_k(y_true, y_score, k)\n",
    "    best = dcg_at_k(y_true, y_true, k) \n",
    "    ndcg = actual / best\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def dcg_at_k(y_true, y_score, k=None):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    if k is not None:\n",
    "        order = order[:k]\n",
    "\n",
    "    y_true = np.take(y_true, order)\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(2, gains.size + 2))\n",
    "    dcg = np.sum(gains / discounts)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the prediction and relevance label for one of group and compute the NDCG metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of examples in group:\n",
      " 15\n",
      "prediction score:\n",
      " [0.5332428  0.3766683  0.46111014 0.6059945  0.60195273 0.37404552\n",
      " 0.40666327 0.37734008 0.60195273 0.39321342 0.37554443 0.38511944\n",
      " 0.37404552 0.37647572 0.41525683]\n",
      "relevance label:\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "n_group = group_val[0]\n",
    "y_score = pred_val[:n_group]\n",
    "y_true = y_val[:n_group]\n",
    "print('num of examples in group:\\n', n_group)\n",
    "print('prediction score:\\n', y_score)\n",
    "print('relevance label:\\n', y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503449055347546"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over each group to compute the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyuliu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.85034491, 0.38685281, 0.63990933, 0.63752463,        nan,\n",
       "       0.35620719, 0.98289208, 0.85261156, 0.82131371, 1.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_array = []\n",
    "\n",
    "start = 0\n",
    "for n_group in group_val:\n",
    "    end = start + n_group\n",
    "    y_score = pred_val[start:end]\n",
    "    y_true = y_val[start:end]\n",
    "    ndcg = ndcg_at_k(y_true, y_score)\n",
    "    ndcg_array.append(ndcg)\n",
    "    start = end\n",
    "\n",
    "ndcg_array = np.array(ndcg_array)\n",
    "ndcg_array[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when computing the evaluation metric for each group, we get nan results for some of the groups. This is due to the fact that the relevance labels in that group is all 0. We can filter out these nan elements afterwards, or exclude them altogether from the data at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5731795  0.37404552 0.3808377  0.45763242 0.37404552 0.6059945\n",
      " 0.3766683 ]\n",
      "[0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyuliu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = np.sum(group_val[:4])\n",
    "n_group = group_val[4]\n",
    "end = start + n_group\n",
    "y_score = pred_val[start:end]\n",
    "y_true = y_val[start:end]\n",
    "print(y_score)\n",
    "print(y_true)\n",
    "\n",
    "ndcg = ndcg_at_k(y_true, y_score)\n",
    "ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop we've implemented above can become very slow for large datasets, there are different approach for speeding up the crude for loop, the following code chunk uses pandas to perform vectorized computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qid:15928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qid:15928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qid:15928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qid:15928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qid:15928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid  label  score\n",
       "0  qid:15928      0  0.533\n",
       "1  qid:15928      0  0.377\n",
       "2  qid:15928      1  0.461\n",
       "3  qid:15928      1  0.606\n",
       "4  qid:15928      0  0.602"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_col = 'qid'\n",
    "label_col = 'label'\n",
    "score_col = 'score'\n",
    "df_val = pd.DataFrame({\n",
    "    group_col: query_val,\n",
    "    label_col: y_val,\n",
    "    score_col: pred_val\n",
    "})\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg(df, group_col, label_col, score_col, k=None):\n",
    "\n",
    "    df_label_sorted = df.sort_values([group_col, label_col], ascending=False)\n",
    "    df_score_sorted = df.sort_values([group_col, score_col], ascending=False)\n",
    "    \n",
    "    df_score_sorted['actual_rank'] = df_score_sorted.groupby(group_col).cumcount()\n",
    "    df_score_sorted['best_rank'] = df_label_sorted.groupby(group_col).cumcount()\n",
    "\n",
    "    item_gain = 2 ** df_score_sorted[label_col] - 1\n",
    "    df_score_sorted['actual_dcg'] = item_gain / np.log2(df_score_sorted['actual_rank'] + 2)\n",
    "    df_score_sorted['best_dcg'] = item_gain / np.log2(df_score_sorted['best_rank'] + 2)\n",
    "\n",
    "    if k is not None and k > 0:\n",
    "        df_actual_dcg_at_k = df_score_sorted[df_score_sorted['actual_rank'] < k]\n",
    "        df_best_dcg_at_k = df_score_sorted[df_score_sorted['best_rank'] < k]\n",
    "    else:\n",
    "        df_actual_dcg_at_k = df_score_sorted\n",
    "        df_best_dcg_at_k = df_score_sorted\n",
    "\n",
    "    actual_dcg = df_actual_dcg_at_k.groupby(group_col)['actual_dcg'].sum()\n",
    "    best_dcg = df_best_dcg_at_k.groupby(group_col)['best_dcg'].sum()\n",
    "    ndcg_array = np.array(actual_dcg / best_dcg)\n",
    "    ndcg_array = ndcg_array[~np.isnan(ndcg_array)]\n",
    "    return ndcg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85034491, 0.38685281, 0.63990933, 0.70218067, 0.33333333,\n",
       "       0.98289208, 0.85261156, 0.82131371, 1.        , 0.81243718])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_array = compute_ndcg(df_val, group_col, label_col, score_col)\n",
    "ndcg_array[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Github: XGBoost Example - Learning to Rank](https://github.com/dmlc/xgboost/tree/master/demo/rank)\n",
    "- [Blog: Learning to Rank Explained (with Code)](https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
