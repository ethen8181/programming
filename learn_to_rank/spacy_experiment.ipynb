{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2018-11-13 10:10:08 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.4.0\n",
      "\n",
      "numpy 1.14.1\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "matplotlib 2.2.2\n",
      "spacy 2.0.16\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# change default style figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,matplotlib,spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/processing-pipelines#multi-processing-example\n",
    "\n",
    "# changes in 2.0 versus that makes the nlp.pip behave differently than 1.0\n",
    "# https://github.com/explosion/spaCy/issues/2075\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from toolz import partition_all\n",
    "import thinc.extra.datasets\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "def is_valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number, or is one\n",
    "    of the un-wanted POS tag\n",
    "\n",
    "    http://universaldependencies.org/u/pos/\n",
    "    \"\"\"\n",
    "    pos_flag = token.pos_ not in {'ADP', 'CCONJ', 'PRON'}\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num or token.is_stop)\n",
    "    word_len_flag = len(token) >= 2\n",
    "    valid_flag = pos_flag and word_flag and word_len_flag\n",
    "    return valid_flag\n",
    "\n",
    "\n",
    "def preprocess_text(texts, nlp):\n",
    "    \"\"\"https://gist.github.com/smsubrahmannian/2835bd32c688b7b57a5300f94af07b1b\"\"\"\n",
    "\n",
    "    for word in nlp.pipe(texts, n_threads=1):\n",
    "        ' '.join(word.lemma_ for word in doc if is_valid_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "for w in ENGLISH_STOP_WORDS:\n",
    "    # spacy syntax for adding custom stop words\n",
    "    nlp.vocab[w].is_stop = True\n",
    "    \n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# imdb() returns train and test data\n",
    "# data is a list of tuple (text, sentiment label)\n",
    "data, _ = thinc.extra.datasets.imdb()\n",
    "text, _ = zip(*data)\n",
    "partitions = partition_all(batch_size, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parallel = Parallel(n_jobs=-1)\n",
    "task = delayed(preprocess_text)\n",
    "parallel(task(batch, nlp) for batch in partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can i say about the first film ever?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You can't rate this, because it's not supposed to be entertaining.\n",
      "But if you HAVE to rate it, you should give it a 10.\n",
      "It is stunning to see moving images from the year 1895.\n",
      "This was one of the most important movies in history.\n",
      "I wonder how it was to be one of the people who saw the first movie ever!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for word in doc.sents:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What can i say about the first film ever?\\n\\n\\n\\nYou can't rate this, because it's not supposed to be entertaining. But if you HAVE to rate it, you should give it a 10. It is stunning to see moving images from the year 1895. This was one of the most important movies in history. I wonder how it was to be one of the people who saw the first movie ever!\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = text[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "What can i say about the first film ever?\n",
       "\n",
       "\n",
       "\n",
       "You can't rate this, because it's not supposed to be entertaining. But if you HAVE to rate it, you should give it a 10. It is stunning to see moving images from the year 1895. This was one of the most important movies in history. I wonder how it was to be one of the people who saw the first movie ever!\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(temp)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n",
      "as ADP mark\n",
      "it PRON nsubj\n",
      "is VERB advcl\n",
      "quite ADJ advmod\n",
      "a DET det\n",
      "succesful ADJ amod\n",
      "startup NOUN attr\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/spacy-101#section-features\n",
    "import spacy\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion as it is quite a succesful startup')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "sentences = [doc.strip().split() for doc in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from time import time\n",
    "from joblib import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from toolz import partition_all\n",
    "\n",
    "\n",
    "def export_unigrams(unigram_path, texts, parser,\n",
    "                    batch_size = 50, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Preprocessed the raw text and export it to a .txt file,\n",
    "    where each line is one document, for what sort of preprocessing\n",
    "    is done, please refer to the `clean_corpus` function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unigram_path : str\n",
    "        output file path of the preprocessed unigram text\n",
    "\n",
    "    texts : iterable\n",
    "        iterable can be simply a list, but for larger corpora,\n",
    "        consider an iterable that streams the sentences directly from\n",
    "        disk/network using Gensim's Linsentence or something along\n",
    "        those line\n",
    "\n",
    "    parser : spacy model object\n",
    "        e.g. parser = spacy.load('en')\n",
    "\n",
    "    batch_size : int, default 10000\n",
    "        batch size for the spacy preprocessing\n",
    "\n",
    "    n_jobs : int, default -1\n",
    "        number of jobs/cores/threads to use for the spacy preprocessing\n",
    "    \"\"\"\n",
    "    with open(unigram_path, 'w', encoding='utf_8') as f:\n",
    "        # partition = partition_all(batch_size, texts)\n",
    "\n",
    "        parallel = Parallel(n_jobs=n_jobs)\n",
    "        task = delayed(preprocess_text)\n",
    "        cleaned_text = parallel(task(text) for text in texts)\n",
    "        f.write(cleaned_text + '\\n')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Generator function using spaCy to parse reviews:\n",
    "    - lemmatize the text\n",
    "    - remove punctuation, whitespace and number\n",
    "    - remove some pos tags\n",
    "    \"\"\"\n",
    "    nlp_parser = spacy.load('en_core_web_sm')\n",
    "    parsed_text = nlp_parser(text)\n",
    "    cleaned_text = ' '.join(token.lemma_ for token in parsed_text if is_valid_word(token))\n",
    "    return cleaned_texts\n",
    "\n",
    "\n",
    "def is_valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number, or is one\n",
    "    of the un-wanted POS tag\n",
    "\n",
    "    http://universaldependencies.org/u/pos/\n",
    "    \"\"\"\n",
    "    pos_flag = token.pos_ not in {'ADP', 'CCONJ'}\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num or token.is_stop)\n",
    "    word_len_flag = len(token) >= 2\n",
    "    valid_flag = pos_flag and word_flag and word_len_flag\n",
    "    return valid_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-63:\n",
      "Process ForkPoolWorker-64:\n",
      "Process ForkPoolWorker-61:\n",
      "Process ForkPoolWorker-60:\n",
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-62:\n",
      "Process ForkPoolWorker-58:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/mingyuliu/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/mingyuliu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50190a997fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNIGRAM_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mexport_unigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNIGRAM_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewsgroups_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0melapse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text preprocessing, elapse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-46c11fc51557>\u001b[0m in \u001b[0;36mexport_unigrams\u001b[0;34m(unigram_path, texts, parser, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib-0.11-py3.6.egg/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# create a directory called 'model' to\n",
    "# store all outputs in later section\n",
    "MODEL_DIR = 'model'\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "UNIGRAM_PATH = os.path.join(MODEL_DIR, 'unigram.txt')\n",
    "if not os.path.exists(UNIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_unigrams(UNIGRAM_PATH, texts = newsgroups_train.data[:100], parser = nlp)\n",
    "    elapse = time() - start\n",
    "    print('text preprocessing, elapse', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from time import time\n",
    "from joblib import cpu_count\n",
    "\n",
    "\n",
    "def export_unigrams(unigram_path, texts, parser,\n",
    "                    batch_size = 10000, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Preprocessed the raw text and export it to a .txt file,\n",
    "    where each line is one document, for what sort of preprocessing\n",
    "    is done, please refer to the `clean_corpus` function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unigram_path : str\n",
    "        output file path of the preprocessed unigram text\n",
    "\n",
    "    texts : iterable\n",
    "        iterable can be simply a list, but for larger corpora,\n",
    "        consider an iterable that streams the sentences directly from\n",
    "        disk/network using Gensim's Linsentence or something along\n",
    "        those line\n",
    "\n",
    "    parser : spacy model object\n",
    "        e.g. parser = spacy.load('en')\n",
    "\n",
    "    batch_size : int, default 10000\n",
    "        batch size for the spacy preprocessing\n",
    "\n",
    "    n_jobs : int, default -1\n",
    "        number of jobs/cores/threads to use for the spacy preprocessing\n",
    "    \"\"\"\n",
    "    with open(unigram_path, 'w', encoding = 'utf_8') as f:\n",
    "        for cleaned_text in clean_corpus(texts, parser, batch_size, n_jobs):\n",
    "            f.write(cleaned_text + '\\n')\n",
    "\n",
    "\n",
    "def clean_corpus(texts, parser, batch_size, n_jobs):\n",
    "    \"\"\"\n",
    "    Generator function using spaCy to parse reviews:\n",
    "    - lemmatize the text\n",
    "    - remove punctuation, whitespace and number\n",
    "    - remove some pos tags\n",
    "    \"\"\"\n",
    "    n_threads = cpu_count()\n",
    "    if n_jobs > 0 and n_jobs < n_threads:\n",
    "        n_threads = n_jobs\n",
    "\n",
    "    # use the .pip to process texts as a stream;\n",
    "    # this functionality supports using multi-threads\n",
    "    for parsed_text in parser.pipe(texts, n_threads = 1, batch_size = batch_size):\n",
    "        tokens = []\n",
    "        for token in parsed_text:\n",
    "            if is_valid_word(token):\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        yield cleaned_text\n",
    "\n",
    "\n",
    "def is_valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number, or is one\n",
    "    of the un-wanted POS tag\n",
    "\n",
    "    http://universaldependencies.org/u/pos/\n",
    "    \"\"\"\n",
    "    pos_flag = token.pos_ not in {'ADP', 'CCONJ'}\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num or token.is_stop)\n",
    "    word_len_flag = len(token) >= 2\n",
    "    valid_flag = pos_flag and word_flag and word_len_flag\n",
    "    return valid_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocessing, elapse 6.267408847808838\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# create a directory called 'model' to\n",
    "# store all outputs in later section\n",
    "MODEL_DIR = 'model'\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "UNIGRAM_PATH = os.path.join(MODEL_DIR, 'unigram.txt')\n",
    "if not os.path.exists(UNIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_unigrams(UNIGRAM_PATH, texts = newsgroups_train.data[:100], parser = nlp)\n",
    "    elapse = time() - start\n",
    "    print('text preprocessing, elapse', elapse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
