{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-09-09 10:35:04 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "matplotlib 2.0.0\n",
      "pyspark 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the SparkSession class,\n",
    "# which is the entry point into all functionality in Spark\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')  # set it to run on all cores on a local machine\n",
    "         .appName('Practice')\n",
    "         .config(conf = SparkConf())\n",
    "         .getOrCreate())\n",
    "\n",
    "# set the log level to ERROR to prevent \n",
    "# the terminal from showing too many information\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "## Background\n",
    "\n",
    "Spark is a fast and general framework for large scale data processing. \n",
    "\n",
    "- It is often times compared with Hadoop MapReduce and it claims to be 10 ~ 100 times faster depending on whether we're running it on memory or disk. Of course, you're mileage may vary, but it is proven to be faster in most cases and has some modern features. One reason that it's faster is that it uses a **directed acyclic graph (DAG)** to optimize the workload. At a high level, this means Spark will not execute our task until we've explicitly specify that we've wish to collect the result. Because of this, it can optimize the execution of all our intermediate steps whenever possible.\n",
    "- The framework gives us access to the power of horizontal scaling, i.e. we can start of by writing spark code that runs on our laptop for development purposes and the same code can be used to run on a cluster on computers and the cloud. At a high level, the way spark works is we start off by developing the driver program, then on top of that, we have a cluster manager. For the cluster manager, spark comes packaged with its own cluster manager, but we can also use other options such as YARN. This cluster manager is responsible for distributing the work defined by our driver script among mutliple nodes/machines. Every machine that we run on will have an Executor process, which has its own cache and list of tasks. Each executor is in charge of finishing the operation we've defined on the chunk of data it receives and then aggregating the results back together to pass it along to the next step. As an end-user, for the most part, all we need to worry about is the logic of our driver program, i.e. how we wish to process the data. And spark and the cluster manager will take care of efficiently distributing the workload among the multiple machines at its disposable.\n",
    "- Spark itself consists of many components, including **Spark Core:** it deals with basic operations, such as manipulating with our data and collecting the results of various operations. Then there are libraries that are built on top of that core to make complex operations easier. e.g. **Spark Streaming** for dealing with data that comes in real time; **Spark SQL** for manipulating with data in a SQL-like manner; **Spark ML/MLLib** allows us to do machine learning on large datasets; **GraphX** framework for getting attributes out of network like data.\n",
    "\n",
    "\n",
    "## Resilient Distributed Dataset (RDD)\n",
    "\n",
    "**Resilient Distributed Dataset (RDD)** was the primary user-facing API in Spark since its inception. At its core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes on our cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
    "\n",
    "The whole idea is that by working with RDDs, all we need to worry about is the operations that we wish to perform to our data. Then we let RDDs handle the 1) Resilient - making sure that it's fault-tolerant, so when running the process on multiple machines, it can still move on with the exeuction if one machine happens to go down. 2) Distributed - making sure the operation is parallelized across multiple machines in an efficient manner. 3) Dataset - At its core, it is still rows and rows of dataset.\n",
    "\n",
    "\n",
    "## DataFrames\n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "## Practice 1\n",
    "\n",
    "Count the number of times each rating number occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n",
      "|user|item|rating|timestamp|\n",
      "+----+----+------+---------+\n",
      "| 196| 242|     3|881250949|\n",
      "| 186| 302|     3|891717742|\n",
      "|  22| 377|     1|878887116|\n",
      "| 244|  51|     2|880606923|\n",
      "| 166| 346|     1|886397596|\n",
      "+----+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('ml-100k', 'u.data')\n",
    "df = spark.read.csv(data_path, sep = '\\t', header = None)\n",
    "\n",
    "# probably easiest way of renaming multiple columns\n",
    "df = df.toDF('user', 'item', 'rating', 'timestamp')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>27145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>21201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>34174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>11370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rating  count\n",
       "0      3  27145\n",
       "1      5  21201\n",
       "2      1   6110\n",
       "3      4  34174\n",
       "4      2  11370"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_col = 'rating'\n",
    "results = (df\n",
    "           .select(rating_col)\n",
    "           .groupBy(rating_col)\n",
    "           .count()\n",
    "           .toPandas())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 2\n",
    "\n",
    "Finding the average ratings per item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>avg(rating)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>829</td>\n",
       "      <td>2.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1436</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>467</td>\n",
       "      <td>3.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>691</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1090</td>\n",
       "      <td>2.405405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item  avg(rating)\n",
       "0   829     2.647059\n",
       "1  1436     2.500000\n",
       "2   467     3.791667\n",
       "3   691     3.500000\n",
       "4  1090     2.405405"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast the ratings to float as oppose to the original string type\n",
    "# so we can perform the average operation\n",
    "item_col = 'item'\n",
    "results = (df\n",
    "           .withColumn(rating_col, F.col(rating_col).cast('float'))\n",
    "           .groupBy(item_col)\n",
    "           .mean(rating_col)\n",
    "           .toPandas())\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 3\n",
    "\n",
    "Finding the minimum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+-----------+\n",
      "|    station|    date|entry_type|temperature|\n",
      "+-----------+--------+----------+-----------+\n",
      "|ITE00100554|18000101|      TMAX|        -75|\n",
      "|ITE00100554|18000101|      TMIN|       -148|\n",
      "|GM000010962|18000101|      PRCP|          0|\n",
      "|EZE00100082|18000101|      TMAX|        -86|\n",
      "|EZE00100082|18000101|      TMIN|       -135|\n",
      "+-----------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('sparkScala', 'code', '1800.csv')\n",
    "df = (spark\n",
    "      .read.csv(data_path, sep = ',', header = None)\n",
    "      .select('_c0', '_c1', '_c2', '_c3')  # the pattern of the default column name _c[number]\n",
    "      .toDF('station', 'date', 'entry_type', 'temperature'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>min(temperature)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>5.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZE00100082</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station  min(temperature)\n",
       "0  ITE00100554              5.36\n",
       "1  EZE00100082              7.70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "def to_fahrenheit(record):\n",
    "    \"\"\"converts a tenth of a celsius to fahrenheit\"\"\"\n",
    "    record = float(record) * 0.1 * 9 / 5 + 32\n",
    "    return record\n",
    "\n",
    "\n",
    "temperature_col = 'temperature'\n",
    "udf_to_fahrenheit = F.udf(to_fahrenheit, FloatType())\n",
    "results = (df\n",
    "           .withColumn(temperature_col, udf_to_fahrenheit(F.col(temperature_col)))\n",
    "           .filter(F.col('entry_type') == 'TMIN')\n",
    "           .groupBy('station')\n",
    "           .min(temperature_col)\n",
    "           .toPandas())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 4\n",
    "\n",
    "Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            sentence|\n",
      "+--------------------+\n",
      "|            Hi, you!|\n",
      "|     No under_score!|\n",
      "| *      Remove pu...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "sentence = spark.createDataFrame([('Hi, you!',),\n",
    "                                  (' No under_score!',),\n",
    "                                  (' *      Remove punctuation then spaces  * ',)], ['sentence'])\n",
    "sentence.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a function that does some standard text preprocessing:\n",
    "\n",
    "- All punctuation should be removed.\n",
    "- Any leading or trailing spaces on a line should be removed.\n",
    "- Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word) .\n",
    "\n",
    "For these tasks we shouldn't need to use user-defined functions. Unlike UDFs, Spark SQL functions operate directly on JVM, so they can operate on data in its \"native\" representation without having to perform a lot of serialization and deserialization and typically are well integrated with both Catalyst and Tungsten. It means these can be optimized in the execution plan. If we wrote the UDFs in non-native language in Python is might even lead to more overhead as it involves data movement between Python interpreter and JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(column):\n",
    "    \"\"\"\n",
    "    Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    column : Column\n",
    "        A Column containing a sentence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cleaned : Column\n",
    "        A Column with clean-up operations applied.\n",
    "    \"\"\"\n",
    "    removed_punct = F.regexp_replace(column, r'[^a-zA-Z0-9\\s]', '')\n",
    "    cleaned = F.trim(F.lower(removed_punct))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "| underscore|    1|\n",
      "|        you|    1|\n",
      "|     remove|    1|\n",
      "|     spaces|    1|\n",
      "|         no|    1|\n",
      "|punctuation|    1|\n",
      "|         hi|    1|\n",
      "|       then|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_col = 'word'\n",
    "cleaned_col = 'cleaned'\n",
    "\n",
    "# after cleaning up the text, we apply the .split spark SQL function\n",
    "# to split the words into an array and apply .explode to return a new\n",
    "# row for every element in the array (documentation page listed below)\n",
    "# http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "(sentence\n",
    " .select(clean(F.col('sentence')).alias(cleaned_col))\n",
    " .select(F.explode(F.split(cleaned_col, r'\\s+')).alias(word_col))\n",
    " .groupBy(word_col)\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 5\n",
    "\n",
    "Most popular movies (popular as in the number of times it's been viewed).\n",
    "\n",
    "Broadcast variables: explicitly sending the read-only piece of data to all the nodes, so that it will only be send across the network once and will be available when it's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.Broadcast to ship off the variable\n",
    "data_path = os.path.join('ml-100k', 'u.data')\n",
    "df = (spark\n",
    "      .read.csv(data_path, sep = '\\t', header = None)\n",
    "      .toDF('user', 'item', 'rating', 'timestamp'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_col = 'item'\n",
    "results = (df\n",
    "           .select(item_col)\n",
    "           .groupBy(item_col)\n",
    "           .count()\n",
    "           .orderBy(F.desc('count'))\n",
    "           .toPandas())\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Stackoverflow: Spark functions vs UDF performance?](https://stackoverflow.com/questions/38296609/spark-functions-vs-udf-performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'hostid': [1, 1, 2, 2],\n",
    "    'itemname': ['A', 'B', 'A', 'C'],\n",
    "    'itemvalue': [10, 3, 9, 40]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow: Pivot rows in mysql](https://stackoverflow.com/questions/1241178/mysql-rows-to-columns/9668036#9668036)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"history\")\n",
    "sql_df1 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        hostid,\n",
    "        CASE WHEN itemname = \"A\" THEN itemvalue END AS A,\n",
    "        CASE WHEN itemname = \"B\" THEN itemvalue END AS B,\n",
    "        CASE WHEN itemname = \"C\" THEN itemvalue END AS C\n",
    "    FROM \n",
    "        history\n",
    "    \"\"\"\n",
    ")\n",
    "sql_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_df1.createOrReplaceTempView('history_extended')\n",
    "sql_df2 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        hostid,\n",
    "        MIN(A) AS A,\n",
    "        MIN(B) AS B,\n",
    "        MIN(C) AS C\n",
    "    FROM \n",
    "        history_extended\n",
    "    GROUP BY \n",
    "        hostid\n",
    "    \"\"\"\n",
    ")\n",
    "sql_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_df1 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        hostid, \n",
    "        SUM( IF(itemname = 'A', itemvalue, 0) ) AS A,  \n",
    "        SUM( IF(itemname = 'B', itemvalue, 0) ) AS B, \n",
    "        SUM( IF(itemname = 'C', itemvalue, 0) ) AS C \n",
    "    FROM \n",
    "        history\n",
    "    GROUP BY\n",
    "        hostid\n",
    "    \"\"\"\n",
    ")\n",
    "sql_df1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
