{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2018-10-19 11:08:28 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.4.0\n",
      "\n",
      "numpy 1.14.1\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "matplotlib 2.2.2\n",
      "tensorflow 1.7.0\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# change default style figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/awjuliani/DeepRL-Agents\n",
    "- https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149\n",
    "\n",
    "Typical aspect of a task that makes it a RL problems are the following:\n",
    "\n",
    "- Different actions leads to different rewards. e.g. When looking for treasure in a maze, going left may lead to the treasure, whereas going right may lead to a pit of snakes.\n",
    "- Rewards are delayed over time. Even if going left in the example above is the correct right to do, we may not know this till later in the maze.\n",
    "- Reward for an action is conditional on the state of the environment. Continuing with our maze example, going left may be ideal at a certain fork in the path, but not at others.\n",
    "\n",
    "The n-armed bandit is a nice starting place since we don't have to worry about aspect 2 and 3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones. In the context of RL lingo, this is called learning a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lower the value, the more likely a positive reward will be returned,\n",
    "# in this case, bandit 4 is set to be the bandit that provids the most positive reward\n",
    "bandits = np.array([0.2, 0, -0.01, -5])\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "\n",
    "def pull_bandit(bandit):\n",
    "    \"\"\"\n",
    "    The probability of success is drawn from a normal distribution with\n",
    "    a mean around 0. And bandit whose value is below the numbers drawn\n",
    "    from the noraml distribution will receive a positive reward.\n",
    "    \"\"\"\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, axis=0)\n",
    "\n",
    "reward_holder = tf.placeholder(dtype=tf.float32)\n",
    "action_holder = tf.placeholder(dtype=tf.int32)\n",
    "responsible_weight = tf.gather(weights, action_holder)\n",
    "\n",
    "loss = -tf.log(responsible_weight) * reward_holder\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: [1. 0. 0. 0.]\n",
      "Running reward: [-3.  0.  0. 68.]\n",
      "Running reward: [ -2.  -1.   2. 160.]\n",
      "Running reward: [  1.  -1.   2. 255.]\n",
      "Running reward: [ -2.  -3.   1. 347.]\n",
      "Running reward: [ -4.  -6.   0. 441.]\n",
      "Running reward: [ -2.  -6.   0. 539.]\n",
      "Running reward: [ -2.  -7.   0. 632.]\n",
      "Running reward: [  0.  -6.   0. 725.]\n",
      "Running reward: [  1.  -7.   0. 817.]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(total_episodes):\n",
    "\n",
    "        # either explore or exploit\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "\n",
    "        # tally the reward for the action\n",
    "        reward = pull_bandit(bandits[action])\n",
    "        total_reward[action] += reward\n",
    "\n",
    "        sess.run(update, feed_dict={action_holder: action, reward_holder: reward})\n",
    "        if i % 100 == 0:\n",
    "            print('Running reward: ' + str(total_reward))\n",
    "\n",
    "    final_weights = sess.run(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final weights:  [1.000018  0.9939969 1.0040039 1.6794388]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "print('final weights: ', final_weights)\n",
    "max_bandit = np.argmax(final_weights)\n",
    "print('The agent thinks bandit ' + str(max_bandit + 1) + ' is the most promising....')\n",
    "if max_bandit == np.argmax(-bandits):\n",
    "    print('...and it was right!')\n",
    "else:\n",
    "    print('...and it was wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mingyuliu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c\n",
    "\n",
    "In n-armed bandits, there are no environmental states and the agent learn to choose which action it best to take. Without the environmental states, the best action at any moment is also the best action always. The problem we will be looking at here contains states, but they aren't determined by the previous states or actions. Additionally, we won't be considering delayed rewards.\n",
    "\n",
    "Contextual Bandit introduces the concept of the state, the state consists of a description of the environment that the agent can use to take more informed actions. In this problem, instead of a single bandit, there are now multiple bandits and the state of the environment tells us which bandit we are dealing with. The goal of the agent is to learn the best action not just for a single bandit, but for any number of them. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2, 0, -0.0, -5],\n",
    "                                 [0.1, -5, 1, 0.25],\n",
    "                                 [-5, 5, 5, 5]])\n",
    "        self.num_bandits, self.num_actions = self.bandits.shape\n",
    "\n",
    "    def get_state(self):\n",
    "        self.state = np.random.randint(num_bandits)\n",
    "        return self.state\n",
    "\n",
    "    def pull_arm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.rand(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "contextual_bandit = ContextualBandit()\n",
    "print(contextual_bandit.num_bandits)\n",
    "print(contextual_bandit.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
