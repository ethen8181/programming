{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Learning how to implement grid search from [Source code: scikit-learn's model selection](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted parameters, values:  [('a', [1, 2]), ('b', [True, False])]\n",
      "\n",
      "parameters:  ('a', 'b')\n",
      "values ([1, 2], [True, False])\n",
      "\n",
      "grid search parameters\n",
      "{'b': True, 'a': 1}\n",
      "{'b': False, 'a': 1}\n",
      "{'b': True, 'a': 2}\n",
      "{'b': False, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "from itertools import product\n",
    "from collections import Mapping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "\n",
    "# ensures that it also supports list of dictionary,\n",
    "# Mapping ensures a object has keys, values, items, etc. methods\n",
    "# which matches a dictionary\n",
    "# https://docs.python.org/3/library/collections.abc.html\n",
    "if isinstance(params_grid, Mapping):\n",
    "    params_grid = [params_grid]\n",
    "    \n",
    "for p in params_grid:\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    # this will become a list of paired tuples\n",
    "    items = sorted(p.items())\n",
    "    print('sorted parameters, values: ', items)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of tuples into two lists tuples, so what's originally \n",
    "    # a list of items [('a', [1, 2]), ('b', [True, False])], becomes\n",
    "    # two lists ('a', 'b'), ([1, 2], [True, False]), with all the keys being the parameter\n",
    "    # and the value being the list of possible values that the parameter can take\n",
    "    # http://stackoverflow.com/questions/7558908/unpacking-a-list-tuple-of-pairs-into-two-lists-tuples\n",
    "    key, value = zip(*items)\n",
    "    print('parameters: ', key)\n",
    "    print('values', value)\n",
    "    print()\n",
    "    \n",
    "    # unpack the list of values to compute the cartesian product\n",
    "    # [(1, True), (1, False), (2, True), (2, False)], and zip it\n",
    "    # back to the original key\n",
    "    print('grid search parameters')\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': True},\n",
       " {'a': 1, 'b': False},\n",
       " {'a': 2, 'b': True},\n",
       " {'a': 2, 'b': False}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm with scikit-learn's output\n",
    "list( ParameterGrid(params_grid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': True, 'a': 1}\n",
      "{'b': False, 'a': 1}\n",
      "{'b': True, 'a': 2}\n",
      "{'b': False, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "# making our function\n",
    "def _get_params_grid(params_grid):\n",
    "    \"\"\"\n",
    "    create cartesian product of parameters (grid search),\n",
    "    this will be a generator that will allow looping through\n",
    "    all possible parameter combination, note if we want to\n",
    "    expand this to cross validation we'll have to turn it to a list\n",
    "    \"\"\"\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    items = sorted(params_grid.items())\n",
    "    \n",
    "    # unpack parameter and the range of values\n",
    "    # into separate list; then unpack the range \n",
    "    # of values to compute the cartesian product\n",
    "    # and zip it back to the original key\n",
    "    key, value = zip(*items)\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        yield params\n",
    "\n",
    "params_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "params = _get_params_grid(params_grid)\n",
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# self._fit(X, y, groups, ParameterGrid(self.param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    \"\"\"\n",
    "    K-Folds cross-validation\n",
    "    Provides train/test indices to split data in train/test sets. Split\n",
    "    dataset into k consecutive folds; Each fold is then used once as \n",
    "    a validation while the k - 1 remaining folds form the training set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        number of folds. Must be at least 2\n",
    "    \n",
    "    shuffle : boolean, default True\n",
    "        whether to shuffle the data before splitting into batches\n",
    "    \n",
    "    seed : int, default 4321\n",
    "        When shuffle = True, pseudo-random number generator state used for\n",
    "        shuffling; this ensures reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits, shuffle = True, seed = 4321):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X):\n",
    "        \"\"\"pass in the data to create train/test split for k fold\"\"\"\n",
    "        # shuffle modifies indices inplace\n",
    "        indices = np.arange(X.shape[0])\n",
    "        if self.shuffle:\n",
    "            rstate = np.random.RandomState(self.seed)\n",
    "            rstate.shuffle(indices)\n",
    "\n",
    "        for test_mask in self._iter_test_masks(X, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            yield train_index, test_index\n",
    "        \n",
    "    def _iter_test_masks(self, X, indices):\n",
    "        \"\"\"\n",
    "        create the mask for the test set, then the indices that\n",
    "        are not in the test set belongs in the training set\n",
    "        \"\"\"\n",
    "        # indicate the number of samples in each fold, and also\n",
    "        # make sure the ones that are not evenly splitted also\n",
    "        # gets assigned to a fold (e.g. if we do 2 fold on a\n",
    "        # dataset that has 5 samples, then 1 will be left out,\n",
    "        # and has to be assigned to one of the other fold)\n",
    "        n_samples = X.shape[0]\n",
    "        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(n_samples, dtype = np.bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n",
      "\n",
      "confirm results with scikit-learn\n",
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create some sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "kf = KFolds(n_splits = 2, shuffle = False, seed = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print('\\nconfirm results with scikit-learn')\n",
    "kf = KFold(n_splits = 2, random_state = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://zacharyst.com/2016/03/31/parallelize-a-multifunction-argument-in-python/\n",
    "- https://pythonhosted.org/joblib/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs = 2, verbose = 1)( delayed(sqrt)(i ** 2) for i in range(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fit_and_score(estimator, X, y, scorer, \n",
    "                   train_index, test_index,\n",
    "                   parameters, fit_params):\n",
    "    \n",
    "    # create the train/test split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # fit the model\n",
    "    estimator.set_params(**parameters)\n",
    "    estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # obtain the train/test score\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "    y_pred_test  = estimator.predict(X_test)\n",
    "    train_score = scorer(y_train, y_pred_train)\n",
    "    test_score  = scorer(y_test, y_pred_test)\n",
    "    output = [train_score, test_score]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GridSearchCV:\n",
    "    \n",
    "    def __init__(self, estimator, scorer, cv, param_grid,\n",
    "                 fit_params = None, verbose = True, n_jobs = -1, \n",
    "                 pre_dispatch = '2*n_jobs', refit = True):\n",
    "        self.cv = cv\n",
    "        self.refit = refit\n",
    "        self.n_jobs = n_jobs\n",
    "        self.scorer = scorer\n",
    "        self.verbose = verbose\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.fit_params = fit_params\n",
    "        self.pre_dispatch = pre_dispatch     \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # object used as a cross-validation generator\n",
    "        # is passed without any modification\n",
    "        if isinstance(self.cv, int):\n",
    "            cv = KFolds(n_splits = self.cv)\n",
    "        else:\n",
    "            cv = self.cv\n",
    "        \n",
    "        # obtain the train/test set index, the parameters\n",
    "        # and perform cross validation\n",
    "        cv_iter = cv.split(X)\n",
    "        params_iterable = list(_get_params_grid(self.param_grid))\n",
    "        fit_params = self.fit_params if self.fit_params is not None else {}\n",
    "        \n",
    "        parallel = Parallel(n_jobs = self.n_jobs, verbose = self.verbose, \n",
    "                            pre_dispatch = self.pre_dispatch)\n",
    "        output = parallel(delayed(_fit_and_score)(deepcopy(self.estimator), \n",
    "                                                  X, y, self.scorer,\n",
    "                                                  train_index, test_index, \n",
    "                                                  parameters, fit_params)\n",
    "                          for train_index, test_index in cv_iter\n",
    "                          for parameters in params_iterable)\n",
    "\n",
    "        # unpack training/testing scores\n",
    "        n_splits = cv.n_splits\n",
    "        n_candidates = len(params_iterable)\n",
    "        train_score, test_score = zip(*output)\n",
    "        train_score = np.array(train_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        test_score = np.array(test_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        \n",
    "        # obtain the best score and parameter using the \n",
    "        # best mean test scores across all folds, where\n",
    "        # best here means the higher the better\n",
    "        mean_test_score = np.mean(test_score, axis = 0)\n",
    "        best_index = np.argmax(mean_test_score)\n",
    "        self.best_score = mean_test_score[best_index]\n",
    "        self.best_param = params_iterable[best_index]\n",
    "\n",
    "        # list the mean, std train and test score\n",
    "        # for each parameters combination;\n",
    "        # not sure if 'params', the column with the\n",
    "        # values in the dictionary format is useful or not\n",
    "        mean_train_score = np.mean(train_score, axis = 0)\n",
    "        std_test_score = np.std(test_score, axis = 0)\n",
    "        std_train_score = np.std(train_score, axis = 0)\n",
    "        cv_results = {\n",
    "            'mean_train_score': mean_train_score,\n",
    "            'std_train_score': std_train_score,\n",
    "            'mean_test_score': mean_test_score,\n",
    "            'std_test_score': std_test_score\n",
    "        }\n",
    "\n",
    "        # ensure the columns appear in this order (train score, test score, parameters)\n",
    "        # and order by the best test score\n",
    "        cols = ['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']\n",
    "        cv_results = pd.DataFrame(cv_results, columns = cols)\n",
    "        df_params  = pd.DataFrame(params_iterable)\n",
    "        cv_results = pd.concat([cv_results, df_params], axis = 1)\n",
    "        cv_results['params'] = params_iterable\n",
    "        cv_results = cv_results.sort_values(['mean_test_score', 'std_test_score'], ascending = False)\n",
    "        cv_results = cv_results.reset_index(drop = True)\n",
    "        self.cv_results = cv_results\n",
    "        \n",
    "        # refit on the entire dataset after performing cross validation\n",
    "        if self.refit:\n",
    "            best_estimator = deepcopy(self.estimator)\n",
    "            best_estimator.set_params(**self.best_param)\n",
    "            best_estimator.fit(X, y, **fit_params)\n",
    "            self.best_estimator = best_estimator\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"call predict on the estimator with the best found parameter\"\"\"\n",
    "        if not self.refit:\n",
    "            raise ValueError('Only available if refit=True')\n",
    "        \n",
    "        return self.best_estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 10, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 10, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.963333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3, 'crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3, 'crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 3, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 1, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.048990</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 1, 'crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.048990</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 10, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 1, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 10, 'cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 3, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 1, 'crit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  std_train_score  mean_test_score  std_test_score  \\\n",
       "0           0.980000         0.000000         0.966667        0.033993   \n",
       "1           0.966667         0.004714         0.953333        0.024944   \n",
       "2           0.963333         0.009428         0.946667        0.049889   \n",
       "3           0.973333         0.004714         0.946667        0.033993   \n",
       "4           0.986667         0.009428         0.946667        0.024944   \n",
       "5           0.996667         0.004714         0.946667        0.009428   \n",
       "6           0.966667         0.012472         0.940000        0.048990   \n",
       "7           0.966667         0.012472         0.940000        0.048990   \n",
       "8           0.993333         0.004714         0.940000        0.032660   \n",
       "9           0.976667         0.004714         0.940000        0.028284   \n",
       "10          0.996667         0.004714         0.933333        0.024944   \n",
       "11          0.976667         0.004714         0.933333        0.009428   \n",
       "\n",
       "   criterion  max_depth  min_samples_split  \\\n",
       "0       gini        NaN                 10   \n",
       "1    entropy        NaN                 10   \n",
       "2       gini        3.0                  3   \n",
       "3    entropy        3.0                  3   \n",
       "4       gini        NaN                  3   \n",
       "5    entropy        NaN                  1   \n",
       "6       gini        3.0                  1   \n",
       "7       gini        3.0                 10   \n",
       "8       gini        NaN                  1   \n",
       "9    entropy        3.0                 10   \n",
       "10   entropy        NaN                  3   \n",
       "11   entropy        3.0                  1   \n",
       "\n",
       "                                               params  \n",
       "0   {'max_depth': None, 'min_samples_split': 10, '...  \n",
       "1   {'max_depth': None, 'min_samples_split': 10, '...  \n",
       "2   {'max_depth': 3, 'min_samples_split': 3, 'crit...  \n",
       "3   {'max_depth': 3, 'min_samples_split': 3, 'crit...  \n",
       "4   {'max_depth': None, 'min_samples_split': 3, 'c...  \n",
       "5   {'max_depth': None, 'min_samples_split': 1, 'c...  \n",
       "6   {'max_depth': 3, 'min_samples_split': 1, 'crit...  \n",
       "7   {'max_depth': 3, 'min_samples_split': 10, 'cri...  \n",
       "8   {'max_depth': None, 'min_samples_split': 1, 'c...  \n",
       "9   {'max_depth': 3, 'min_samples_split': 10, 'cri...  \n",
       "10  {'max_depth': None, 'min_samples_split': 3, 'c...  \n",
       "11  {'max_depth': 3, 'min_samples_split': 1, 'crit...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_jobs = -1\n",
    "#verbose = True\n",
    "#pre_dispatch = '2*n_jobs'\n",
    "\n",
    "# dictionary of\n",
    "# additional parameters pass to fit\n",
    "# or just None\n",
    "#fit_params = None \n",
    "\n",
    "#n_splits = 3\n",
    "#kf = KFolds(n_splits = n_splits, shuffle = True, seed = 4312)\n",
    "#cv_iter = kf.split(X)\n",
    "#params_iterable = list(_get_params_grid(param_grid))\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# options\n",
    "cv = 3\n",
    "scorer = accuracy_score\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = {'max_depth': [3, None],\n",
    "              'min_samples_split': [1, 3, 10],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# fit grid search\n",
    "grid_search = GridSearchCV(estimator = clf, scorer = scorer, cv = cv, param_grid = param_grid)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "def report(results):\n",
    "    \"\"\"report best scores and corresponding parameters\"\"\"\n",
    "    print( 'Best score obtained: {0}'.format(results.best_score_) )\n",
    "    print('Parameters:')\n",
    "    for param, value in results.best_params_.items():\n",
    "        print( '\\t{}: {}'.format(param, value) )\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'max_depth': [3, None],\n",
    "              'min_samples_split': randint(1, 11),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 6\n",
    "random_search = RandomizedSearchCV(clf, param_distributions = param_dist,\n",
    "                                   n_iter = n_iter_search)\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print('RandomizedSearchCV took %.2f seconds for %d candidates'\n",
    "      ' parameter settings.' % ((time() - start), n_iter_search))\n",
    "report(random_search)\n",
    "\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'max_depth': [3, None],\n",
    "              'min_samples_split': [1, 3, 10],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid = param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "print('GridSearchCV took %.2f seconds for %d candidate parameter settings.'\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [Github: scikit-learn's KFold](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_split.py#L347)\n",
    "- [Github: scikit-learn's GridSearch](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_search.py#L685)\n",
    "- [Scikit-learn Documentation: Comparing randomized search and grid search for hyperparameter estimation](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
