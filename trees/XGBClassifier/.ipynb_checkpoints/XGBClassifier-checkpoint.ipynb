{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost QuickStart\n",
    "\n",
    "Reimplemented from this [blog post](http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and testing data and combine them together for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124737, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Dataset/Train.csv')\n",
    "test  = pd.read_csv('Dataset/Test.csv')\n",
    "train['source'] = 'train' # add another column to differentiate training and testing \n",
    "test['source']  = 'test'\n",
    "data = pd.concat( [train, test], ignore_index = True )\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at categories for *object* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.985371\n",
       "1    0.014629\n",
       "Name: Disbursed, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "# data.isnull().sum()\n",
    "\n",
    "# looping through all of them\n",
    "# var = data.columns[ data.dtypes == 'object' ]\n",
    "# for v in var:\n",
    "#    print '\\nFrequency count for variable %s' % v\n",
    "#    print data[v].value_counts()\n",
    "train['Disbursed'].value_counts() / train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling individual variables.\n",
    "\n",
    "1. `City`, `EmployerName`, `Salary_Account`, `LoggedIn` variables dropped because of too many distinct levels.\n",
    "2. `Lead_Creation_Date` dropped because it doesn't appear to affect much intuitively.\n",
    "2. `DOB` date of birth is dropped after converted to `Age`.\n",
    "3. \"*_Missing\" named column created ( the star represents the original column name) and takes the value of 1 if the original column is missing and 0 if not. And the original column is then removed. `EMI_Loan_Submitted`, `Interest_Rate`, `Loan_Amount_Submitted`, `Loan_Tenure_Submitted`, `Processing_Fee`.\n",
    "5. `Existing_EMI`, `Loan_Amount_Applied`, `Loan_Tenure_Applied` imputed with 0 (median) since very few observations are missing.\n",
    "6. `Source`. The top 2 most frequent level kept as it is and all others combined into one single category \"others\".\n",
    "7. One-Hot-Encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    \"\"\"\n",
    "    Pass in data ( train and test data concated together ),\n",
    "    performs multiple preprocessing steps and returns the cleaned\n",
    "    training and testing data \n",
    "    \"\"\"\n",
    "    # 1. drop because of too many unique levels\n",
    "    # print len( data['City'].unique() )\n",
    "    too_many_levels = [ 'City', 'Employer_Name', 'Salary_Account', 'LoggedIn' ]\n",
    "    data.drop( too_many_levels, axis = 1, inplace = True )\n",
    "\n",
    "    # 2. drop this variable because doesn't appear to affect much intuitively\n",
    "    remove = [ 'ID', 'Lead_Creation_Date' ]\n",
    "    data.drop( remove, axis = 1, inplace = True )\n",
    "\n",
    "    # 3. Determine Age from DOB ( date of birth )\n",
    "    # extract the last two element of the string (year)\n",
    "    data['Age'] = data['DOB'].apply( lambda x: 115 - int(x[-2:]) )\n",
    "    data.drop( 'DOB', axis = 1, inplace = True )\n",
    "\n",
    "    # 4. record whether the value is missing or not ; drop original vaiables\n",
    "    missing_or_not = [ 'EMI_Loan_Submitted', 'Loan_Amount_Submitted', \n",
    "                       'Loan_Tenure_Submitted', 'Interest_Rate', 'Processing_Fee' ]\n",
    "    for col in missing_or_not:\n",
    "        column_name = col + '_Missing'\n",
    "        data[column_name] = data[col].apply( lambda x: 1 if pd.isnull(x) else 0 )\n",
    "    data.drop( missing_or_not, axis = 1, inplace = True )\n",
    "\n",
    "    # 5. impute by median, because just 111 observations are missing\n",
    "    impute_by_median = [ 'Existing_EMI', 'Loan_Amount_Applied', 'Loan_Tenure_Applied' ]\n",
    "    for col in impute_by_median:\n",
    "        data[col].fillna( data[col].median(), inplace = True )\n",
    "\n",
    "    # 6. categorize all the other levels as 'others' except for 'S122' and 'S133'\n",
    "    data['Source'] = data['Source'].apply( lambda x: 'others' if x not in ['S122','S133'] else x )\n",
    "    # data['Source'].value_counts()\n",
    "\n",
    "    # 7. one hot encode, exclude the last column 'source', which just keeps track of the train / test\n",
    "    object_cols = data.columns[ data.dtypes == 'object'][:-1]\n",
    "    data = pd.get_dummies( data, columns = object_cols  )\n",
    "\n",
    "    train = data[ data['source'] == 'train' ].copy()\n",
    "    test  = data[ data['source'] == 'test' ].copy()\n",
    "    train.drop( 'source', axis = 1, inplace = True )\n",
    "    test.drop( [ 'source', 'Disbursed' ], axis = 1, inplace = True )\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = data_preprocessing(data)\n",
    "# set the input and output columns\n",
    "target = 'Disbursed'\n",
    "predictors = [ x for x in train.columns if x not in [target] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier\n",
    "\n",
    "xgboost parameter [API](http://xgboost.readthedocs.org/en/latest/parameter.html#general-parameters).\n",
    "\n",
    "Initial try with some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifiers and the parameters' dictionary that does the random searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate = 0.005,\n",
    "    n_estimators = 10, # number of trees to fit \n",
    "    gamma = 0, # the minimum loss reduction to split the tree, the larger, the more conservative\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    objective = 'binary:logistic',\n",
    "    nthread = -1,\n",
    "    # scale_pos_weight = 1, # unbalance class\n",
    "    seed = 27\n",
    ")\n",
    "\n",
    "params_dict = {\n",
    "    'max_depth': [ 4, 7, 10 ],\n",
    "    'min_child_weight': [ 10, 15, 23 ] # the minimum number of node of a leaf node\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator = xgb1,\n",
    "    param_distributions = params_dict,\n",
    "    scoring = 'roc_auc',\n",
    "    cv = 10,\n",
    "    n_iter = 5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.005, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.8),\n",
       "          fit_params={}, iid=True, n_iter=5, n_jobs=1,\n",
       "          param_distributions={'max_depth': [4, 7, 10], 'min_child_weight': [10, 15, 23]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.fit( train[predictors].values, train[target].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'min_child_weight': 23}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.815490\n"
     ]
    }
   ],
   "source": [
    "pred = rs.predict(train[predictors])\n",
    "prob = rs.predict_proba( train[predictors])[ :, 1 ]\n",
    "        \n",
    "#Print model report:\n",
    "print \"\\nModel Report\"\n",
    "print \"Accuracy : %.4f\" % accuracy_score( train[target].values, pred )\n",
    "print \"AUC Score (Train): %f\" % roc_auc_score( train[target].values, prob )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.005, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=23, missing=None, n_estimators=10, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_estimator_.fit( train[predictors].values, train[target].values )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
